{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OoLfn3TmDC5f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Importing libraries\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, TimeDistributed, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5cBJRK3Dxhm",
        "outputId": "07253c68-2e54-4e1a-dd90-f5cab399dd55"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, clone_model\n",
        "from tensorflow.keras.layers import TimeDistributed, Conv1D, MaxPooling1D, Dropout, Flatten, LSTM, Dense, InputLayer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "dataset_path = \"/content/drive/MyDrive/UCI_HAR_Dataset\"\n",
        "test_subjects = [14]  # hold-out subjects for testing\n",
        "batch_size = 32\n",
        "epochs_general = 30   # epochs for general model\n",
        "epochs_finetune = 5   # epochs for per-subject fine-tuning\n",
        "n_steps, n_length = 4, 32\n",
        "n_hidden = 16\n",
        "\n",
        "SIGNALS = [\n",
        "    \"body_acc_x\", \"body_acc_y\", \"body_acc_z\",\n",
        "    \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\",\n",
        "    \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"\n",
        "]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(dataset_path, test_size=0.2, seed=42):\n",
        "    def _read_csv(filename):\n",
        "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "    def load_signals(subset):\n",
        "        signals_data = []\n",
        "        for signal in SIGNALS:\n",
        "            filename = f\"{dataset_path}/{subset}/Inertial Signals/{signal}_{subset}.txt\"\n",
        "            signals_data.append(_read_csv(filename).to_numpy())\n",
        "        return np.transpose(signals_data, (1, 2, 0))  # (samples, timesteps=128, 9 signals)\n",
        "\n",
        "    def load_y(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/y_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    def load_subjects(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/subject_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    # Load full dataset (train + test)\n",
        "    X_train, y_train, subj_train = load_signals(\"train\"), load_y(\"train\"), load_subjects(\"train\")\n",
        "    X_test,  y_test,  subj_test  = load_signals(\"test\"),  load_y(\"test\"),  load_subjects(\"test\")\n",
        "\n",
        "    # Merge all\n",
        "    X_all = np.vstack([X_train, X_test])\n",
        "    y_all = np.concatenate([y_train, y_test])\n",
        "    subjects = np.concatenate([subj_train, subj_test])  # not used anymore\n",
        "\n",
        "    print(f\"âœ… Loaded all subjects together: {X_all.shape[0]} samples\")\n",
        "\n",
        "    # Shuffle + split (ignore subject IDs)\n",
        "    train_X, test_X, train_y, test_y = train_test_split(\n",
        "        X_all, y_all, test_size=test_size, random_state=seed, shuffle=True, stratify=y_all\n",
        "    )\n",
        "\n",
        "    print(f\"ðŸ“Š Final split -> Train: {train_X.shape}, Test: {test_X.shape}\")\n",
        "\n",
        "    return train_X, train_y, test_X, test_y\n"
      ],
      "metadata": {
        "id": "pYg87mnXFILb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN-1D"
      ],
      "metadata": {
        "id": "4lLTpTPS998k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ---------------------------\n",
        "# Load dataset (merged subjects, 80/20 split)\n",
        "# ---------------------------\n",
        "X_train, y_train, X_test, y_test = load_data(dataset_path, test_size=0.2, seed=42)\n",
        "\n",
        "# ---------------------------\n",
        "# Encode labels\n",
        "# ---------------------------\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded  = le.transform(y_test)\n",
        "\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Classes found:\", le.classes_)\n",
        "\n",
        "# reshape into subsequences for CNN-GRU\n",
        "X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, X_train.shape[2]))\n",
        "X_test  = X_test.reshape((X_test.shape[0],  n_steps, n_length, X_test.shape[2]))\n",
        "\n",
        "# convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Define pure CNN model\n",
        "# ---------------------------\n",
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, n_steps, n_length, input_dim, n_classes):\n",
        "        super(CNN1D, self).__init__()\n",
        "\n",
        "        # Conv layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=5, padding=2)\n",
        "        self.bn1   = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
        "        self.bn2   = nn.BatchNorm1d(128)\n",
        "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.pool    = nn.AdaptiveMaxPool1d(1)  # global pooling\n",
        "\n",
        "        # FC layer\n",
        "        self.fc = nn.Linear(256, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, n_steps, n_length, input_dim)\n",
        "        # Merge steps and time\n",
        "        x = x.reshape(x.size(0), x.size(1) * x.size(2), x.size(3))  # (B, steps*len, channels)\n",
        "        x = x.permute(0, 2, 1)  # (B, channels=input_dim, seq_len)\n",
        "\n",
        "        # Conv stack\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Global pooling\n",
        "        x = self.pool(x).squeeze(-1)  # (B, 256)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Train general model\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "general_model = CNN1D(n_steps, n_length, X_train.shape[3], n_classes).to(device)\n",
        "\n",
        "\n",
        "# Label smoothing loss\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "optimizer = torch.optim.Adam(general_model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "early_stop_patience = 10\n",
        "\n",
        "for epoch in range(epochs_general):\n",
        "    # --- Train ---\n",
        "    general_model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = general_model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(general_model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation ---\n",
        "    general_model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = general_model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    val_loss = val_loss / len(test_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs_general} \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_weights = general_model.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stop_patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "\n",
        "# restore best weights\n",
        "general_model.load_state_dict(best_weights)\n",
        "\n",
        "# Save weights of general model\n",
        "general_weights = general_model.state_dict()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-mMrMT35GoJ",
        "outputId": "2a66d7b4-61e7-4a1f-879c-20ebb31761ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded all subjects together: 10299 samples\n",
            "ðŸ“Š Final split -> Train: (8239, 128, 9), Test: (2060, 128, 9)\n",
            "Classes found: [1 2 3 4 5 6]\n",
            "Epoch 1/30 Train Loss: 0.6233, Train Acc: 0.9198 | Val Loss: 0.6900, Val Acc: 0.9621\n",
            "Epoch 2/30 Train Loss: 0.5270, Train Acc: 0.9567 | Val Loss: 0.6795, Val Acc: 0.9665\n",
            "Epoch 3/30 Train Loss: 0.5109, Train Acc: 0.9608 | Val Loss: 0.6744, Val Acc: 0.9733\n",
            "Epoch 4/30 Train Loss: 0.5049, Train Acc: 0.9646 | Val Loss: 0.6762, Val Acc: 0.9757\n",
            "Epoch 5/30 Train Loss: 0.4976, Train Acc: 0.9681 | Val Loss: 0.6672, Val Acc: 0.9684\n",
            "Epoch 6/30 Train Loss: 0.4929, Train Acc: 0.9703 | Val Loss: 0.6731, Val Acc: 0.9816\n",
            "Epoch 7/30 Train Loss: 0.4914, Train Acc: 0.9707 | Val Loss: 0.6683, Val Acc: 0.9767\n",
            "Epoch 8/30 Train Loss: 0.4845, Train Acc: 0.9746 | Val Loss: 0.6490, Val Acc: 0.9801\n",
            "Epoch 9/30 Train Loss: 0.4861, Train Acc: 0.9709 | Val Loss: 0.6669, Val Acc: 0.9786\n",
            "Epoch 10/30 Train Loss: 0.4789, Train Acc: 0.9765 | Val Loss: 0.6521, Val Acc: 0.9791\n",
            "Epoch 11/30 Train Loss: 0.4803, Train Acc: 0.9763 | Val Loss: 0.6768, Val Acc: 0.9830\n",
            "Epoch 12/30 Train Loss: 0.4769, Train Acc: 0.9773 | Val Loss: 0.6865, Val Acc: 0.9791\n",
            "Epoch 13/30 Train Loss: 0.4681, Train Acc: 0.9806 | Val Loss: 0.6602, Val Acc: 0.9820\n",
            "Epoch 14/30 Train Loss: 0.4678, Train Acc: 0.9828 | Val Loss: 0.6647, Val Acc: 0.9830\n",
            "Epoch 15/30 Train Loss: 0.4657, Train Acc: 0.9847 | Val Loss: 0.6626, Val Acc: 0.9845\n",
            "Epoch 16/30 Train Loss: 0.4641, Train Acc: 0.9845 | Val Loss: 0.6661, Val Acc: 0.9869\n",
            "Epoch 17/30 Train Loss: 0.4614, Train Acc: 0.9857 | Val Loss: 0.6721, Val Acc: 0.9830\n",
            "Epoch 18/30 Train Loss: 0.4579, Train Acc: 0.9883 | Val Loss: 0.6611, Val Acc: 0.9840\n",
            "Early stopping triggered!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer"
      ],
      "metadata": {
        "id": "BeyuaJjwzmS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration\n",
        "# ---------------------------\n",
        "batch_size = 32\n",
        "epochs = 30\n",
        "learning_rate = 0.0005\n",
        "d_model = 128          # transformer embedding size\n",
        "nhead = 8             # number of attention heads\n",
        "num_layers = 4        # number of transformer encoder layers\n",
        "dropout = 0.5\n",
        "\n",
        "# ---------------------------\n",
        "# Load dataset (merged subjects, 80/20 split)\n",
        "# ---------------------------\n",
        "# X_train, y_train, X_test, y_test = load_data(dataset_path, test_size=0.2, seed=42)\n",
        "# Ensure X_train.shape = (num_samples, seq_len, n_features)\n",
        "\n",
        "# ---------------------------\n",
        "# Encode labels\n",
        "# ---------------------------\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded  = le.transform(y_test)\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Classes found:\", le.classes_)\n",
        "\n",
        "# Convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# Datasets & loaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ---------------------------\n",
        "# Positional Encoding\n",
        "# ---------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "# ---------------------------\n",
        "# Transformer-based Model\n",
        "# ---------------------------\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_layers, n_classes, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
        "                                                   dim_feedforward=d_model*2, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(d_model, n_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, input_dim)\n",
        "        x = self.input_proj(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x[:, -1, :]           # take the last time step\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "# ---------------------------\n",
        "# Initialize model\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerClassifier(input_dim=X_train_tensor.shape[2],\n",
        "                              d_model=d_model, nhead=nhead, num_layers=num_layers,\n",
        "                              n_classes=n_classes, dropout=dropout).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# ---------------------------\n",
        "# Training loop\n",
        "# ---------------------------\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    val_loss = val_loss / len(test_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kpEJLhzzllz",
        "outputId": "5c846eb3-2411-4b52-f5b8-096c923e31e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes found: [1 2 3 4 5 6]\n",
            "Epoch 1/30 Train Loss: 1.4435, Train Acc: 0.3957 | Val Loss: 1.1733, Val Acc: 0.5184\n",
            "Epoch 2/30 Train Loss: 0.9770, Train Acc: 0.6002 | Val Loss: 0.6660, Val Acc: 0.7471\n",
            "Epoch 3/30 Train Loss: 0.6116, Train Acc: 0.7670 | Val Loss: 0.5198, Val Acc: 0.8204\n",
            "Epoch 4/30 Train Loss: 0.4696, Train Acc: 0.8270 | Val Loss: 0.3798, Val Acc: 0.8553\n",
            "Epoch 5/30 Train Loss: 0.4283, Train Acc: 0.8422 | Val Loss: 0.4326, Val Acc: 0.8476\n",
            "Epoch 6/30 Train Loss: 0.4012, Train Acc: 0.8544 | Val Loss: 0.3464, Val Acc: 0.8767\n",
            "Epoch 7/30 Train Loss: 0.3785, Train Acc: 0.8635 | Val Loss: 0.3621, Val Acc: 0.8723\n",
            "Epoch 8/30 Train Loss: 0.3533, Train Acc: 0.8730 | Val Loss: 0.3271, Val Acc: 0.8825\n",
            "Epoch 9/30 Train Loss: 0.3513, Train Acc: 0.8752 | Val Loss: 0.3544, Val Acc: 0.8777\n",
            "Epoch 10/30 Train Loss: 0.3284, Train Acc: 0.8792 | Val Loss: 0.3653, Val Acc: 0.8762\n",
            "Epoch 11/30 Train Loss: 0.3234, Train Acc: 0.8838 | Val Loss: 0.3648, Val Acc: 0.8796\n",
            "Epoch 12/30 Train Loss: 0.3302, Train Acc: 0.8800 | Val Loss: 0.3129, Val Acc: 0.8840\n",
            "Epoch 13/30 Train Loss: 0.2983, Train Acc: 0.8929 | Val Loss: 0.3082, Val Acc: 0.9005\n",
            "Epoch 14/30 Train Loss: 0.2858, Train Acc: 0.8944 | Val Loss: 0.3107, Val Acc: 0.8947\n",
            "Epoch 15/30 Train Loss: 0.3050, Train Acc: 0.8877 | Val Loss: 0.2769, Val Acc: 0.9058\n",
            "Epoch 16/30 Train Loss: 0.2740, Train Acc: 0.9014 | Val Loss: 0.3185, Val Acc: 0.8869\n",
            "Epoch 17/30 Train Loss: 0.2962, Train Acc: 0.8919 | Val Loss: 0.2828, Val Acc: 0.9015\n",
            "Epoch 18/30 Train Loss: 0.2644, Train Acc: 0.9028 | Val Loss: 0.3497, Val Acc: 0.8845\n",
            "Epoch 19/30 Train Loss: 0.2588, Train Acc: 0.9021 | Val Loss: 0.2983, Val Acc: 0.9019\n",
            "Epoch 20/30 Train Loss: 0.2408, Train Acc: 0.9087 | Val Loss: 0.2603, Val Acc: 0.9126\n",
            "Epoch 21/30 Train Loss: 0.2452, Train Acc: 0.9107 | Val Loss: 0.2906, Val Acc: 0.9097\n",
            "Epoch 22/30 Train Loss: 0.2588, Train Acc: 0.9064 | Val Loss: 0.2718, Val Acc: 0.9044\n",
            "Epoch 23/30 Train Loss: 0.2496, Train Acc: 0.9104 | Val Loss: 0.2829, Val Acc: 0.9117\n",
            "Epoch 24/30 Train Loss: 0.2472, Train Acc: 0.9090 | Val Loss: 0.2517, Val Acc: 0.9209\n",
            "Epoch 25/30 Train Loss: 0.2507, Train Acc: 0.9110 | Val Loss: 0.6406, Val Acc: 0.8568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN-GRU"
      ],
      "metadata": {
        "id": "ltgp-ESG5BGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ---------------------------\n",
        "# Load dataset (merged subjects, 80/20 split)\n",
        "# ---------------------------\n",
        "X_train, y_train, X_test, y_test = load_data(dataset_path, test_size=0.2, seed=42)\n",
        "\n",
        "# ---------------------------\n",
        "# Encode labels\n",
        "# ---------------------------\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded  = le.transform(y_test)\n",
        "\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Classes found:\", le.classes_)\n",
        "\n",
        "# reshape into subsequences for CNN-GRU\n",
        "X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, X_train.shape[2]))\n",
        "X_test  = X_test.reshape((X_test.shape[0],  n_steps, n_length, X_test.shape[2]))\n",
        "\n",
        "# convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Define CNN-GRU with residuals + norm\n",
        "# ---------------------------\n",
        "class CNNGRU(nn.Module):\n",
        "    def __init__(self, n_steps, n_length, input_dim, n_hidden, n_classes):\n",
        "        super(CNNGRU, self).__init__()\n",
        "\n",
        "        # --- CNN layers (from the pure CNN above) ---\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=5, padding=2)\n",
        "        self.bn1   = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
        "        self.bn2   = nn.BatchNorm1d(128)\n",
        "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.dropout_cnn = nn.Dropout(0.5)\n",
        "        self.pool        = nn.AdaptiveMaxPool1d(1)  # global pooling\n",
        "\n",
        "        # --- GRU ---\n",
        "        self.gru = nn.GRU(input_size=256, hidden_size=n_hidden, batch_first=True)\n",
        "        self.layer_norm = nn.LayerNorm(n_hidden)\n",
        "\n",
        "        # --- FC layer (single, like pure CNN) ---\n",
        "        self.dropout_fc = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(n_hidden, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, n_steps, n_length, input_dim)\n",
        "        x = x.permute(0, 1, 3, 2)   # (B, steps, in_ch, len)\n",
        "        batch, n_steps, in_ch, seq_len = x.size()\n",
        "        x = x.reshape(batch * n_steps, in_ch, seq_len)\n",
        "\n",
        "        # CNN stack\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout_cnn(x)\n",
        "\n",
        "        # Global pooling â†’ (batch*steps, 256)\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "\n",
        "        # Reshape for GRU â†’ (batch, steps, features)\n",
        "        x = x.reshape(batch, n_steps, -1)\n",
        "\n",
        "        # GRU\n",
        "        out, _ = self.gru(x)       # (batch, steps, hidden)\n",
        "        out = out[:, -1, :]        # take last hidden state\n",
        "        out = self.layer_norm(out)\n",
        "\n",
        "        # FC classifier\n",
        "        out = self.dropout_fc(out)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Train general model\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "general_model = CNNGRU(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "\n",
        "# Label smoothing loss\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "optimizer = torch.optim.Adam(general_model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "early_stop_patience = 10\n",
        "\n",
        "for epoch in range(epochs_general):\n",
        "    # --- Train ---\n",
        "    general_model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = general_model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(general_model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation ---\n",
        "    general_model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = general_model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    val_loss = val_loss / len(test_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs_general} \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_weights = general_model.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stop_patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "\n",
        "# restore best weights\n",
        "general_model.load_state_dict(best_weights)\n",
        "\n",
        "# Save weights of general model\n",
        "general_weights = general_model.state_dict()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuyPwRAYFpeO",
        "outputId": "1e8cae5c-6803-481f-8e5a-f17fc4a7a437"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded all subjects together: 10299 samples\n",
            "ðŸ“Š Final split -> Train: (8239, 128, 9), Test: (2060, 128, 9)\n",
            "Classes found: [1 2 3 4 5 6]\n",
            "Epoch 1/30 Train Loss: 0.9705, Train Acc: 0.7684 | Val Loss: 0.6503, Val Acc: 0.9398\n",
            "Epoch 2/30 Train Loss: 0.7450, Train Acc: 0.8968 | Val Loss: 0.5692, Val Acc: 0.9485\n",
            "Epoch 3/30 Train Loss: 0.6935, Train Acc: 0.9133 | Val Loss: 0.5312, Val Acc: 0.9481\n",
            "Epoch 4/30 Train Loss: 0.6803, Train Acc: 0.9201 | Val Loss: 0.5229, Val Acc: 0.9583\n",
            "Epoch 5/30 Train Loss: 0.6632, Train Acc: 0.9235 | Val Loss: 0.5308, Val Acc: 0.9485\n",
            "Epoch 6/30 Train Loss: 0.6612, Train Acc: 0.9267 | Val Loss: 0.5183, Val Acc: 0.9573\n",
            "Epoch 7/30 Train Loss: 0.6556, Train Acc: 0.9280 | Val Loss: 0.5134, Val Acc: 0.9583\n",
            "Epoch 8/30 Train Loss: 0.6495, Train Acc: 0.9290 | Val Loss: 0.5123, Val Acc: 0.9631\n",
            "Epoch 9/30 Train Loss: 0.6430, Train Acc: 0.9379 | Val Loss: 0.5089, Val Acc: 0.9617\n",
            "Epoch 10/30 Train Loss: 0.6344, Train Acc: 0.9392 | Val Loss: 0.5100, Val Acc: 0.9631\n",
            "Epoch 11/30 Train Loss: 0.6302, Train Acc: 0.9421 | Val Loss: 0.5037, Val Acc: 0.9646\n",
            "Epoch 12/30 Train Loss: 0.6252, Train Acc: 0.9455 | Val Loss: 0.5102, Val Acc: 0.9597\n",
            "Epoch 13/30 Train Loss: 0.6292, Train Acc: 0.9410 | Val Loss: 0.5007, Val Acc: 0.9641\n",
            "Epoch 14/30 Train Loss: 0.6220, Train Acc: 0.9450 | Val Loss: 0.5049, Val Acc: 0.9592\n",
            "Epoch 15/30 Train Loss: 0.6191, Train Acc: 0.9484 | Val Loss: 0.4940, Val Acc: 0.9670\n",
            "Epoch 16/30 Train Loss: 0.6116, Train Acc: 0.9530 | Val Loss: 0.4930, Val Acc: 0.9675\n",
            "Epoch 17/30 Train Loss: 0.6093, Train Acc: 0.9518 | Val Loss: 0.4977, Val Acc: 0.9650\n",
            "Epoch 18/30 Train Loss: 0.6054, Train Acc: 0.9542 | Val Loss: 0.4950, Val Acc: 0.9680\n",
            "Epoch 19/30 Train Loss: 0.6062, Train Acc: 0.9544 | Val Loss: 0.4932, Val Acc: 0.9665\n",
            "Epoch 20/30 Train Loss: 0.5997, Train Acc: 0.9570 | Val Loss: 0.4866, Val Acc: 0.9709\n",
            "Epoch 21/30 Train Loss: 0.5982, Train Acc: 0.9587 | Val Loss: 0.4903, Val Acc: 0.9704\n",
            "Epoch 22/30 Train Loss: 0.5945, Train Acc: 0.9587 | Val Loss: 0.4860, Val Acc: 0.9723\n",
            "Epoch 23/30 Train Loss: 0.5926, Train Acc: 0.9625 | Val Loss: 0.4836, Val Acc: 0.9714\n",
            "Epoch 24/30 Train Loss: 0.5902, Train Acc: 0.9596 | Val Loss: 0.5177, Val Acc: 0.9505\n",
            "Epoch 25/30 Train Loss: 0.5882, Train Acc: 0.9620 | Val Loss: 0.4895, Val Acc: 0.9680\n",
            "Epoch 26/30 Train Loss: 0.5821, Train Acc: 0.9665 | Val Loss: 0.4701, Val Acc: 0.9825\n",
            "Epoch 27/30 Train Loss: 0.5824, Train Acc: 0.9660 | Val Loss: 0.4760, Val Acc: 0.9801\n",
            "Epoch 28/30 Train Loss: 0.5798, Train Acc: 0.9686 | Val Loss: 0.4900, Val Acc: 0.9631\n",
            "Epoch 29/30 Train Loss: 0.5765, Train Acc: 0.9699 | Val Loss: 0.4630, Val Acc: 0.9811\n",
            "Epoch 30/30 Train Loss: 0.5708, Train Acc: 0.9716 | Val Loss: 0.4614, Val Acc: 0.9840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN-LSTM"
      ],
      "metadata": {
        "id": "jfRrBDjT-m1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ---------------------------\n",
        "# Load dataset (merged subjects, 80/20 split)\n",
        "# ---------------------------\n",
        "X_train, y_train, X_test, y_test = load_data(dataset_path, test_size=0.2, seed=42)\n",
        "\n",
        "# ---------------------------\n",
        "# Encode labels\n",
        "# ---------------------------\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded  = le.transform(y_test)\n",
        "\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Classes found:\", le.classes_)\n",
        "\n",
        "# reshape into subsequences for CNN-GRU\n",
        "X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, X_train.shape[2]))\n",
        "X_test  = X_test.reshape((X_test.shape[0],  n_steps, n_length, X_test.shape[2]))\n",
        "\n",
        "# convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "class CNNLSTM(nn.Module):\n",
        "    def __init__(self, n_steps, n_length, input_dim, n_hidden, n_classes):\n",
        "        super(CNNLSTM, self).__init__()\n",
        "\n",
        "        # --- CNN layers (from the pure CNN above) ---\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=5, padding=2)\n",
        "        self.bn1   = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
        "        self.bn2   = nn.BatchNorm1d(128)\n",
        "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.dropout_cnn = nn.Dropout(0.5)\n",
        "        self.pool        = nn.AdaptiveMaxPool1d(1)  # global pooling\n",
        "\n",
        "        # --- LSTM instead of GRU ---\n",
        "        self.lstm = nn.LSTM(input_size=256, hidden_size=n_hidden, batch_first=True)\n",
        "        self.layer_norm = nn.LayerNorm(n_hidden)\n",
        "\n",
        "        # --- Single FC layer ---\n",
        "        self.dropout_fc = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(n_hidden, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, n_steps, n_length, input_dim)\n",
        "        x = x.permute(0, 1, 3, 2)   # (B, steps, in_ch, len)\n",
        "        batch, n_steps, in_ch, seq_len = x.size()\n",
        "        x = x.reshape(batch * n_steps, in_ch, seq_len)\n",
        "\n",
        "        # CNN stack\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout_cnn(x)\n",
        "\n",
        "        # Global pooling â†’ (batch*steps, 256)\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "\n",
        "        # Reshape for LSTM â†’ (batch, steps, features)\n",
        "        x = x.reshape(batch, n_steps, -1)\n",
        "\n",
        "        # LSTM (ignore cell state)\n",
        "        out, (h_n, c_n) = self.lstm(x)   # out: (batch, steps, hidden)\n",
        "        out = out[:, -1, :]              # take last hidden state\n",
        "        out = self.layer_norm(out)\n",
        "\n",
        "        # FC classifier\n",
        "        out = self.dropout_fc(out)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Train general model\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "general_model = CNNGRU(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "\n",
        "# Label smoothing loss\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "optimizer = torch.optim.Adam(general_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "early_stop_patience = 10\n",
        "\n",
        "for epoch in range(epochs_general):\n",
        "    # --- Train ---\n",
        "    general_model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = general_model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(general_model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation ---\n",
        "    general_model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = general_model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    val_loss = val_loss / len(test_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs_general} \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_weights = general_model.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stop_patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "\n",
        "# restore best weights\n",
        "general_model.load_state_dict(best_weights)\n",
        "\n",
        "# Save weights of general model\n",
        "general_weights = general_model.state_dict()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTxp8miK-vBi",
        "outputId": "8581e824-30a9-44db-a5a8-c5233eac9499"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded all subjects together: 10299 samples\n",
            "ðŸ“Š Final split -> Train: (8239, 128, 9), Test: (2060, 128, 9)\n",
            "Classes found: [1 2 3 4 5 6]\n",
            "Epoch 1/30 Train Loss: 0.9329, Train Acc: 0.7893 | Val Loss: 0.6706, Val Acc: 0.8903\n",
            "Epoch 2/30 Train Loss: 0.7353, Train Acc: 0.8864 | Val Loss: 0.5577, Val Acc: 0.9398\n",
            "Epoch 3/30 Train Loss: 0.6975, Train Acc: 0.9050 | Val Loss: 0.5300, Val Acc: 0.9485\n",
            "Epoch 4/30 Train Loss: 0.6819, Train Acc: 0.9124 | Val Loss: 0.5287, Val Acc: 0.9515\n",
            "Epoch 5/30 Train Loss: 0.6640, Train Acc: 0.9149 | Val Loss: 0.5198, Val Acc: 0.9602\n",
            "Epoch 6/30 Train Loss: 0.6524, Train Acc: 0.9273 | Val Loss: 0.5305, Val Acc: 0.9519\n",
            "Epoch 7/30 Train Loss: 0.6525, Train Acc: 0.9254 | Val Loss: 0.5182, Val Acc: 0.9583\n",
            "Epoch 8/30 Train Loss: 0.6414, Train Acc: 0.9309 | Val Loss: 0.5167, Val Acc: 0.9578\n",
            "Epoch 9/30 Train Loss: 0.6362, Train Acc: 0.9345 | Val Loss: 0.5116, Val Acc: 0.9563\n",
            "Epoch 10/30 Train Loss: 0.6436, Train Acc: 0.9294 | Val Loss: 0.5309, Val Acc: 0.9534\n",
            "Epoch 11/30 Train Loss: 0.6280, Train Acc: 0.9410 | Val Loss: 0.5181, Val Acc: 0.9515\n",
            "Epoch 12/30 Train Loss: 0.6189, Train Acc: 0.9426 | Val Loss: 0.5080, Val Acc: 0.9641\n",
            "Epoch 13/30 Train Loss: 0.6183, Train Acc: 0.9425 | Val Loss: 0.5009, Val Acc: 0.9602\n",
            "Epoch 14/30 Train Loss: 0.6184, Train Acc: 0.9421 | Val Loss: 0.5050, Val Acc: 0.9607\n",
            "Epoch 15/30 Train Loss: 0.6188, Train Acc: 0.9422 | Val Loss: 0.4995, Val Acc: 0.9583\n",
            "Epoch 16/30 Train Loss: 0.6095, Train Acc: 0.9472 | Val Loss: 0.4943, Val Acc: 0.9680\n",
            "Epoch 17/30 Train Loss: 0.6039, Train Acc: 0.9491 | Val Loss: 0.5015, Val Acc: 0.9612\n",
            "Epoch 18/30 Train Loss: 0.6054, Train Acc: 0.9511 | Val Loss: 0.4908, Val Acc: 0.9655\n",
            "Epoch 19/30 Train Loss: 0.6039, Train Acc: 0.9531 | Val Loss: 0.4928, Val Acc: 0.9694\n",
            "Epoch 20/30 Train Loss: 0.6042, Train Acc: 0.9513 | Val Loss: 0.4878, Val Acc: 0.9646\n",
            "Epoch 21/30 Train Loss: 0.5953, Train Acc: 0.9565 | Val Loss: 0.4821, Val Acc: 0.9718\n",
            "Epoch 22/30 Train Loss: 0.5883, Train Acc: 0.9589 | Val Loss: 0.4832, Val Acc: 0.9738\n",
            "Epoch 23/30 Train Loss: 0.5926, Train Acc: 0.9558 | Val Loss: 0.4789, Val Acc: 0.9718\n",
            "Epoch 24/30 Train Loss: 0.5946, Train Acc: 0.9568 | Val Loss: 0.4832, Val Acc: 0.9704\n",
            "Epoch 25/30 Train Loss: 0.5874, Train Acc: 0.9640 | Val Loss: 0.4752, Val Acc: 0.9801\n",
            "Epoch 26/30 Train Loss: 0.5964, Train Acc: 0.9567 | Val Loss: 0.4832, Val Acc: 0.9733\n",
            "Epoch 27/30 Train Loss: 0.5794, Train Acc: 0.9641 | Val Loss: 0.4758, Val Acc: 0.9738\n",
            "Epoch 28/30 Train Loss: 0.5739, Train Acc: 0.9672 | Val Loss: 0.4725, Val Acc: 0.9791\n",
            "Epoch 29/30 Train Loss: 0.5793, Train Acc: 0.9678 | Val Loss: 0.4765, Val Acc: 0.9786\n",
            "Epoch 30/30 Train Loss: 0.5739, Train Acc: 0.9666 | Val Loss: 0.4832, Val Acc: 0.9699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inception-Time"
      ],
      "metadata": {
        "id": "jLptpLNHCIS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ---------------------------\n",
        "# Load dataset (merged subjects, 80/20 split)\n",
        "# ---------------------------\n",
        "X_train, y_train, X_test, y_test = load_data(dataset_path, test_size=0.2, seed=42)\n",
        "\n",
        "# ---------------------------\n",
        "# Encode labels\n",
        "# ---------------------------\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded  = le.transform(y_test)\n",
        "\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Classes found:\", le.classes_)\n",
        "\n",
        "# reshape into subsequences for CNN-GRU\n",
        "X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, X_train.shape[2]))\n",
        "X_test  = X_test.reshape((X_test.shape[0],  n_steps, n_length, X_test.shape[2]))\n",
        "\n",
        "# convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Inception Block\n",
        "# ---------------------------\n",
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_sizes=[9, 19, 39], bottleneck_channels=32):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "\n",
        "        self.bottleneck = (\n",
        "            nn.Conv1d(in_channels, bottleneck_channels, kernel_size=1, bias=False)\n",
        "            if in_channels > 1 else nn.Identity()\n",
        "        )\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(\n",
        "                bottleneck_channels if in_channels > 1 else in_channels,\n",
        "                out_channels,\n",
        "                kernel_size=k,\n",
        "                padding=k // 2,\n",
        "                bias=False,\n",
        "            )\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(out_channels * (len(kernel_sizes) + 1))\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_bottleneck = self.bottleneck(x)\n",
        "        out = [conv(x_bottleneck) for conv in self.convs]\n",
        "        out.append(self.maxpool_conv(x))\n",
        "        out = torch.cat(out, dim=1)\n",
        "        out = self.bn(out)\n",
        "        return self.relu(out)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# InceptionTime Model\n",
        "# ---------------------------\n",
        "class InceptionTime(nn.Module):\n",
        "    def __init__(self, n_steps, n_length, input_dim, n_classes, num_blocks=3, out_channels=32):\n",
        "        super(InceptionTime, self).__init__()\n",
        "\n",
        "        in_channels = input_dim\n",
        "        blocks = []\n",
        "        for _ in range(num_blocks):\n",
        "            blocks.append(InceptionBlock(in_channels, out_channels))\n",
        "            in_channels = out_channels * 4  # since 3 convs + 1 maxpool branch\n",
        "        self.inception_blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(in_channels, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, n_steps, n_length, input_dim)\n",
        "        x = x.permute(0, 3, 1, 2)   # (B, in_ch, steps, len)\n",
        "        batch, in_ch, steps, seq_len = x.size()\n",
        "        x = x.reshape(batch * steps, in_ch, seq_len)  # treat each step separately\n",
        "\n",
        "        # Inception blocks\n",
        "        x = self.inception_blocks(x)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = self.gap(x).squeeze(-1)\n",
        "\n",
        "        # Reshape back to (batch, steps, features)\n",
        "        x = x.reshape(batch, steps, -1)\n",
        "\n",
        "        # Take last step (or could use mean over steps)\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # FC layer\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Train general model\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "general_model = InceptionTime(\n",
        "    n_steps=n_steps,\n",
        "    n_length=n_length,\n",
        "    input_dim=X_train.shape[3],\n",
        "    n_classes=n_classes,\n",
        "    num_blocks=3,\n",
        "    out_channels=32\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# Label smoothing loss\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "optimizer = torch.optim.Adam(general_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "early_stop_patience = 10\n",
        "\n",
        "for epoch in range(epochs_general):\n",
        "    # --- Train ---\n",
        "    general_model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = general_model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(general_model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation ---\n",
        "    general_model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = general_model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    val_loss = val_loss / len(test_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs_general} \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_weights = general_model.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stop_patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "\n",
        "# restore best weights\n",
        "general_model.load_state_dict(best_weights)\n",
        "\n",
        "# Save weights of general model\n",
        "general_weights = general_model.state_dict()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdYTfXDDCE9V",
        "outputId": "20f4ee32-bb0c-4717-82b8-2acee53b08a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded all subjects together: 10299 samples\n",
            "ðŸ“Š Final split -> Train: (8239, 128, 9), Test: (2060, 128, 9)\n",
            "Classes found: [1 2 3 4 5 6]\n",
            "Epoch 1/30 Train Loss: 0.8657, Train Acc: 0.7725 | Val Loss: 0.7018, Val Acc: 0.8583\n",
            "Epoch 2/30 Train Loss: 0.6409, Train Acc: 0.8972 | Val Loss: 0.6072, Val Acc: 0.9146\n",
            "Epoch 3/30 Train Loss: 0.5946, Train Acc: 0.9175 | Val Loss: 0.5892, Val Acc: 0.9117\n",
            "Epoch 4/30 Train Loss: 0.5793, Train Acc: 0.9245 | Val Loss: 0.5766, Val Acc: 0.9335\n",
            "Epoch 5/30 Train Loss: 0.5692, Train Acc: 0.9302 | Val Loss: 0.5579, Val Acc: 0.9350\n",
            "Epoch 6/30 Train Loss: 0.5541, Train Acc: 0.9363 | Val Loss: 0.5545, Val Acc: 0.9398\n",
            "Epoch 7/30 Train Loss: 0.5468, Train Acc: 0.9391 | Val Loss: 0.5546, Val Acc: 0.9350\n",
            "Epoch 8/30 Train Loss: 0.5396, Train Acc: 0.9415 | Val Loss: 0.5327, Val Acc: 0.9485\n",
            "Epoch 9/30 Train Loss: 0.5317, Train Acc: 0.9437 | Val Loss: 0.5327, Val Acc: 0.9490\n",
            "Epoch 10/30 Train Loss: 0.5353, Train Acc: 0.9448 | Val Loss: 0.5384, Val Acc: 0.9398\n",
            "Epoch 11/30 Train Loss: 0.5284, Train Acc: 0.9494 | Val Loss: 0.5458, Val Acc: 0.9335\n",
            "Epoch 12/30 Train Loss: 0.5256, Train Acc: 0.9481 | Val Loss: 0.5283, Val Acc: 0.9505\n",
            "Epoch 13/30 Train Loss: 0.5213, Train Acc: 0.9498 | Val Loss: 0.5267, Val Acc: 0.9485\n",
            "Epoch 14/30 Train Loss: 0.5200, Train Acc: 0.9517 | Val Loss: 0.5142, Val Acc: 0.9534\n",
            "Epoch 15/30 Train Loss: 0.5176, Train Acc: 0.9540 | Val Loss: 0.5266, Val Acc: 0.9519\n",
            "Epoch 16/30 Train Loss: 0.5147, Train Acc: 0.9523 | Val Loss: 0.5179, Val Acc: 0.9534\n",
            "Epoch 17/30 Train Loss: 0.5186, Train Acc: 0.9506 | Val Loss: 0.5260, Val Acc: 0.9442\n",
            "Epoch 18/30 Train Loss: 0.5132, Train Acc: 0.9555 | Val Loss: 0.5139, Val Acc: 0.9568\n",
            "Epoch 19/30 Train Loss: 0.5135, Train Acc: 0.9525 | Val Loss: 0.5268, Val Acc: 0.9461\n",
            "Epoch 20/30 Train Loss: 0.5174, Train Acc: 0.9501 | Val Loss: 0.5114, Val Acc: 0.9549\n",
            "Epoch 21/30 Train Loss: 0.5076, Train Acc: 0.9548 | Val Loss: 0.5047, Val Acc: 0.9583\n",
            "Epoch 22/30 Train Loss: 0.5085, Train Acc: 0.9565 | Val Loss: 0.5256, Val Acc: 0.9519\n",
            "Epoch 23/30 Train Loss: 0.5151, Train Acc: 0.9550 | Val Loss: 0.5159, Val Acc: 0.9553\n",
            "Epoch 24/30 Train Loss: 0.5113, Train Acc: 0.9538 | Val Loss: 0.5219, Val Acc: 0.9563\n",
            "Epoch 25/30 Train Loss: 0.5069, Train Acc: 0.9553 | Val Loss: 0.5276, Val Acc: 0.9471\n",
            "Epoch 26/30 Train Loss: 0.4998, Train Acc: 0.9589 | Val Loss: 0.5001, Val Acc: 0.9568\n",
            "Epoch 27/30 Train Loss: 0.4979, Train Acc: 0.9586 | Val Loss: 0.5026, Val Acc: 0.9583\n",
            "Epoch 28/30 Train Loss: 0.4961, Train Acc: 0.9587 | Val Loss: 0.5009, Val Acc: 0.9573\n",
            "Epoch 29/30 Train Loss: 0.4960, Train Acc: 0.9593 | Val Loss: 0.5038, Val Acc: 0.9529\n",
            "Epoch 30/30 Train Loss: 0.4973, Train Acc: 0.9589 | Val Loss: 0.5040, Val Acc: 0.9597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLSTM-FCN"
      ],
      "metadata": {
        "id": "Va0-OK3PzM1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# MLSTM-FCN model\n",
        "# ---------------------------\n",
        "class MLSTM_FCN(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, lstm_hidden=128):\n",
        "        super(MLSTM_FCN, self).__init__()\n",
        "\n",
        "        # LSTM branch\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=n_channels,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # FCN branch\n",
        "        self.conv1 = nn.Conv1d(in_channels=n_channels, out_channels=128, kernel_size=8, padding=4)\n",
        "        self.bn1   = nn.BatchNorm1d(128)\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5, padding=2)\n",
        "        self.bn2   = nn.BatchNorm1d(256)\n",
        "        self.conv3 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm1d(128)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Fully connected classifier\n",
        "        self.fc = nn.Linear(lstm_hidden + 128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, timesteps, channels)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # LSTM branch\n",
        "        lstm_out, _ = self.lstm(x)            # (batch, timesteps, lstm_hidden)\n",
        "        lstm_out = lstm_out[:, -1, :]         # take last timestep\n",
        "\n",
        "        # FCN branch\n",
        "        fcn_x = x.permute(0, 2, 1)            # (batch, channels, timesteps)\n",
        "        fcn_x = F.relu(self.bn1(self.conv1(fcn_x)))\n",
        "        fcn_x = F.relu(self.bn2(self.conv2(fcn_x)))\n",
        "        fcn_x = F.relu(self.bn3(self.conv3(fcn_x)))\n",
        "        fcn_x = self.global_pool(fcn_x)       # (batch, channels, 1)\n",
        "        fcn_x = fcn_x.squeeze(-1)             # (batch, channels)\n",
        "\n",
        "        # Concatenate branches\n",
        "        out = torch.cat([lstm_out, fcn_x], dim=1)\n",
        "\n",
        "        # FC classifier\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Example usage\n",
        "# ---------------------------\n",
        "\n",
        "# Reshape your dataset for MLSTM-FCN: (samples, timesteps, channels)\n",
        "X_train_mlfcn = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2], X_train.shape[3])\n",
        "X_test_mlfcn  = X_test.reshape(X_test.shape[0],  X_test.shape[1]*X_test.shape[2],  X_test.shape[3])\n",
        "\n",
        "# Convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train_mlfcn, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "X_test_tensor  = torch.tensor(X_test_mlfcn, dtype=torch.float32)\n",
        "y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# DataLoaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader   = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ---------------------------\n",
        "# Train MLSTM-FCN\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MLSTM_FCN(n_channels=X_train_tensor.shape[2], n_classes=len(le.classes_)).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "    train_acc = correct / total\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    val_acc = correct / total\n",
        "    val_loss /= len(test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs_general} \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYO-vw-4NY33",
        "outputId": "c985c2f5-5d4d-491a-e2af-4c945782037a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 Train Loss: 0.2822, Train Acc: 0.9125 | Val Loss: 0.1710, Val Acc: 0.9311\n",
            "Epoch 2/30 Train Loss: 0.1441, Train Acc: 0.9440 | Val Loss: 0.1345, Val Acc: 0.9432\n",
            "Epoch 3/30 Train Loss: 0.1461, Train Acc: 0.9410 | Val Loss: 0.1149, Val Acc: 0.9544\n",
            "Epoch 4/30 Train Loss: 0.1245, Train Acc: 0.9494 | Val Loss: 0.1267, Val Acc: 0.9417\n",
            "Epoch 5/30 Train Loss: 0.1224, Train Acc: 0.9485 | Val Loss: 0.1205, Val Acc: 0.9490\n",
            "Epoch 6/30 Train Loss: 0.1277, Train Acc: 0.9460 | Val Loss: 0.1061, Val Acc: 0.9578\n",
            "Epoch 7/30 Train Loss: 0.1217, Train Acc: 0.9481 | Val Loss: 0.1014, Val Acc: 0.9583\n",
            "Epoch 8/30 Train Loss: 0.1149, Train Acc: 0.9525 | Val Loss: 0.1017, Val Acc: 0.9568\n",
            "Epoch 9/30 Train Loss: 0.1179, Train Acc: 0.9510 | Val Loss: 0.1033, Val Acc: 0.9636\n",
            "Epoch 10/30 Train Loss: 0.1134, Train Acc: 0.9495 | Val Loss: 0.1130, Val Acc: 0.9578\n",
            "Epoch 11/30 Train Loss: 0.1236, Train Acc: 0.9504 | Val Loss: 0.1113, Val Acc: 0.9510\n",
            "Epoch 12/30 Train Loss: 0.1129, Train Acc: 0.9519 | Val Loss: 0.1004, Val Acc: 0.9568\n",
            "Epoch 13/30 Train Loss: 0.1086, Train Acc: 0.9533 | Val Loss: 0.1091, Val Acc: 0.9485\n",
            "Epoch 14/30 Train Loss: 0.1096, Train Acc: 0.9552 | Val Loss: 0.1191, Val Acc: 0.9500\n",
            "Epoch 15/30 Train Loss: 0.1084, Train Acc: 0.9533 | Val Loss: 0.0968, Val Acc: 0.9621\n",
            "Epoch 16/30 Train Loss: 0.1067, Train Acc: 0.9535 | Val Loss: 0.1139, Val Acc: 0.9461\n",
            "Epoch 17/30 Train Loss: 0.1004, Train Acc: 0.9569 | Val Loss: 0.0871, Val Acc: 0.9660\n",
            "Epoch 18/30 Train Loss: 0.1047, Train Acc: 0.9551 | Val Loss: 0.0932, Val Acc: 0.9597\n",
            "Epoch 19/30 Train Loss: 0.1023, Train Acc: 0.9552 | Val Loss: 0.0906, Val Acc: 0.9675\n",
            "Epoch 20/30 Train Loss: 0.0996, Train Acc: 0.9579 | Val Loss: 0.0857, Val Acc: 0.9675\n",
            "Epoch 21/30 Train Loss: 0.0984, Train Acc: 0.9580 | Val Loss: 0.0857, Val Acc: 0.9694\n",
            "Epoch 22/30 Train Loss: 0.0957, Train Acc: 0.9591 | Val Loss: 0.0964, Val Acc: 0.9660\n",
            "Epoch 23/30 Train Loss: 0.0988, Train Acc: 0.9573 | Val Loss: 0.0847, Val Acc: 0.9665\n",
            "Epoch 24/30 Train Loss: 0.0918, Train Acc: 0.9602 | Val Loss: 0.0858, Val Acc: 0.9694\n",
            "Epoch 25/30 Train Loss: 0.0901, Train Acc: 0.9607 | Val Loss: 0.0956, Val Acc: 0.9592\n",
            "Epoch 26/30 Train Loss: 0.0972, Train Acc: 0.9585 | Val Loss: 0.0829, Val Acc: 0.9675\n",
            "Epoch 27/30 Train Loss: 0.0932, Train Acc: 0.9609 | Val Loss: 0.0868, Val Acc: 0.9602\n",
            "Epoch 28/30 Train Loss: 0.0917, Train Acc: 0.9623 | Val Loss: 0.0808, Val Acc: 0.9684\n",
            "Epoch 29/30 Train Loss: 0.0914, Train Acc: 0.9602 | Val Loss: 0.0766, Val Acc: 0.9694\n",
            "Epoch 30/30 Train Loss: 0.0908, Train Acc: 0.9606 | Val Loss: 0.0825, Val Acc: 0.9704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TCN"
      ],
      "metadata": {
        "id": "ttYTTSjtzQ4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration\n",
        "# ---------------------------\n",
        "batch_size = 32\n",
        "epochs = 30\n",
        "learning_rate = 0.0005\n",
        "dropout = 0.3\n",
        "n_hidden = 64\n",
        "kernel_size = 3\n",
        "n_steps = 4\n",
        "n_length = 32\n",
        "\n",
        "# ---------------------------\n",
        "# Load dataset (merged subjects, 80/20 split)\n",
        "# ---------------------------\n",
        "# X_train, y_train, X_test, y_test = load_data(dataset_path, test_size=0.2, seed=42)\n",
        "# Example: make sure X_train has shape (num_samples, n_steps, n_length, n_channels)\n",
        "# For demonstration, you must replace the above line with your actual data loading function\n",
        "\n",
        "# ---------------------------\n",
        "# Encode labels\n",
        "# ---------------------------\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded  = le.transform(y_test)\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Classes found:\", le.classes_)\n",
        "\n",
        "# Reshape into (batch, channels, sequence_length)\n",
        "X_train = X_train.reshape((X_train.shape[0], n_steps * X_train.shape[2], X_train.shape[3]))\n",
        "X_test  = X_test.reshape((X_test.shape[0], n_steps * X_test.shape[2], X_test.shape[3]))\n",
        "X_train = X_train.transpose(0, 2, 1)  # (batch, channels, seq_len)\n",
        "X_test  = X_test.transpose(0, 2, 1)\n",
        "\n",
        "# Convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# Datasets & loaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ---------------------------\n",
        "# Define TCN model\n",
        "# ---------------------------\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding, dropout):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                               stride=stride, padding=padding, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
        "                               stride=stride, padding=padding, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) \\\n",
        "            if in_channels != out_channels else None\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout2(out)\n",
        "\n",
        "        # Crop output to match input length\n",
        "        if out.size(2) > x.size(2):\n",
        "            out = out[:, :, :x.size(2)]\n",
        "\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        if res.size(2) != out.size(2):\n",
        "            res = res[:, :, :out.size(2)]\n",
        "\n",
        "        return self.relu(out + res)\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size, dropout, n_classes):\n",
        "        super(TCN, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            padding = (kernel_size - 1) * dilation_size\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size,\n",
        "                                     stride=1, dilation=dilation_size, padding=padding, dropout=dropout)]\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self.fc = nn.Linear(num_channels[-1], n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y1 = self.network(x)\n",
        "        y1 = y1[:, :, -1]  # take last time step\n",
        "        return self.fc(y1)\n",
        "\n",
        "# ---------------------------\n",
        "# Initialize model\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_channels = [64, 64, 64]  # three TCN layers\n",
        "model = TCN(num_inputs=X_train_tensor.shape[1], num_channels=num_channels,\n",
        "            kernel_size=kernel_size, dropout=dropout, n_classes=n_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# ---------------------------\n",
        "# Training loop\n",
        "# ---------------------------\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    val_loss = val_loss / len(test_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKlbOYV7Nm3P",
        "outputId": "b0f04d0a-e239-4392-8e2a-e0504829edb5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes found: [1 2 3 4 5 6]\n",
            "Epoch 1/30 Train Loss: 0.8867, Train Acc: 0.6438 | Val Loss: 0.5043, Val Acc: 0.8053\n",
            "Epoch 2/30 Train Loss: 0.5187, Train Acc: 0.7912 | Val Loss: 0.2954, Val Acc: 0.9049\n",
            "Epoch 3/30 Train Loss: 0.3792, Train Acc: 0.8582 | Val Loss: 0.2319, Val Acc: 0.9155\n",
            "Epoch 4/30 Train Loss: 0.3046, Train Acc: 0.8835 | Val Loss: 0.1822, Val Acc: 0.9311\n",
            "Epoch 5/30 Train Loss: 0.2672, Train Acc: 0.8967 | Val Loss: 0.1568, Val Acc: 0.9422\n",
            "Epoch 6/30 Train Loss: 0.2454, Train Acc: 0.9038 | Val Loss: 0.1577, Val Acc: 0.9374\n",
            "Epoch 7/30 Train Loss: 0.2217, Train Acc: 0.9143 | Val Loss: 0.1458, Val Acc: 0.9442\n",
            "Epoch 8/30 Train Loss: 0.2085, Train Acc: 0.9195 | Val Loss: 0.1421, Val Acc: 0.9408\n",
            "Epoch 9/30 Train Loss: 0.1994, Train Acc: 0.9240 | Val Loss: 0.1387, Val Acc: 0.9437\n",
            "Epoch 10/30 Train Loss: 0.1993, Train Acc: 0.9244 | Val Loss: 0.1368, Val Acc: 0.9481\n",
            "Epoch 11/30 Train Loss: 0.1849, Train Acc: 0.9271 | Val Loss: 0.1329, Val Acc: 0.9471\n",
            "Epoch 12/30 Train Loss: 0.1775, Train Acc: 0.9295 | Val Loss: 0.1311, Val Acc: 0.9466\n",
            "Epoch 13/30 Train Loss: 0.1833, Train Acc: 0.9264 | Val Loss: 0.1331, Val Acc: 0.9476\n",
            "Epoch 14/30 Train Loss: 0.1688, Train Acc: 0.9343 | Val Loss: 0.1278, Val Acc: 0.9466\n",
            "Epoch 15/30 Train Loss: 0.1666, Train Acc: 0.9346 | Val Loss: 0.1309, Val Acc: 0.9519\n",
            "Epoch 16/30 Train Loss: 0.1697, Train Acc: 0.9335 | Val Loss: 0.1320, Val Acc: 0.9447\n",
            "Epoch 17/30 Train Loss: 0.1614, Train Acc: 0.9356 | Val Loss: 0.1230, Val Acc: 0.9515\n",
            "Epoch 18/30 Train Loss: 0.1582, Train Acc: 0.9359 | Val Loss: 0.1249, Val Acc: 0.9466\n",
            "Epoch 19/30 Train Loss: 0.1623, Train Acc: 0.9374 | Val Loss: 0.1227, Val Acc: 0.9534\n",
            "Epoch 20/30 Train Loss: 0.1573, Train Acc: 0.9364 | Val Loss: 0.1193, Val Acc: 0.9529\n",
            "Epoch 21/30 Train Loss: 0.1504, Train Acc: 0.9372 | Val Loss: 0.1177, Val Acc: 0.9549\n",
            "Epoch 22/30 Train Loss: 0.1487, Train Acc: 0.9405 | Val Loss: 0.1245, Val Acc: 0.9437\n",
            "Epoch 23/30 Train Loss: 0.1588, Train Acc: 0.9362 | Val Loss: 0.1204, Val Acc: 0.9510\n",
            "Epoch 24/30 Train Loss: 0.1505, Train Acc: 0.9368 | Val Loss: 0.1251, Val Acc: 0.9485\n",
            "Epoch 25/30 Train Loss: 0.1494, Train Acc: 0.9421 | Val Loss: 0.1215, Val Acc: 0.9510\n",
            "Epoch 26/30 Train Loss: 0.1481, Train Acc: 0.9388 | Val Loss: 0.1227, Val Acc: 0.9515\n",
            "Epoch 27/30 Train Loss: 0.1453, Train Acc: 0.9393 | Val Loss: 0.1251, Val Acc: 0.9495\n",
            "Epoch 28/30 Train Loss: 0.1362, Train Acc: 0.9451 | Val Loss: 0.1178, Val Acc: 0.9529\n",
            "Epoch 29/30 Train Loss: 0.1429, Train Acc: 0.9433 | Val Loss: 0.1196, Val Acc: 0.9539\n",
            "Epoch 30/30 Train Loss: 0.1377, Train Acc: 0.9450 | Val Loss: 0.1128, Val Acc: 0.9524\n"
          ]
        }
      ]
    }
  ]
}