{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4esodIWl3Dgi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Importing libraries\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, TimeDistributed, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDqiftzT3VyX",
        "outputId": "8d96e067-f833-4682-aa9c-85ce23915f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Centralized CNN (merge all subjects' data in a pool)"
      ],
      "metadata": {
        "id": "YaKdn6e9Kdjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, clone_model\n",
        "from tensorflow.keras.layers import TimeDistributed, Conv1D, MaxPooling1D, Dropout, Flatten, LSTM, Dense, InputLayer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "dataset_path = \"/content/drive/MyDrive/UCI_HAR_Dataset\"\n",
        "batch_size = 32\n",
        "epochs_general = 30   # epochs for general model\n",
        "epochs_finetune = 5   # epochs for per-subject fine-tuning\n",
        "n_steps, n_length = 4, 32\n",
        "n_hidden = 16\n",
        "\n",
        "SIGNALS = [\n",
        "    \"body_acc_x\", \"body_acc_y\", \"body_acc_z\",\n",
        "    \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\",\n",
        "    \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"\n",
        "]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(dataset_path, test_size=0.2, seed=42):\n",
        "    def _read_csv(filename):\n",
        "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "    def load_signals(subset):\n",
        "        signals_data = []\n",
        "        for signal in SIGNALS:\n",
        "            filename = f\"{dataset_path}/{subset}/Inertial Signals/{signal}_{subset}.txt\"\n",
        "            signals_data.append(_read_csv(filename).to_numpy())\n",
        "        return np.transpose(signals_data, (1, 2, 0))  # (samples, timesteps=128, 9 signals)\n",
        "\n",
        "    def load_y(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/y_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    def load_subjects(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/subject_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    # Load full dataset (train + test)\n",
        "    X_train, y_train, subj_train = load_signals(\"train\"), load_y(\"train\"), load_subjects(\"train\")\n",
        "    X_test,  y_test,  subj_test  = load_signals(\"test\"),  load_y(\"test\"),  load_subjects(\"test\")\n",
        "\n",
        "    # Merge all\n",
        "    X_all = np.vstack([X_train, X_test])\n",
        "    y_all = np.concatenate([y_train, y_test])\n",
        "    subjects = np.concatenate([subj_train, subj_test])  # not used anymore\n",
        "\n",
        "    print(f\"âœ… Loaded all subjects together: {X_all.shape[0]} samples\")\n",
        "\n",
        "    # Shuffle + split (ignore subject IDs)\n",
        "    train_X, test_X, train_y, test_y = train_test_split(\n",
        "        X_all, y_all, test_size=test_size, random_state=seed, shuffle=True, stratify=y_all\n",
        "    )\n",
        "\n",
        "    print(f\"ðŸ“Š Final split -> Train: {train_X.shape}, Test: {test_X.shape}\")\n",
        "\n",
        "    return train_X, train_y, test_X, test_y\n"
      ],
      "metadata": {
        "id": "bjw74z0L3c2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ---------------------------\n",
        "# Load dataset (merged subjects, 80/20 split)\n",
        "# ---------------------------\n",
        "X_train, y_train, X_test, y_test = load_data(dataset_path, test_size=0.2, seed=42)\n",
        "\n",
        "# ---------------------------\n",
        "# Encode labels\n",
        "# ---------------------------\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded  = le.transform(y_test)\n",
        "\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Classes found:\", le.classes_)\n",
        "\n",
        "# reshape into subsequences for CNN-GRU\n",
        "X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, X_train.shape[2]))\n",
        "X_test  = X_test.reshape((X_test.shape[0],  n_steps, n_length, X_test.shape[2]))\n",
        "\n",
        "# convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Define pure CNN model\n",
        "# ---------------------------\n",
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, n_steps, n_length, input_dim, n_classes):\n",
        "        super(CNN1D, self).__init__()\n",
        "\n",
        "        # Conv layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=5, padding=2)\n",
        "        self.bn1   = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
        "        self.bn2   = nn.BatchNorm1d(128)\n",
        "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.pool    = nn.AdaptiveMaxPool1d(1)  # global pooling\n",
        "\n",
        "        # FC layer\n",
        "        self.fc = nn.Linear(256, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, n_steps, n_length, input_dim)\n",
        "        # Merge steps and time\n",
        "        x = x.reshape(x.size(0), x.size(1) * x.size(2), x.size(3))  # (B, steps*len, channels)\n",
        "        x = x.permute(0, 2, 1)  # (B, channels=input_dim, seq_len)\n",
        "\n",
        "        # Conv stack\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Global pooling\n",
        "        x = self.pool(x).squeeze(-1)  # (B, 256)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Train general model\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "general_model = CNN1D(n_steps, n_length, X_train.shape[3], n_classes).to(device)\n",
        "\n",
        "\n",
        "# Label smoothing loss\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "optimizer = torch.optim.Adam(general_model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "early_stop_patience = 10\n",
        "\n",
        "for epoch in range(epochs_general):\n",
        "    # --- Train ---\n",
        "    general_model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = general_model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(general_model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation ---\n",
        "    general_model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = general_model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    val_loss = val_loss / len(test_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs_general} \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_weights = general_model.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stop_patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "\n",
        "# restore best weights\n",
        "general_model.load_state_dict(best_weights)\n",
        "\n",
        "# Save weights of general model\n",
        "general_weights = general_model.state_dict()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0kw2brg3qNV",
        "outputId": "8f79b4d5-e0ed-4653-f8f6-4789710ced3a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded all subjects together: 10299 samples\n",
            "ðŸ“Š Final split -> Train: (8239, 128, 9), Test: (2060, 128, 9)\n",
            "Classes found: [1 2 3 4 5 6]\n",
            "Epoch 1/30 Train Loss: 0.6289, Train Acc: 0.9183 | Val Loss: 0.6986, Val Acc: 0.9417\n",
            "Epoch 2/30 Train Loss: 0.5304, Train Acc: 0.9539 | Val Loss: 0.6722, Val Acc: 0.9684\n",
            "Epoch 3/30 Train Loss: 0.5103, Train Acc: 0.9640 | Val Loss: 0.6657, Val Acc: 0.9680\n",
            "Epoch 4/30 Train Loss: 0.5023, Train Acc: 0.9641 | Val Loss: 0.6806, Val Acc: 0.9709\n",
            "Epoch 5/30 Train Loss: 0.4975, Train Acc: 0.9664 | Val Loss: 0.6956, Val Acc: 0.9748\n",
            "Epoch 6/30 Train Loss: 0.4935, Train Acc: 0.9692 | Val Loss: 0.6748, Val Acc: 0.9743\n",
            "Epoch 7/30 Train Loss: 0.4885, Train Acc: 0.9722 | Val Loss: 0.6785, Val Acc: 0.9801\n",
            "Epoch 8/30 Train Loss: 0.4784, Train Acc: 0.9769 | Val Loss: 0.6674, Val Acc: 0.9796\n",
            "Epoch 9/30 Train Loss: 0.4783, Train Acc: 0.9762 | Val Loss: 0.6640, Val Acc: 0.9825\n",
            "Epoch 10/30 Train Loss: 0.4770, Train Acc: 0.9763 | Val Loss: 0.6732, Val Acc: 0.9820\n",
            "Epoch 11/30 Train Loss: 0.4715, Train Acc: 0.9799 | Val Loss: 0.6718, Val Acc: 0.9772\n",
            "Epoch 12/30 Train Loss: 0.4720, Train Acc: 0.9802 | Val Loss: 0.6578, Val Acc: 0.9820\n",
            "Epoch 13/30 Train Loss: 0.4715, Train Acc: 0.9788 | Val Loss: 0.6660, Val Acc: 0.9840\n",
            "Epoch 14/30 Train Loss: 0.4708, Train Acc: 0.9806 | Val Loss: 0.6685, Val Acc: 0.9835\n",
            "Epoch 15/30 Train Loss: 0.4694, Train Acc: 0.9819 | Val Loss: 0.6675, Val Acc: 0.9859\n",
            "Epoch 16/30 Train Loss: 0.4676, Train Acc: 0.9824 | Val Loss: 0.6568, Val Acc: 0.9816\n",
            "Epoch 17/30 Train Loss: 0.4676, Train Acc: 0.9819 | Val Loss: 0.6705, Val Acc: 0.9840\n",
            "Epoch 18/30 Train Loss: 0.4666, Train Acc: 0.9822 | Val Loss: 0.6827, Val Acc: 0.9806\n",
            "Epoch 19/30 Train Loss: 0.4651, Train Acc: 0.9840 | Val Loss: 0.6549, Val Acc: 0.9840\n",
            "Epoch 20/30 Train Loss: 0.4642, Train Acc: 0.9862 | Val Loss: 0.6659, Val Acc: 0.9850\n",
            "Epoch 21/30 Train Loss: 0.4625, Train Acc: 0.9845 | Val Loss: 0.6624, Val Acc: 0.9796\n",
            "Epoch 22/30 Train Loss: 0.4626, Train Acc: 0.9852 | Val Loss: 0.6752, Val Acc: 0.9864\n",
            "Epoch 23/30 Train Loss: 0.4625, Train Acc: 0.9856 | Val Loss: 0.6628, Val Acc: 0.9859\n",
            "Epoch 24/30 Train Loss: 0.4577, Train Acc: 0.9885 | Val Loss: 0.6632, Val Acc: 0.9864\n",
            "Epoch 25/30 Train Loss: 0.4562, Train Acc: 0.9885 | Val Loss: 0.6688, Val Acc: 0.9869\n",
            "Epoch 26/30 Train Loss: 0.4558, Train Acc: 0.9885 | Val Loss: 0.6678, Val Acc: 0.9879\n",
            "Epoch 27/30 Train Loss: 0.4549, Train Acc: 0.9886 | Val Loss: 0.6724, Val Acc: 0.9859\n",
            "Epoch 28/30 Train Loss: 0.4524, Train Acc: 0.9899 | Val Loss: 0.6677, Val Acc: 0.9869\n",
            "Epoch 29/30 Train Loss: 0.4519, Train Acc: 0.9897 | Val Loss: 0.6647, Val Acc: 0.9888\n",
            "Early stopping triggered!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_by_subject(dataset_path):\n",
        "    def _read_csv(filename):\n",
        "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "    def load_signals(subset):\n",
        "        signals_data = []\n",
        "        for signal in SIGNALS:\n",
        "            filename = f\"{dataset_path}/{subset}/Inertial Signals/{signal}_{subset}.txt\"\n",
        "            signals_data.append(_read_csv(filename).to_numpy())\n",
        "        return np.transpose(signals_data, (1, 2, 0))  # (samples, timesteps=128, 9 signals)\n",
        "\n",
        "    def load_y(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/y_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    def load_subjects(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/subject_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    # Load train & test partitions\n",
        "    X_train, y_train, subj_train = load_signals(\"train\"), load_y(\"train\"), load_subjects(\"train\")\n",
        "    X_test,  y_test,  subj_test  = load_signals(\"test\"),  load_y(\"test\"),  load_subjects(\"test\")\n",
        "\n",
        "    # Merge everything\n",
        "    X_all = np.vstack([X_train, X_test])\n",
        "    y_all = np.concatenate([y_train, y_test])\n",
        "    subjects = np.concatenate([subj_train, subj_test])\n",
        "\n",
        "    # Organize by subject\n",
        "    subject_dict = {}\n",
        "    for subj_id in np.unique(subjects):\n",
        "        mask = (subjects == subj_id)\n",
        "        subject_dict[subj_id] = (X_all[mask], y_all[mask])\n",
        "\n",
        "    print(f\"âœ… Loaded dataset: {X_all.shape[0]} samples from {len(subject_dict)} subjects\")\n",
        "    return subject_dict"
      ],
      "metadata": {
        "id": "UqEl19g36Zfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load per-subject data\n",
        "subject_data = load_data_by_subject(dataset_path)\n",
        "\n",
        "# Run inference for one subject (e.g., subject 5)\n",
        "X_subj, y_subj = subject_data[5]\n",
        "print(\"Subject 5 ->\", X_subj.shape, y_subj.shape)\n",
        "\n",
        "# Later: use your trained model\n",
        "# preds = model.predict(X_subj)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSOJXBhT6jv9",
        "outputId": "37ac1688-a827-4420-f41e-8a061eeec8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded dataset: 10299 samples from 30 subjects\n",
            "Subject 5 -> (302, 128, 9) (302,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate_per_subject(general_model, subject_data, le, n_steps, n_length, device=\"cuda\"):\n",
        "    general_model.eval()\n",
        "    subject_accuracies = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for subj_id, (X_subj, y_subj) in subject_data.items():\n",
        "            # reshape into subsequences like training\n",
        "            X_subj = X_subj.reshape((X_subj.shape[0], n_steps, n_length, X_subj.shape[2]))\n",
        "\n",
        "            # encode labels with same LabelEncoder\n",
        "            y_subj_encoded = le.transform(y_subj)\n",
        "\n",
        "            # convert to torch tensors\n",
        "            X_tensor = torch.tensor(X_subj, dtype=torch.float32).to(device)\n",
        "            y_tensor = torch.tensor(y_subj_encoded, dtype=torch.long).to(device)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = general_model(X_tensor)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            acc = accuracy_score(y_tensor.cpu().numpy(), preds.cpu().numpy())\n",
        "            subject_accuracies[subj_id] = acc\n",
        "\n",
        "    # Convert to numpy array for stats\n",
        "    acc_values = np.array(list(subject_accuracies.values()))\n",
        "    mean_acc = acc_values.mean()\n",
        "    std_acc = acc_values.std()\n",
        "\n",
        "    print(\"\\nðŸ“Š Per-subject accuracies:\")\n",
        "    for subj_id, acc in subject_accuracies.items():\n",
        "        print(f\"  Subject {subj_id}: {acc:.4f}\")\n",
        "\n",
        "    print(f\"\\nâœ… Mean Accuracy: {mean_acc:.4f}, Std: {std_acc:.4f}\")\n",
        "\n",
        "    return subject_accuracies, mean_acc, std_acc\n"
      ],
      "metadata": {
        "id": "XKkAyeWE8gzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load subject-wise data\n",
        "subject_data = load_data_by_subject(dataset_path)\n",
        "\n",
        "# 2. Evaluate general_model on each subject\n",
        "subject_accuracies, mean_acc, std_acc = evaluate_per_subject(\n",
        "    general_model, subject_data, le, n_steps, n_length, device=device\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je76HEDZ8mA7",
        "outputId": "966c0bde-2ed4-46c5-e829-5a52f2f19bc3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded dataset: 10299 samples from 30 subjects\n",
            "\n",
            "ðŸ“Š Per-subject accuracies:\n",
            "  Subject 1: 1.0000\n",
            "  Subject 2: 0.9636\n",
            "  Subject 3: 1.0000\n",
            "  Subject 4: 0.9937\n",
            "  Subject 5: 0.9834\n",
            "  Subject 6: 0.9969\n",
            "  Subject 7: 0.9870\n",
            "  Subject 8: 0.9537\n",
            "  Subject 9: 0.9861\n",
            "  Subject 10: 0.9762\n",
            "  Subject 11: 1.0000\n",
            "  Subject 12: 1.0000\n",
            "  Subject 13: 1.0000\n",
            "  Subject 14: 1.0000\n",
            "  Subject 15: 1.0000\n",
            "  Subject 16: 0.9781\n",
            "  Subject 17: 0.9946\n",
            "  Subject 18: 1.0000\n",
            "  Subject 19: 1.0000\n",
            "  Subject 20: 1.0000\n",
            "  Subject 21: 1.0000\n",
            "  Subject 22: 1.0000\n",
            "  Subject 23: 0.9973\n",
            "  Subject 24: 1.0000\n",
            "  Subject 25: 0.9756\n",
            "  Subject 26: 1.0000\n",
            "  Subject 27: 1.0000\n",
            "  Subject 28: 0.9764\n",
            "  Subject 29: 0.9942\n",
            "  Subject 30: 1.0000\n",
            "\n",
            "âœ… Mean Accuracy: 0.9919, Std: 0.0122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Domain Generalization"
      ],
      "metadata": {
        "id": "PwVKyATkL5kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "dataset_path = \"/content/drive/MyDrive/UCI_HAR_Dataset\"\n",
        "test_subjects = [2, 5, 7, 8, 9, 10]  # hold-out subjects for testing\n",
        "batch_size = 32\n",
        "epochs_general = 10   # epochs for general model\n",
        "epochs_finetune = 5   # epochs for per-subject fine-tuning\n",
        "n_steps, n_length = 4, 32\n",
        "n_hidden = 16\n",
        "\n",
        "SIGNALS = [\n",
        "    \"body_acc_x\", \"body_acc_y\", \"body_acc_z\",\n",
        "    \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\",\n",
        "    \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"\n",
        "]\n",
        "\n",
        "def load_data(dataset_path, test_subjects):\n",
        "    def _read_csv(filename):\n",
        "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "    def load_signals(subset):\n",
        "        signals_data = []\n",
        "        for signal in SIGNALS:\n",
        "            filename = f\"{dataset_path}/{subset}/Inertial Signals/{signal}_{subset}.txt\"\n",
        "            signals_data.append(_read_csv(filename).to_numpy())\n",
        "        return np.transpose(signals_data, (1, 2, 0))  # (samples, timesteps=128, 9 signals)\n",
        "\n",
        "    def load_y(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/y_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    def load_subjects(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/subject_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    X_train, y_train, subj_train = load_signals(\"train\"), load_y(\"train\"), load_subjects(\"train\")\n",
        "    X_test,  y_test,  subj_test  = load_signals(\"test\"),  load_y(\"test\"),  load_subjects(\"test\")\n",
        "\n",
        "    X_all = np.vstack([X_train, X_test])\n",
        "    y_all = np.concatenate([y_train, y_test])\n",
        "    subjects = np.concatenate([subj_train, subj_test])\n",
        "\n",
        "    # Split by subject IDs\n",
        "    train_mask = ~np.isin(subjects, test_subjects)\n",
        "    test_mask  = np.isin(subjects, test_subjects)\n",
        "\n",
        "    train_X, train_y = X_all[train_mask], y_all[train_mask]\n",
        "    test_X,  test_y  = X_all[test_mask],  y_all[test_mask]\n",
        "    train_subjects   = subjects[train_mask]\n",
        "\n",
        "    print(f\"Selected test subjects: {test_subjects}\")\n",
        "    print(\"Train:\", train_X.shape, train_y.shape)\n",
        "    print(\"Test :\", test_X.shape, test_y.shape)\n",
        "\n",
        "    return train_X, train_y, test_X, test_y, train_subjects\n",
        "\n",
        "X_train, y_train, X_test, y_test, train_subjects = load_data(dataset_path, test_subjects=test_subjects)\n",
        "\n",
        "# ---------------------------\n",
        "# Encode labels\n",
        "# ---------------------------\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded  = le.transform(y_test)\n",
        "\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Classes found:\", le.classes_)\n",
        "\n",
        "# reshape into subsequences for CNN-GRU\n",
        "X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, X_train.shape[2]))\n",
        "X_test  = X_test.reshape((X_test.shape[0],  n_steps, n_length, X_test.shape[2]))\n",
        "\n",
        "# convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISsAt_O7Juvv",
        "outputId": "0dc86b44-ed79-459d-ce9f-bfe882573e25"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected test subjects: [2, 5, 7, 8, 9, 10]\n",
            "Train: (8524, 128, 9) (8524,)\n",
            "Test : (1775, 128, 9) (1775,)\n",
            "Classes found: [1 2 3 4 5 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN**"
      ],
      "metadata": {
        "id": "JPxCst0pLf4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Define pure CNN model\n",
        "# ---------------------------\n",
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, n_steps, n_length, input_dim, n_classes):\n",
        "        super(CNN1D, self).__init__()\n",
        "\n",
        "        # Conv layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=5, padding=2)\n",
        "        self.bn1   = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
        "        self.bn2   = nn.BatchNorm1d(128)\n",
        "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.pool    = nn.AdaptiveMaxPool1d(1)  # global pooling\n",
        "\n",
        "        # FC layer\n",
        "        self.fc = nn.Linear(256, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, n_steps, n_length, input_dim)\n",
        "        # Merge steps and time\n",
        "        x = x.reshape(x.size(0), x.size(1) * x.size(2), x.size(3))  # (B, steps*len, channels)\n",
        "        x = x.permute(0, 2, 1)  # (B, channels=input_dim, seq_len)\n",
        "\n",
        "        # Conv stack\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Global pooling\n",
        "        x = self.pool(x).squeeze(-1)  # (B, 256)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Train general model\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "general_model = CNN1D(n_steps, n_length, X_train.shape[3], n_classes).to(device)\n",
        "\n",
        "\n",
        "# Label smoothing loss\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "optimizer = torch.optim.Adam(general_model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "early_stop_patience = 10\n",
        "\n",
        "for epoch in range(epochs_general):\n",
        "    # --- Train ---\n",
        "    general_model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = general_model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(general_model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation ---\n",
        "    general_model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = general_model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    val_loss = val_loss / len(test_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs_general} \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_weights = general_model.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stop_patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "\n",
        "# restore best weights\n",
        "general_model.load_state_dict(best_weights)\n",
        "\n",
        "# Save weights of general model\n",
        "general_weights = general_model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onlJVemSLdkM",
        "outputId": "59b5e1c4-ccf5-4d57-8306-dc70dba3376a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 Train Loss: 0.6000, Train Acc: 0.9352 | Val Loss: 0.8360, Val Acc: 0.8518\n",
            "Epoch 2/10 Train Loss: 0.5110, Train Acc: 0.9673 | Val Loss: 0.8061, Val Acc: 0.8620\n",
            "Epoch 3/10 Train Loss: 0.4952, Train Acc: 0.9716 | Val Loss: 0.8131, Val Acc: 0.8879\n",
            "Epoch 4/10 Train Loss: 0.4866, Train Acc: 0.9750 | Val Loss: 0.8392, Val Acc: 0.8885\n",
            "Epoch 5/10 Train Loss: 0.4851, Train Acc: 0.9749 | Val Loss: 0.8031, Val Acc: 0.8907\n",
            "Epoch 6/10 Train Loss: 0.4811, Train Acc: 0.9767 | Val Loss: 0.8319, Val Acc: 0.8834\n",
            "Epoch 7/10 Train Loss: 0.4736, Train Acc: 0.9803 | Val Loss: 0.8035, Val Acc: 0.9076\n",
            "Epoch 8/10 Train Loss: 0.4741, Train Acc: 0.9802 | Val Loss: 0.7891, Val Acc: 0.9054\n",
            "Epoch 9/10 Train Loss: 0.4705, Train Acc: 0.9817 | Val Loss: 0.8295, Val Acc: 0.9087\n",
            "Epoch 10/10 Train Loss: 0.4690, Train Acc: 0.9826 | Val Loss: 0.8205, Val Acc: 0.8885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN + Gradient Reversal Layer**"
      ],
      "metadata": {
        "id": "QFnscr9IIOza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "dataset_path = \"/content/drive/MyDrive/UCI_HAR_Dataset\"\n",
        "test_subjects = [2, 5, 7, 8, 9, 10]  # hold-out subjects for testing\n",
        "batch_size = 32\n",
        "epochs_general = 10\n",
        "n_steps, n_length = 4, 32\n",
        "\n",
        "SIGNALS = [\n",
        "    \"body_acc_x\", \"body_acc_y\", \"body_acc_z\",\n",
        "    \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\",\n",
        "    \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# Load data: returns subject ids per sample (train + test)\n",
        "# ---------------------------\n",
        "def load_data(dataset_path, test_subjects):\n",
        "    def _read_csv(filename):\n",
        "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "    def load_signals(subset):\n",
        "        signals_data = []\n",
        "        for signal in SIGNALS:\n",
        "            filename = f\"{dataset_path}/{subset}/Inertial Signals/{signal}_{subset}.txt\"\n",
        "            signals_data.append(_read_csv(filename).to_numpy())\n",
        "        return np.transpose(signals_data, (1, 2, 0))  # (samples, timesteps, channels)\n",
        "\n",
        "    def load_y(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/y_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    def load_subjects(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/subject_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    # Load partitions\n",
        "    X_train_part, y_train_part, subj_train_part = load_signals(\"train\"), load_y(\"train\"), load_subjects(\"train\")\n",
        "    X_test_part,  y_test_part,  subj_test_part  = load_signals(\"test\"),  load_y(\"test\"),  load_subjects(\"test\")\n",
        "\n",
        "    # Merge\n",
        "    X_all = np.vstack([X_train_part, X_test_part])\n",
        "    y_all = np.concatenate([y_train_part, y_test_part])\n",
        "    subjects_all = np.concatenate([subj_train_part, subj_test_part])\n",
        "\n",
        "    # Split by subject IDs (hold-out subjects -> test)\n",
        "    train_mask = ~np.isin(subjects_all, test_subjects)\n",
        "    test_mask  = np.isin(subjects_all, test_subjects)\n",
        "\n",
        "    train_X, train_y, train_subj = X_all[train_mask], y_all[train_mask], subjects_all[train_mask]\n",
        "    test_X,  test_y,  test_subj  = X_all[test_mask],  y_all[test_mask],  subjects_all[test_mask]\n",
        "\n",
        "    print(f\"Selected test subjects: {test_subjects}\")\n",
        "    print(\"Train samples:\", train_X.shape[0], \"Test samples:\", test_X.shape[0])\n",
        "\n",
        "    return train_X, train_y, train_subj, test_X, test_y, test_subj\n",
        "\n",
        "# ---------------------------\n",
        "# Load + preprocess for CNN + GRL\n",
        "# ---------------------------\n",
        "X_train, y_train, train_subj, X_test, y_test, test_subj = load_data(dataset_path, test_subjects=test_subjects)\n",
        "\n",
        "# Encode activity labels\n",
        "le_activity = LabelEncoder()\n",
        "y_train_encoded = le_activity.fit_transform(y_train)\n",
        "y_test_encoded  = le_activity.transform(y_test)\n",
        "n_classes = len(le_activity.classes_)\n",
        "print(\"Activity classes found:\", le_activity.classes_)\n",
        "\n",
        "# Encode training subjects only for GRL\n",
        "le_subjects = LabelEncoder()\n",
        "train_subj_encoded = le_subjects.fit_transform(train_subj)  # only training subjects\n",
        "n_subjects = len(le_subjects.classes_)\n",
        "print(\"Unique subjects in training:\", n_subjects)\n",
        "\n",
        "# Optionally encode test subjects (for analysis only, not used in GRL)\n",
        "# For evaluation, GRL head is ignored\n",
        "test_subj_encoded = np.array([-1]*len(test_subj))  # dummy labels for test\n",
        "\n",
        "# Reshape for CNN input: (samples, n_steps, n_length, channels)\n",
        "expected_timesteps = n_steps * n_length\n",
        "assert X_train.shape[1] == expected_timesteps\n",
        "assert X_test.shape[1]  == expected_timesteps\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, X_train.shape[2]))\n",
        "X_test  = X_test.reshape((X_test.shape[0],  n_steps, n_length, X_test.shape[2]))\n",
        "\n",
        "# Convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "subj_train_tensor = torch.tensor(train_subj_encoded, dtype=torch.long)\n",
        "# Test loader will ignore subject labels\n",
        "subj_test_tensor  = torch.tensor(test_subj_encoded, dtype=torch.long)\n",
        "\n",
        "# Create datasets\n",
        "# Training dataset includes subject labels for GRL\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor, subj_train_tensor)\n",
        "# Test dataset ignores subject labels to prevent IndexError\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Data loaders ready. Train batches:\", len(train_loader), \"Test batches:\", len(test_loader))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBUKRSn5Qz-A",
        "outputId": "686b242f-6e64-489c-d490-625a0f5ff260"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected test subjects: [2, 5, 7, 8, 9, 10]\n",
            "Train samples: 8524 Test samples: 1775\n",
            "Activity classes found: [1 2 3 4 5 6]\n",
            "Unique subjects in training: 24\n",
            "Data loaders ready. Train batches: 267 Test batches: 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# ---------------------------\n",
        "# GRL (Gradient Reversal Layer)\n",
        "# ---------------------------\n",
        "class GradReverse(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambda_):\n",
        "        ctx.lambda_ = lambda_\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return -ctx.lambda_ * grad_output, None\n",
        "\n",
        "def grad_reverse(x, lambda_=1.0):\n",
        "    return GradReverse.apply(x, lambda_)\n",
        "\n",
        "# ---------------------------\n",
        "# Subject-Invariant CNN (uses same CNN architecture as before)\n",
        "# ---------------------------\n",
        "class SubjectInvariantCNN(nn.Module):\n",
        "    def __init__(self, n_steps, n_length, input_dim, n_classes, n_subjects, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.n_steps = n_steps\n",
        "        self.n_length = n_length\n",
        "        self.input_dim = input_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.n_subjects = n_subjects\n",
        "\n",
        "        # Conv backbone (same as your CNN1D)\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=5, padding=2)\n",
        "        self.bn1   = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
        "        self.bn2   = nn.BatchNorm1d(128)\n",
        "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.pool    = nn.AdaptiveMaxPool1d(1)  # global pooling\n",
        "\n",
        "        # projection to hidden_dim used by both heads\n",
        "        self.feature_proj = nn.Linear(256, hidden_dim)\n",
        "\n",
        "        # Activity classifier\n",
        "        self.fc_activity = nn.Linear(hidden_dim, n_classes)\n",
        "\n",
        "        # Subject classifier (adversarial head)\n",
        "        self.fc_subject = nn.Linear(hidden_dim, n_subjects)\n",
        "\n",
        "    def forward(self, x, lambda_adv=1.0):\n",
        "        \"\"\"\n",
        "        x: (batch, n_steps, n_length, channels)\n",
        "        returns: logits_activity, logits_subject\n",
        "        \"\"\"\n",
        "        # Merge steps and length into time axis, then conv expects (B, channels, seq_len)\n",
        "        x = x.reshape(x.size(0), x.size(1) * x.size(2), x.size(3))   # (B, steps*len, channels)\n",
        "        x = x.permute(0, 2, 1)  # (B, channels, seq_len)\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(x).squeeze(-1)  # (B, 256)\n",
        "\n",
        "        features = self.feature_proj(x)  # (B, hidden_dim)\n",
        "\n",
        "        logits_activity = self.fc_activity(features)\n",
        "\n",
        "        # apply GRL for adversarial subject classification\n",
        "        rev_features = grad_reverse(features, lambda_adv)\n",
        "        logits_subject = self.fc_subject(rev_features)\n",
        "\n",
        "        return logits_activity, logits_subject\n",
        "\n",
        "# ---------------------------\n",
        "# Training function (fixed lambda, early stopping based on val accuracy)\n",
        "# ---------------------------\n",
        "def train_cnn_grl(\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    n_steps,\n",
        "    n_length,\n",
        "    input_dim,\n",
        "    n_classes,\n",
        "    n_subjects,\n",
        "    device=None,\n",
        "    epochs=10,\n",
        "    lr=5e-4,\n",
        "    lambda_adv=1.0,  # fixed lambda\n",
        "    weight_decay=1e-4,\n",
        "    early_stop_patience=10,\n",
        "    save_path=\"best_grl_model.pth\"\n",
        "):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SubjectInvariantCNN(n_steps, n_length, input_dim, n_classes, n_subjects).to(device)\n",
        "\n",
        "    criterion_activity = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    criterion_subject  = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_val_acc = 0.0  # track best accuracy\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start = time.time()\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_act_losses = []\n",
        "        train_subj_losses = []\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for X_batch, y_activity, y_subject in train_loader:\n",
        "            X_batch, y_activity, y_subject = X_batch.to(device), y_activity.to(device), y_subject.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits_act, logits_subj = model(X_batch, lambda_adv=lambda_adv)\n",
        "\n",
        "            loss_act = criterion_activity(logits_act, y_activity)\n",
        "            loss_sub = criterion_subject(logits_subj, y_subject)\n",
        "            loss = loss_act - lambda_adv * loss_sub\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            train_act_losses.append(loss_act.item())\n",
        "            train_subj_losses.append(loss_sub.item())\n",
        "            train_correct += (logits_act.argmax(dim=1) == y_activity).sum().item()\n",
        "            train_total += y_activity.size(0)\n",
        "\n",
        "        train_loss = float(np.mean(train_losses))\n",
        "        train_act_loss = float(np.mean(train_act_losses))\n",
        "        train_subj_loss = float(np.mean(train_subj_losses))\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # ---------------------------\n",
        "        # Validation (no GRL)\n",
        "        # ---------------------------\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        val_act_losses = []\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_activity in test_loader:  # only 2 items in test_loader\n",
        "                X_batch, y_activity = X_batch.to(device), y_activity.to(device)\n",
        "                logits_act, _ = model(X_batch, lambda_adv=0.0)\n",
        "                loss_act = criterion_activity(logits_act, y_activity)\n",
        "\n",
        "                val_losses.append(loss_act.item())\n",
        "                val_act_losses.append(loss_act.item())\n",
        "                val_correct += (logits_act.argmax(dim=1) == y_activity).sum().item()\n",
        "                val_total += y_activity.size(0)\n",
        "\n",
        "        val_loss = float(np.mean(val_losses))\n",
        "        val_act_loss = float(np.mean(val_act_losses))\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(f\"Epoch {epoch:03d} | {epoch_time:.1f}s | lambda {lambda_adv:.3f} | \"\n",
        "              f\"Train loss {train_loss:.4f} (act {train_act_loss:.4f}, subj {train_subj_loss:.4f}), acc {train_acc:.4f} | \"\n",
        "              f\"Val loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
        "\n",
        "        # Early stopping + save best based on **val accuracy**\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience = 0\n",
        "            torch.save({\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"val_acc\": val_acc,\n",
        "            }, save_path)\n",
        "            print(f\"  -> Saved best model to {save_path} (val_acc={val_acc:.4f})\")\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= early_stop_patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # Load best model weights\n",
        "    if os.path.exists(save_path):\n",
        "        ckpt = torch.load(save_path, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "        print(f\"Loaded best model from epoch {ckpt.get('epoch')} with val_acc {ckpt.get('val_acc'):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SX0Py9EtM0VP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Setup and training script for CNN+GRL\n",
        "# ---------------------------\n",
        "import torch\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset properties from loaded data\n",
        "# ---------------------------\n",
        "input_dim = X_train.shape[3]    # number of sensor channels (e.g., 9)\n",
        "print(\"Input dim:\", input_dim)\n",
        "\n",
        "n_classes = len(le_activity.classes_)  # activity classes\n",
        "print(\"Activity classes:\", n_classes)\n",
        "\n",
        "n_subjects = len(np.unique(train_subj))  # unique training subjects only\n",
        "print(\"Subjects in training:\", n_subjects)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------------------\n",
        "# Train CNN+GRL\n",
        "# ---------------------------\n",
        "trained_model = train_cnn_grl(\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    n_steps=n_steps,\n",
        "    n_length=n_length,\n",
        "    input_dim=input_dim,\n",
        "    n_classes=n_classes,\n",
        "    n_subjects=n_subjects,\n",
        "    device=device,\n",
        "    epochs=30,\n",
        "    lr=5e-4,\n",
        "    lambda_adv=0,\n",
        "    weight_decay=1e-4,\n",
        "    early_stop_patience=10,\n",
        "    save_path=\"best_grl_model.pth\"\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluate final model on test set\n",
        "# ---------------------------\n",
        "trained_model.eval()\n",
        "test_correct, test_total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_act in test_loader:   # <- only 2 values\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_act   = y_act.to(device)\n",
        "\n",
        "        logits_act, _ = trained_model(X_batch, lambda_adv=0.0)  # no adversarial effect in inference\n",
        "        preds = torch.argmax(logits_act, dim=1)\n",
        "\n",
        "        test_correct += (preds == y_act).sum().item()\n",
        "        test_total += y_act.size(0)\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"âœ… Final Test Accuracy (Activity): {test_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "839Sh-wXUIRa",
        "outputId": "641872cc-2a7a-41f7-d0a0-a4e2a192464d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dim: 9\n",
            "Activity classes: 6\n",
            "Subjects in training: 24\n",
            "Epoch 001 | 33.7s | lambda 0.000 | Train loss 0.6076 (act 0.6076, subj 3.2812), acc 0.9291 | Val loss 0.8185, acc 0.8777\n",
            "  -> Saved best model to best_grl_model.pth (val_acc=0.8777)\n",
            "Epoch 002 | 35.2s | lambda 0.000 | Train loss 0.5163 (act 0.5163, subj 3.1781), acc 0.9636 | Val loss 0.8483, acc 0.8456\n",
            "Epoch 003 | 33.9s | lambda 0.000 | Train loss 0.5127 (act 0.5127, subj 3.1781), acc 0.9640 | Val loss 0.8004, acc 0.8879\n",
            "  -> Saved best model to best_grl_model.pth (val_acc=0.8879)\n",
            "Epoch 004 | 35.2s | lambda 0.000 | Train loss 0.4967 (act 0.4967, subj 3.1781), acc 0.9708 | Val loss 0.8135, acc 0.8992\n",
            "  -> Saved best model to best_grl_model.pth (val_acc=0.8992)\n",
            "Epoch 005 | 34.1s | lambda 0.000 | Train loss 0.4913 (act 0.4913, subj 3.1781), acc 0.9713 | Val loss 0.8555, acc 0.8648\n",
            "Epoch 006 | 35.4s | lambda 0.000 | Train loss 0.4853 (act 0.4853, subj 3.1781), acc 0.9761 | Val loss 0.8141, acc 0.8704\n",
            "Epoch 007 | 34.0s | lambda 0.000 | Train loss 0.4810 (act 0.4810, subj 3.1781), acc 0.9767 | Val loss 0.8251, acc 0.8952\n",
            "Epoch 008 | 35.1s | lambda 0.000 | Train loss 0.4756 (act 0.4756, subj 3.1781), acc 0.9792 | Val loss 0.8336, acc 0.8755\n",
            "Epoch 009 | 34.0s | lambda 0.000 | Train loss 0.4731 (act 0.4731, subj 3.1781), acc 0.9809 | Val loss 0.8124, acc 0.8924\n",
            "Epoch 010 | 35.5s | lambda 0.000 | Train loss 0.4728 (act 0.4728, subj 3.1781), acc 0.9806 | Val loss 0.8120, acc 0.8851\n",
            "Epoch 011 | 34.2s | lambda 0.000 | Train loss 0.4695 (act 0.4695, subj 3.1781), acc 0.9812 | Val loss 0.8330, acc 0.8896\n",
            "Epoch 012 | 35.5s | lambda 0.000 | Train loss 0.4663 (act 0.4663, subj 3.1781), acc 0.9853 | Val loss 0.8183, acc 0.8986\n",
            "Epoch 013 | 34.2s | lambda 0.000 | Train loss 0.4642 (act 0.4642, subj 3.1781), acc 0.9860 | Val loss 0.8145, acc 0.9054\n",
            "  -> Saved best model to best_grl_model.pth (val_acc=0.9054)\n",
            "Epoch 014 | 35.3s | lambda 0.000 | Train loss 0.4649 (act 0.4649, subj 3.1781), acc 0.9844 | Val loss 0.8394, acc 0.8817\n",
            "Epoch 015 | 34.0s | lambda 0.000 | Train loss 0.4610 (act 0.4610, subj 3.1781), acc 0.9877 | Val loss 0.8070, acc 0.9008\n",
            "Epoch 016 | 35.4s | lambda 0.000 | Train loss 0.4640 (act 0.4640, subj 3.1781), acc 0.9866 | Val loss 0.8283, acc 0.8868\n",
            "Epoch 017 | 34.6s | lambda 0.000 | Train loss 0.4596 (act 0.4596, subj 3.1781), acc 0.9880 | Val loss 0.8000, acc 0.9093\n",
            "  -> Saved best model to best_grl_model.pth (val_acc=0.9093)\n",
            "Epoch 018 | 35.3s | lambda 0.000 | Train loss 0.4565 (act 0.4565, subj 3.1781), acc 0.9884 | Val loss 0.8339, acc 0.8885\n",
            "Epoch 019 | 34.1s | lambda 0.000 | Train loss 0.4578 (act 0.4578, subj 3.1781), acc 0.9887 | Val loss 0.8307, acc 0.8986\n",
            "Epoch 020 | 35.3s | lambda 0.000 | Train loss 0.4574 (act 0.4574, subj 3.1781), acc 0.9879 | Val loss 0.8214, acc 0.8980\n",
            "Epoch 021 | 34.7s | lambda 0.000 | Train loss 0.4559 (act 0.4559, subj 3.1781), acc 0.9898 | Val loss 0.8441, acc 0.8879\n",
            "Epoch 022 | 34.6s | lambda 0.000 | Train loss 0.4558 (act 0.4558, subj 3.1781), acc 0.9882 | Val loss 0.8207, acc 0.9121\n",
            "  -> Saved best model to best_grl_model.pth (val_acc=0.9121)\n",
            "Epoch 023 | 35.2s | lambda 0.000 | Train loss 0.4565 (act 0.4565, subj 3.1781), acc 0.9886 | Val loss 0.8465, acc 0.8913\n",
            "Epoch 024 | 34.1s | lambda 0.000 | Train loss 0.4540 (act 0.4540, subj 3.1781), acc 0.9907 | Val loss 0.8345, acc 0.9020\n",
            "Epoch 025 | 35.2s | lambda 0.000 | Train loss 0.4513 (act 0.4513, subj 3.1781), acc 0.9917 | Val loss 0.8416, acc 0.8907\n",
            "Epoch 026 | 33.9s | lambda 0.000 | Train loss 0.4527 (act 0.4527, subj 3.1781), acc 0.9903 | Val loss 0.8036, acc 0.9115\n",
            "Epoch 027 | 35.0s | lambda 0.000 | Train loss 0.4517 (act 0.4517, subj 3.1781), acc 0.9911 | Val loss 0.8208, acc 0.9070\n",
            "Epoch 028 | 34.1s | lambda 0.000 | Train loss 0.4546 (act 0.4546, subj 3.1781), acc 0.9898 | Val loss 0.8182, acc 0.9093\n",
            "Epoch 029 | 35.2s | lambda 0.000 | Train loss 0.4502 (act 0.4502, subj 3.1781), acc 0.9926 | Val loss 0.8184, acc 0.8901\n",
            "Epoch 030 | 34.2s | lambda 0.000 | Train loss 0.4498 (act 0.4498, subj 3.1781), acc 0.9920 | Val loss 0.8438, acc 0.8924\n",
            "Loaded best model from epoch 22 with val_acc 0.9121\n",
            "âœ… Final Test Accuracy (Activity): 0.9121\n"
          ]
        }
      ]
    }
  ]
}