{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLtyKtnRmH79",
        "outputId": "443749d6-fddf-4a60-f15e-1c9107529798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/UCI_HAR_Dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fECyEOphPfCc"
      },
      "source": [
        "# DataSet Creation and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2h_OD-AnSWQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7cEoNAAnc9Y",
        "outputId": "05498d31-cc71-4b80-9e4f-8f14e7dc37d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No of Features: 561\n"
          ]
        }
      ],
      "source": [
        "# get feature names from the file features.txt\n",
        "features = list()\n",
        "with open('/content/drive/MyDrive/UCI_HAR_Dataset/features.txt') as f:\n",
        "    features = [line.split()[1] for line in f.readlines()]\n",
        "\n",
        "print('No of Features: {}'.format(len(features)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmUpvq57nnL5",
        "outputId": "dfd04db4-9a2c-43b3-aca6-d1b6fdf34a9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "561"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seen = set()\n",
        "uniq_features = []\n",
        "for idx, x in enumerate(features):\n",
        "    if x not in seen:\n",
        "        uniq_features.append(x)\n",
        "        seen.add(x)\n",
        "    elif x + 'n' not in seen:\n",
        "        uniq_features.append(x + 'n')\n",
        "        seen.add(x + 'n')\n",
        "    else:\n",
        "        uniq_features.append(x + 'nn')\n",
        "        seen.add(x + 'nn')\n",
        "len(uniq_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Qplo0OD1c6"
      },
      "source": [
        "Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4QIp4aKnrIx",
        "outputId": "bad0ba48-93e9-41ec-db70-26a10196e3c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-661308084.py:6: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  sep='\\s+',              # instead of delim_whitespace=True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7352, 564)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# get the data from txt files to pandas dataframe\n",
        "X_train = pd.read_csv(\n",
        "    '/content/drive/MyDrive/UCI_HAR_Dataset/train/X_train.txt',\n",
        "    sep='\\s+',              # instead of delim_whitespace=True\n",
        "    header=None,\n",
        "    names=uniq_features\n",
        ")\n",
        "\n",
        "# add subject column to the dataframe\n",
        "subjects_train = pd.read_csv(\n",
        "    '/content/drive/MyDrive/UCI_HAR_Dataset/train/subject_train.txt',\n",
        "    header=None\n",
        ").squeeze(\"columns\")        # replaces squeeze=True\n",
        "\n",
        "X_train['subject'] = subjects_train\n",
        "\n",
        "# load y_train\n",
        "y_train = pd.read_csv(\n",
        "    '/content/drive/MyDrive/UCI_HAR_Dataset/train/y_train.txt',\n",
        "    header=None,\n",
        "    names=['Activity']\n",
        ").squeeze(\"columns\")\n",
        "\n",
        "# map activity labels to human-readable names\n",
        "y_train_labels = y_train.map({\n",
        "    1: 'WALKING',\n",
        "    2: 'WALKING_UPSTAIRS',\n",
        "    3: 'WALKING_DOWNSTAIRS',\n",
        "    4: 'SITTING',\n",
        "    5: 'STANDING',\n",
        "    6: 'LAYING'\n",
        "})\n",
        "\n",
        "# put all columns in a single dataframe\n",
        "train = X_train.copy()\n",
        "train['Activity'] = y_train\n",
        "train['ActivityName'] = y_train_labels\n",
        "\n",
        "print(train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd9wktlsHHEV"
      },
      "source": [
        "Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xhbErmJGC85",
        "outputId": "917bcb5f-e493-413f-92a1-831134fcb57d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-3925408283.py:6: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  sep='\\s+',              # instead of delim_whitespace=True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2947, 564)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# get the data from txt files to pandas dataframe\n",
        "X_test = pd.read_csv(\n",
        "    '/content/drive/MyDrive/UCI_HAR_Dataset/test/X_test.txt',\n",
        "    sep='\\s+',              # instead of delim_whitespace=True\n",
        "    header=None,\n",
        "    names=uniq_features\n",
        ")\n",
        "\n",
        "# add subject column to the dataframe\n",
        "subjects_test = pd.read_csv(\n",
        "    '/content/drive/MyDrive/UCI_HAR_Dataset/test/subject_test.txt',\n",
        "    header=None\n",
        ").squeeze(\"columns\")        # replaces squeeze=True\n",
        "\n",
        "X_test['subject'] = subjects_test\n",
        "\n",
        "# get y labels from the txt file\n",
        "y_test = pd.read_csv(\n",
        "    '/content/drive/MyDrive/UCI_HAR_Dataset/test/y_test.txt',\n",
        "    header=None,\n",
        "    names=['Activity']\n",
        ").squeeze(\"columns\")\n",
        "\n",
        "# map activity labels to human-readable names\n",
        "y_test_labels = y_test.map({\n",
        "    1: 'WALKING',\n",
        "    2: 'WALKING_UPSTAIRS',\n",
        "    3: 'WALKING_DOWNSTAIRS',\n",
        "    4: 'SITTING',\n",
        "    5: 'STANDING',\n",
        "    6: 'LAYING'\n",
        "})\n",
        "\n",
        "# put all columns in a single dataframe\n",
        "test = X_test.copy()\n",
        "test['Activity'] = y_test\n",
        "test['ActivityName'] = y_test_labels\n",
        "\n",
        "print(test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "3rCov_gDJNqT",
        "outputId": "009a6246-9b75-4eaf-c296-ac53df550c72"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1600x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['font.family'] = 'Dejavu Sans'\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.title('Data provided by each user', fontsize=20)\n",
        "sns.countplot(x='subject',hue='ActivityName', data = train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFAbkiIBJozb",
        "outputId": "706d3dad-65aa-4a36-fa9b-256fe1cee42c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z',\n",
              "       'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z',\n",
              "       'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z',\n",
              "       'tBodyAcc-max()-X',\n",
              "       ...\n",
              "       'angle(tBodyAccMean,gravity)', 'angle(tBodyAccJerkMean),gravityMean)',\n",
              "       'angle(tBodyGyroMean,gravityMean)',\n",
              "       'angle(tBodyGyroJerkMean,gravityMean)', 'angle(X,gravityMean)',\n",
              "       'angle(Y,gravityMean)', 'angle(Z,gravityMean)', 'subject', 'Activity',\n",
              "       'ActivityName'],\n",
              "      dtype='object', length=564)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns = train.columns\n",
        "\n",
        "# Removing '()' from column names\n",
        "columns = columns.str.replace('[()]','')s\n",
        "columns = columns.str.replace('[-]', '')\n",
        "columns = columns.str.replace('[,]','')\n",
        "\n",
        "train.columns = columns\n",
        "test.columns = columns\n",
        "\n",
        "test.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWdDTKAMJyqP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(dataset_path + '/data', exist_ok=True)\n",
        "\n",
        "train.to_csv(dataset_path + '/data/train.csv', index=False)\n",
        "test.to_csv(dataset_path + '/data/test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqFPiB432eBU"
      },
      "source": [
        "# Train an MLP model on extracted features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTz6sMowYZBZ"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1keIYdBYVoS"
      },
      "outputs": [],
      "source": [
        "# Import different classifiers\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofDC_gpQfNfH"
      },
      "source": [
        "**Define a class model and train function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvx8Ret6uiro"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class FlexibleMLP(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
        "        super(FlexibleMLP, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for hdim in hidden_dims:\n",
        "            layers.append(torch.nn.Linear(prev_dim, hdim))\n",
        "            layers.append(torch.nn.ReLU())\n",
        "            prev_dim = hdim\n",
        "        self.feature_extractor = torch.nn.Sequential(*layers)\n",
        "        self.classifier = torch.nn.Linear(prev_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.feature_extractor(x)\n",
        "        out = self.classifier(feats)\n",
        "        return out\n",
        "\n",
        "    # Add this method for embedding extraction\n",
        "    def hidden_forward(self, x):\n",
        "        return self.feature_extractor(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---- Train model ----\n",
        "def train_model(X_train, y_train, model,\n",
        "                X_test=None, y_test=None,\n",
        "                epochs=100, lr=0.01, batch_size=64):\n",
        "    \"\"\"\n",
        "    Train a PyTorch model and optionally evaluate on a test set each epoch.\n",
        "\n",
        "    Parameters:\n",
        "        X_train, y_train: training data (numpy arrays)\n",
        "        X_test, y_test: test data (numpy arrays), optional\n",
        "        model: PyTorch nn.Module\n",
        "        epochs: number of epochs\n",
        "        lr: learning rate\n",
        "        batch_size: batch size\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    # Convert data to tensors\n",
        "    X_train = torch.FloatTensor(X_train)\n",
        "    y_train = torch.LongTensor(y_train)\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    if X_test is not None and y_test is not None:\n",
        "        X_test = torch.FloatTensor(X_test)\n",
        "        y_test = torch.LongTensor(y_test)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        for xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            correct += (out.argmax(dim=1) == yb).sum().item()\n",
        "\n",
        "        train_loss = total_loss / len(train_loader.dataset)\n",
        "        train_acc = correct / len(train_loader.dataset)\n",
        "\n",
        "        if X_test is not None and y_test is not None:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                out_test = model(X_test)\n",
        "                test_loss = criterion(out_test, y_test).item()\n",
        "                test_acc = (out_test.argmax(dim=1) == y_test).float().mean().item()\n",
        "                '''\n",
        "            print(f\"Epoch {epoch+1}: \"\n",
        "                  f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
        "                  \"Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}\")\n",
        "            '''\n",
        "    if X_test is not None and y_test is not None:\n",
        "       print(f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B46A4msXfW32"
      },
      "source": [
        "**Load the train and test data, merge them, specify the test subjects, and train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ILliH-oDAp6D",
        "outputId": "ed451aa9-0b70-4f80-c19e-ff2725a20ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All data: (10299, 561) (10299,)\n",
            "Selected test subjects: [10]\n",
            "Train : (10005, 561) (10005,)\n",
            "Test  : (294, 561) (294,)\n",
            "Train Loss=0.0496, Train Acc=0.9826 | Test Loss=0.3074, Test Acc=0.8537\n",
            "\n",
            " Prediction Entropy (General Model)\n",
            "Per-sample entropy (first 10): [1.1006866e-04 5.0554629e-02 6.6941440e-01 7.4720848e-04 1.3369434e-01\n",
            " 1.5835803e-04 3.5905361e-02 6.6004968e-01 1.0832670e-01 7.1546346e-02]\n",
            "Average entropy overall: 0.16381133\n",
            "Average entropy per class: {'LAYING': np.float32(0.2130118), 'SITTING': np.float32(0.20250928), 'STANDING': np.float32(0.22455402), 'WALKING': np.float32(0.04745704), 'WALKING_DOWNSTAIRS': np.float32(0.00078153005), 'WALKING_UPSTAIRS': np.float32(0.26478827)}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.stats import entropy  # <-- for prediction entropy\n",
        "\n",
        "# ---------------------------\n",
        "# Load full dataset (train + test merged)\n",
        "# ---------------------------\n",
        "def read_data(files, exclude_subjects=None):\n",
        "    # Merge multiple csv files\n",
        "    data = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "    data = sklearn.utils.shuffle(data)\n",
        "\n",
        "    if exclude_subjects is not None:\n",
        "        data = data[~data[\"subject\"].isin(exclude_subjects)]\n",
        "\n",
        "    X_data = data.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n",
        "    y_data = data.ActivityName\n",
        "    subjects = data[\"subject\"].values\n",
        "\n",
        "    return np.array(X_data), np.array(y_data), subjects\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Merge train + test\n",
        "# ---------------------------\n",
        "all_X, all_y, all_subjects = read_data([\n",
        "    dataset_path + '/data/train.csv',\n",
        "    dataset_path + '/data/test.csv'\n",
        "])\n",
        "\n",
        "print(\"All data:\", all_X.shape, all_y.shape)\n",
        "\n",
        "# ---------------------------\n",
        "# Choose test subjects\n",
        "# ---------------------------\n",
        "test_subjects = [10]   # tough test subject set = [9, 10, 14, 16]\n",
        "print(\"Selected test subjects:\", test_subjects)\n",
        "\n",
        "# Split into train/test based on subject IDs\n",
        "train_mask = ~np.isin(all_subjects, test_subjects)\n",
        "test_mask  = np.isin(all_subjects, test_subjects)\n",
        "\n",
        "train_X, train_y = all_X[train_mask], all_y[train_mask]\n",
        "test_X, test_y   = all_X[test_mask], all_y[test_mask]\n",
        "\n",
        "print(\"Train :\", train_X.shape, train_y.shape)\n",
        "print(\"Test  :\", test_X.shape, test_y.shape)\n",
        "\n",
        "# ---------------------------\n",
        "# Encode labels to integers\n",
        "# ---------------------------\n",
        "label_encoder = LabelEncoder()\n",
        "train_y_encoded = label_encoder.fit_transform(train_y)\n",
        "test_y_encoded  = label_encoder.transform(test_y)\n",
        "\n",
        "# Convert X to float32 (for PyTorch compatibility)\n",
        "train_X = train_X.astype(\"float32\")\n",
        "test_X  = test_X.astype(\"float32\")\n",
        "\n",
        "# ---------------------------\n",
        "# Model setup\n",
        "# ---------------------------\n",
        "input_dim = train_X.shape[1]\n",
        "hidden_dims = [100, 100, 100]\n",
        "output_dim = len(np.unique(train_y))\n",
        "\n",
        "common_model = FlexibleMLP(input_dim, hidden_dims, output_dim)\n",
        "\n",
        "# Train common model (assuming you have train_model() defined elsewhere)\n",
        "common_model = train_model(\n",
        "    train_X, train_y_encoded,\n",
        "    common_model,\n",
        "    test_X, test_y_encoded,\n",
        "    epochs=100, lr=0.01, batch_size=64\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Save model + label encoder\n",
        "# ---------------------------\n",
        "model_file = os.path.join(dataset_path, \"models\", \"common_mlp.pt\")\n",
        "torch.save(common_model.state_dict(), model_file)\n",
        "\n",
        "le_file = os.path.join(dataset_path, \"models\", \"label_encoder.pkl\")\n",
        "with open(le_file, \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "#print(f\" Model saved to {model_file}\")\n",
        "#print(f\" Label encoder saved to {le_file}\")\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "#  Entropy evaluation for general model\n",
        "# =======================================================\n",
        "common_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = common_model(torch.from_numpy(test_X))\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "# Per-sample entropy\n",
        "per_sample_entropy = entropy(probs.T)  # shape (n_samples,)\n",
        "\n",
        "# Average entropy overall\n",
        "avg_entropy = np.mean(per_sample_entropy)\n",
        "\n",
        "# Average entropy per class\n",
        "class_entropies = {}\n",
        "for cls in np.unique(test_y_encoded):\n",
        "    cls_mask = test_y_encoded == cls\n",
        "    class_entropies[label_encoder.inverse_transform([cls])[0]] = np.mean(per_sample_entropy[cls_mask])\n",
        "\n",
        "print(\"\\n Prediction Entropy (General Model)\")\n",
        "print(\"Per-sample entropy (first 10):\", per_sample_entropy[:10])\n",
        "print(\"Average entropy overall:\", avg_entropy)\n",
        "print(\"Average entropy per class:\", class_entropies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO4l0k1v15mh"
      },
      "source": [
        "**Fine-tune the general model on each specific train subject's data, then evaluate on the test subject's data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oHgcbNp4dhKY",
        "outputId": "98cbd9e1-497f-47f8-a99e-e5d575ca374b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Fine-tuning on Train Subject 1\n",
            " Train Subj 1 → Test Subj 10: 87.76% (n=294)\n",
            "    Avg Prediction Entropy: 0.1295\n",
            "\n",
            " Fine-tuning on Train Subject 2\n",
            " Train Subj 2 → Test Subj 10: 85.37% (n=294)\n",
            "    Avg Prediction Entropy: 0.1060\n",
            "\n",
            " Fine-tuning on Train Subject 3\n",
            " Train Subj 3 → Test Subj 10: 86.73% (n=294)\n",
            "    Avg Prediction Entropy: 0.0871\n",
            "\n",
            " Fine-tuning on Train Subject 4\n",
            " Train Subj 4 → Test Subj 10: 83.67% (n=294)\n",
            "    Avg Prediction Entropy: 0.1141\n",
            "\n",
            " Fine-tuning on Train Subject 5\n",
            " Train Subj 5 → Test Subj 10: 84.69% (n=294)\n",
            "    Avg Prediction Entropy: 0.1002\n",
            "\n",
            " Fine-tuning on Train Subject 6\n",
            " Train Subj 6 → Test Subj 10: 89.12% (n=294)\n",
            "    Avg Prediction Entropy: 0.0799\n",
            "\n",
            " Fine-tuning on Train Subject 7\n",
            " Train Subj 7 → Test Subj 10: 88.44% (n=294)\n",
            "    Avg Prediction Entropy: 0.1336\n",
            "\n",
            " Fine-tuning on Train Subject 8\n",
            " Train Subj 8 → Test Subj 10: 79.59% (n=294)\n",
            "    Avg Prediction Entropy: 0.1048\n",
            "\n",
            " Fine-tuning on Train Subject 9\n",
            " Train Subj 9 → Test Subj 10: 90.48% (n=294)\n",
            "    Avg Prediction Entropy: 0.0973\n",
            "\n",
            " Fine-tuning on Train Subject 11\n",
            " Train Subj 11 → Test Subj 10: 82.65% (n=294)\n",
            "    Avg Prediction Entropy: 0.1155\n",
            "\n",
            " Fine-tuning on Train Subject 12\n",
            " Train Subj 12 → Test Subj 10: 87.07% (n=294)\n",
            "    Avg Prediction Entropy: 0.0902\n",
            "\n",
            " Fine-tuning on Train Subject 13\n",
            " Train Subj 13 → Test Subj 10: 87.07% (n=294)\n",
            "    Avg Prediction Entropy: 0.1209\n",
            "\n",
            " Fine-tuning on Train Subject 14\n",
            " Train Subj 14 → Test Subj 10: 85.03% (n=294)\n",
            "    Avg Prediction Entropy: 0.1105\n",
            "\n",
            " Fine-tuning on Train Subject 15\n",
            " Train Subj 15 → Test Subj 10: 90.14% (n=294)\n",
            "    Avg Prediction Entropy: 0.0915\n",
            "\n",
            " Fine-tuning on Train Subject 16\n",
            " Train Subj 16 → Test Subj 10: 88.78% (n=294)\n",
            "    Avg Prediction Entropy: 0.1078\n",
            "\n",
            " Fine-tuning on Train Subject 17\n",
            " Train Subj 17 → Test Subj 10: 84.01% (n=294)\n",
            "    Avg Prediction Entropy: 0.0797\n",
            "\n",
            " Fine-tuning on Train Subject 18\n",
            " Train Subj 18 → Test Subj 10: 82.65% (n=294)\n",
            "    Avg Prediction Entropy: 0.1024\n",
            "\n",
            " Fine-tuning on Train Subject 19\n",
            " Train Subj 19 → Test Subj 10: 83.67% (n=294)\n",
            "    Avg Prediction Entropy: 0.0787\n",
            "\n",
            " Fine-tuning on Train Subject 20\n",
            " Train Subj 20 → Test Subj 10: 81.63% (n=294)\n",
            "    Avg Prediction Entropy: 0.1071\n",
            "\n",
            " Fine-tuning on Train Subject 21\n",
            " Train Subj 21 → Test Subj 10: 90.14% (n=294)\n",
            "    Avg Prediction Entropy: 0.0799\n",
            "\n",
            " Fine-tuning on Train Subject 22\n",
            " Train Subj 22 → Test Subj 10: 87.07% (n=294)\n",
            "    Avg Prediction Entropy: 0.1279\n",
            "\n",
            " Fine-tuning on Train Subject 23\n",
            " Train Subj 23 → Test Subj 10: 89.12% (n=294)\n",
            "    Avg Prediction Entropy: 0.1088\n",
            "\n",
            " Fine-tuning on Train Subject 24\n",
            " Train Subj 24 → Test Subj 10: 88.78% (n=294)\n",
            "    Avg Prediction Entropy: 0.1022\n",
            "\n",
            " Fine-tuning on Train Subject 25\n",
            " Train Subj 25 → Test Subj 10: 82.65% (n=294)\n",
            "    Avg Prediction Entropy: 0.1109\n",
            "\n",
            " Fine-tuning on Train Subject 26\n",
            " Train Subj 26 → Test Subj 10: 81.63% (n=294)\n",
            "    Avg Prediction Entropy: 0.1103\n",
            "\n",
            " Fine-tuning on Train Subject 27\n",
            " Train Subj 27 → Test Subj 10: 85.03% (n=294)\n",
            "    Avg Prediction Entropy: 0.1080\n",
            "\n",
            " Fine-tuning on Train Subject 28\n",
            " Train Subj 28 → Test Subj 10: 90.14% (n=294)\n",
            "    Avg Prediction Entropy: 0.0845\n",
            "\n",
            " Fine-tuning on Train Subject 29\n",
            " Train Subj 29 → Test Subj 10: 90.14% (n=294)\n",
            "    Avg Prediction Entropy: 0.0905\n",
            "\n",
            " Fine-tuning on Train Subject 30\n",
            " Train Subj 30 → Test Subj 10: 85.71% (n=294)\n",
            "    Avg Prediction Entropy: 0.1118\n",
            "\n",
            "\n",
            " Accuracy Results: {np.int64(1): 87.75510204081633, np.int64(2): 85.37414965986395, np.int64(3): 86.73469387755102, np.int64(4): 83.6734693877551, np.int64(5): 84.6938775510204, np.int64(6): 89.1156462585034, np.int64(7): 88.43537414965986, np.int64(8): 79.59183673469387, np.int64(9): 90.47619047619048, np.int64(11): 82.6530612244898, np.int64(12): 87.07482993197279, np.int64(13): 87.07482993197279, np.int64(14): 85.03401360544217, np.int64(15): 90.1360544217687, np.int64(16): 88.77551020408163, np.int64(17): 84.01360544217688, np.int64(18): 82.6530612244898, np.int64(19): 83.6734693877551, np.int64(20): 81.63265306122449, np.int64(21): 90.1360544217687, np.int64(22): 87.07482993197279, np.int64(23): 89.1156462585034, np.int64(24): 88.77551020408163, np.int64(25): 82.6530612244898, np.int64(26): 81.63265306122449, np.int64(27): 85.03401360544217, np.int64(28): 90.1360544217687, np.int64(29): 90.1360544217687, np.int64(30): 85.71428571428571}\n",
            "\n",
            " Avg Prediction Entropies: {np.int64(1): np.float32(0.12948713), np.int64(2): np.float32(0.10601267), np.int64(3): np.float32(0.0871233), np.int64(4): np.float32(0.11412313), np.int64(5): np.float32(0.10023061), np.int64(6): np.float32(0.07990736), np.int64(7): np.float32(0.13359557), np.int64(8): np.float32(0.1047582), np.int64(9): np.float32(0.09734378), np.int64(11): np.float32(0.1154558), np.int64(12): np.float32(0.09016943), np.int64(13): np.float32(0.120931976), np.int64(14): np.float32(0.11047293), np.int64(15): np.float32(0.091503665), np.int64(16): np.float32(0.10776562), np.int64(17): np.float32(0.079663455), np.int64(18): np.float32(0.1023805), np.int64(19): np.float32(0.07874647), np.int64(20): np.float32(0.10708583), np.int64(21): np.float32(0.07987482), np.int64(22): np.float32(0.12793142), np.int64(23): np.float32(0.10876269), np.int64(24): np.float32(0.10223255), np.int64(25): np.float32(0.110878885), np.int64(26): np.float32(0.11030806), np.int64(27): np.float32(0.10802008), np.int64(28): np.float32(0.084545314), np.int64(29): np.float32(0.09052846), np.int64(30): np.float32(0.11175266)}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "batch_size = 16\n",
        "epochs = 30\n",
        "lr = 0.005\n",
        "num_unfrozen_layers = 1   # last layers to unfreeze\n",
        "test_subject_id = test_subjects[0]   # chosen test subject\n",
        "\n",
        "# ---------------------------\n",
        "# Split dataset into train/test by subject IDs\n",
        "# ---------------------------\n",
        "train_mask = all_subjects != test_subject_id\n",
        "test_mask  = all_subjects == test_subject_id\n",
        "\n",
        "train_X, train_y = all_X[train_mask], all_y[train_mask]\n",
        "train_subjects   = all_subjects[train_mask]\n",
        "\n",
        "test_X, test_y = all_X[test_mask], all_y[test_mask]\n",
        "\n",
        "train_y_encoded = label_encoder.transform(train_y)\n",
        "test_y_encoded  = label_encoder.transform(test_y)\n",
        "\n",
        "input_dim = train_X.shape[1]\n",
        "hidden_dims = [100, 100, 100]\n",
        "output_dim = len(label_encoder.classes_)\n",
        "\n",
        "# ---------------------------\n",
        "# Dictionary to save models\n",
        "# ---------------------------\n",
        "personalized_models = {\"general\": common_model.state_dict()}  # store general model weights\n",
        "subject_accs = {}\n",
        "subject_entropies = {}\n",
        "\n",
        "# ---------------------------\n",
        "# Fine-tune per train subject\n",
        "# ---------------------------\n",
        "unique_train_subjects = np.unique(train_subjects)\n",
        "\n",
        "for subj in unique_train_subjects:\n",
        "    print(f\" Fine-tuning on Train Subject {subj}\")\n",
        "\n",
        "    # Select data for this train subject\n",
        "    subj_mask = train_subjects == subj\n",
        "    subj_X = train_X[subj_mask].astype(\"float32\")\n",
        "    subj_y = train_y_encoded[subj_mask]\n",
        "\n",
        "    dataset = TensorDataset(torch.from_numpy(subj_X), torch.from_numpy(subj_y))\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Clone common model\n",
        "    personal_model = FlexibleMLP(input_dim, hidden_dims, output_dim)\n",
        "    personal_model.load_state_dict(common_model.state_dict())\n",
        "\n",
        "    # ---------------------------\n",
        "    # Freeze all layers first\n",
        "    for param in personal_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Unfreeze last N layers of feature_extractor\n",
        "    feature_layers = list(personal_model.feature_extractor.children())\n",
        "    if num_unfrozen_layers > len(feature_layers):\n",
        "        raise ValueError(f\"num_unfrozen_layers ({num_unfrozen_layers}) exceeds number of layers in feature_extractor ({len(feature_layers)})\")\n",
        "    for layer in feature_layers[-num_unfrozen_layers:]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # Also unfreeze classifier\n",
        "    for param in personal_model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    personal_model.train()\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, personal_model.parameters()), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        for batch_X, batch_y in loader:\n",
        "            batch_X = batch_X.float()\n",
        "            batch_y = batch_y.long()\n",
        "            optimizer.zero_grad()\n",
        "            logits = personal_model(batch_X)\n",
        "            loss = criterion(logits, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Save only classifier layer for this subject\n",
        "    personalized_models[subj] = {\n",
        "        \"weight\": personal_model.classifier.weight.clone(),\n",
        "        \"bias\": personal_model.classifier.bias.clone()\n",
        "    }\n",
        "\n",
        "    # ---------------------------\n",
        "    # Evaluate on test subject + per-sample entropy\n",
        "    # ---------------------------\n",
        "    model_eval = FlexibleMLP(input_dim, hidden_dims, output_dim)\n",
        "    model_eval.load_state_dict(common_model.state_dict())  # backbone + init classifier\n",
        "    with torch.no_grad():\n",
        "        model_eval.classifier.weight.copy_(personalized_models[subj][\"weight\"])\n",
        "        model_eval.classifier.bias.copy_(personalized_models[subj][\"bias\"])\n",
        "\n",
        "    X_test_tensor = torch.from_numpy(test_X.astype(\"float32\"))\n",
        "    y_test_encoded = test_y_encoded\n",
        "\n",
        "    model_eval.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model_eval(X_test_tensor)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "        preds = probs.argmax(axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_test_encoded, preds) * 100\n",
        "    subject_accs[subj] = acc\n",
        "\n",
        "    # Per-sample entropy (prediction uncertainty)\n",
        "    per_sample_entropy = entropy(probs.T)  # entropy across classes for each sample\n",
        "    avg_entropy = np.mean(per_sample_entropy)\n",
        "    subject_entropies[subj] = {\n",
        "        \"per_sample_entropy\": per_sample_entropy,\n",
        "        \"avg_entropy\": avg_entropy\n",
        "    }\n",
        "\n",
        "    print(f\" Train Subj {subj} → Test Subj {test_subject_id}: {acc:.2f}% (n={len(y_test_encoded)})\")\n",
        "    print(f\"    Avg Prediction Entropy: {avg_entropy:.4f}\\n\")\n",
        "\n",
        "# ---------------------------\n",
        "# Summary\n",
        "# ---------------------------\n",
        "print(\"\\n Accuracy Results:\", subject_accs)\n",
        "print(\"\\n Avg Prediction Entropies:\", {k:v['avg_entropy'] for k,v in subject_entropies.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQWUsPS-KTJJ",
        "outputId": "257beb1a-d4ca-43fd-bdcb-510595db5c2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Test Sample 7\n",
            "   Ground Truth: STANDING (class 2)\n",
            "   General Model → Pred: SITTING (class 1), Entropy=0.6600\n",
            "   Personal Model 1 → Pred: STANDING (class 2), Entropy=0.5284\n",
            "   Personal Model 2 → Pred: SITTING (class 1), Entropy=0.0476\n",
            "   Personal Model 3 → Pred: SITTING (class 1), Entropy=0.1348\n",
            "   Personal Model 4 → Pred: SITTING (class 1), Entropy=0.6780\n",
            "   Personal Model 5 → Pred: STANDING (class 2), Entropy=0.0790\n",
            "   Personal Model 6 → Pred: SITTING (class 1), Entropy=0.1815\n",
            "   Personal Model 7 → Pred: STANDING (class 2), Entropy=0.6801\n",
            "   Personal Model 8 → Pred: STANDING (class 2), Entropy=0.0209\n",
            "   Personal Model 9 → Pred: SITTING (class 1), Entropy=0.4467\n",
            "   Personal Model 11 → Pred: SITTING (class 1), Entropy=0.6129\n",
            "   Personal Model 12 → Pred: SITTING (class 1), Entropy=0.0464\n",
            "   Personal Model 13 → Pred: SITTING (class 1), Entropy=0.4739\n",
            "   Personal Model 14 → Pred: SITTING (class 1), Entropy=0.2779\n",
            "   Personal Model 15 → Pred: SITTING (class 1), Entropy=0.2953\n",
            "   Personal Model 16 → Pred: STANDING (class 2), Entropy=0.5654\n",
            "   Personal Model 17 → Pred: SITTING (class 1), Entropy=0.1595\n",
            "   Personal Model 18 → Pred: SITTING (class 1), Entropy=0.0886\n",
            "   Personal Model 19 → Pred: SITTING (class 1), Entropy=0.6075\n",
            "   Personal Model 20 → Pred: SITTING (class 1), Entropy=0.0504\n",
            "   Personal Model 21 → Pred: SITTING (class 1), Entropy=0.0433\n",
            "   Personal Model 22 → Pred: STANDING (class 2), Entropy=0.6307\n",
            "   Personal Model 23 → Pred: SITTING (class 1), Entropy=0.6811\n",
            "   Personal Model 24 → Pred: STANDING (class 2), Entropy=0.4279\n",
            "   Personal Model 25 → Pred: STANDING (class 2), Entropy=0.0752\n",
            "   Personal Model 26 → Pred: STANDING (class 2), Entropy=0.5445\n",
            "   Personal Model 27 → Pred: STANDING (class 2), Entropy=0.2013\n",
            "   Personal Model 28 → Pred: SITTING (class 1), Entropy=0.1925\n",
            "   Personal Model 29 → Pred: STANDING (class 2), Entropy=0.2150\n",
            "   Personal Model 30 → Pred: SITTING (class 1), Entropy=0.5532\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# Inspect a single test sample\n",
        "# ---------------------------\n",
        "sample_idx = 7  # <-- change to any index you want\n",
        "x_single = torch.from_numpy(test_X[sample_idx].astype(\"float32\")).unsqueeze(0)\n",
        "y_true   = test_y_encoded[sample_idx]\n",
        "\n",
        "print(f\"\\n🔍 Test Sample {sample_idx}\")\n",
        "print(f\"   Ground Truth: {label_encoder.inverse_transform([y_true])[0]} (class {y_true})\")\n",
        "\n",
        "# ---- General Model ----\n",
        "common_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = common_model(x_single)\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy().flatten()\n",
        "    pred = probs.argmax()\n",
        "    ent = entropy(probs)\n",
        "\n",
        "print(f\"   General Model → Pred: {label_encoder.inverse_transform([pred])[0]} (class {pred}), \"\n",
        "      f\"Entropy={ent:.4f}\")\n",
        "\n",
        "# ---- Personalized Models ----\n",
        "for subj in [s for s in personalized_models.keys() if s != \"general\"]:\n",
        "    model_eval = FlexibleMLP(input_dim, hidden_dims, output_dim)\n",
        "    model_eval.load_state_dict(common_model.state_dict())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model_eval.classifier.weight.copy_(personalized_models[subj][\"weight\"])\n",
        "        model_eval.classifier.bias.copy_(personalized_models[subj][\"bias\"])\n",
        "\n",
        "        model_eval.eval()\n",
        "        logits = model_eval(x_single)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy().flatten()\n",
        "        pred = probs.argmax()\n",
        "        ent = entropy(probs)\n",
        "\n",
        "    print(f\"   Personal Model {subj} → Pred: {label_encoder.inverse_transform([pred])[0]} (class {pred}), \"\n",
        "          f\"Entropy={ent:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_tVEhoGS0Iv"
      },
      "source": [
        "# **Domain Adaptation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4XVJYAf27T9"
      },
      "source": [
        "**Similarity between train subjects and a specific test subject**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfrx0TVWRmA0",
        "outputId": "45dc141f-34a8-472e-98e1-45ec44bd2373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Similar train subjects (Input Cosine): [17, 27, 24, 9, 18]\n",
            "✅ Similar train subjects (Input Euclidean): [17, 27, 24, 9, 18]\n",
            "✅ Similar train subjects (Embedding Cosine): [27, 17, 24, 18, 9]\n",
            "✅ Similar train subjects (Embedding Euclidean): [27, 17, 24, 18, 9]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances, euclidean_distances\n",
        "\n",
        "# ============================================================\n",
        "# 1. Build DataFrame\n",
        "# ============================================================\n",
        "def make_df(X: np.ndarray, y, subjects_per_sample) -> pd.DataFrame:\n",
        "    assert len(X) == len(y) == len(subjects_per_sample), (\n",
        "        f\"Length mismatch: X={len(X)}, y={len(y)}, subjects={len(subjects_per_sample)}\"\n",
        "    )\n",
        "    n_features = X.shape[1]\n",
        "    df = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(n_features)])\n",
        "    df[\"subject\"] = np.asarray(subjects_per_sample)\n",
        "    df[\"ActivityName\"] = np.asarray(y)\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "# ============================================================\n",
        "# 2. Compute mean feature vectors (input-level)\n",
        "# ============================================================\n",
        "def compute_mean_feature_vectors(data, n_features, class_order=None):\n",
        "    results = {}\n",
        "    for subject_id, sub_df in data.groupby(\"subject\"):\n",
        "        subject_matrix = []\n",
        "        activities = class_order if class_order else sorted(sub_df[\"ActivityName\"].unique())\n",
        "        for activity_name in activities:\n",
        "            act_df = sub_df[sub_df[\"ActivityName\"] == activity_name]\n",
        "            if len(act_df) > 0:\n",
        "                X = act_df.drop([\"subject\", \"ActivityName\"], axis=1).values\n",
        "                mean_vec = X.mean(axis=0)\n",
        "            else:\n",
        "                mean_vec = np.zeros(n_features)\n",
        "            subject_matrix.append(mean_vec)\n",
        "        results[subject_id] = np.vstack(subject_matrix)\n",
        "    return results\n",
        "\n",
        "# ============================================================\n",
        "# 3. FlexibleMLP model (for embeddings)\n",
        "# ============================================================\n",
        "class FlexibleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
        "        super(FlexibleMLP, self).__init__()\n",
        "        layers = []\n",
        "        last_dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(last_dim, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            last_dim = h\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Linear(last_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.feature_extractor(x)\n",
        "        return self.classifier(hidden)\n",
        "\n",
        "    def hidden_forward(self, x):\n",
        "        return self.feature_extractor(x)\n",
        "\n",
        "# ============================================================\n",
        "# 4. Extract embeddings\n",
        "# ============================================================\n",
        "def extract_embeddings(model, X):\n",
        "    with torch.no_grad():\n",
        "        X_tensor = torch.from_numpy(X.astype(\"float32\"))\n",
        "        hidden_out = model.hidden_forward(X_tensor)\n",
        "    return hidden_out.numpy()\n",
        "\n",
        "def compute_mean_embeddings_from_arrays(X, y, subjects, model, n_features):\n",
        "    df = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(n_features)])\n",
        "    df[\"subject\"] = subjects\n",
        "    df[\"ActivityName\"] = y\n",
        "\n",
        "    results = {}\n",
        "    for subject_id, sub_df in df.groupby(\"subject\"):\n",
        "        subject_matrix = []\n",
        "        for activity_name, act_df in sub_df.groupby(\"ActivityName\"):\n",
        "            X_sub = act_df.drop([\"subject\", \"ActivityName\"], axis=1).values\n",
        "            embeddings = extract_embeddings(model, X_sub)\n",
        "            mean_vec = embeddings.mean(axis=0)\n",
        "            subject_matrix.append(mean_vec)\n",
        "        results[subject_id] = np.vstack(subject_matrix)\n",
        "    return results\n",
        "\n",
        "# ============================================================\n",
        "# 5. MAIN PIPELINE\n",
        "# ============================================================\n",
        "def compute_similar_subjects(train_X, train_y, train_subjects,\n",
        "                             test_X, test_y, test_subjects,\n",
        "                             test_subject_id,\n",
        "                             input_dim=561, hidden_dims=[100,100,100], output_dim=6):\n",
        "    # --- Build train/test DataFrames\n",
        "    train_df = make_df(train_X, train_y, train_subjects)\n",
        "\n",
        "    if len(test_subjects) != len(test_X):\n",
        "        test_subjects_per_sample = np.repeat(test_subjects[0], len(test_X))\n",
        "    else:\n",
        "        test_subjects_per_sample = test_subjects\n",
        "    test_df = make_df(test_X, test_y, test_subjects_per_sample)\n",
        "\n",
        "    all_classes = sorted(train_df[\"ActivityName\"].unique())\n",
        "\n",
        "    # --- Input-level means\n",
        "    train_input_means = compute_mean_feature_vectors(train_df, n_features=input_dim, class_order=all_classes)\n",
        "    test_input_means  = compute_mean_feature_vectors(test_df,  n_features=input_dim, class_order=all_classes)\n",
        "\n",
        "    # --- Embedding-level means\n",
        "    common_model = FlexibleMLP(input_dim, hidden_dims, output_dim)\n",
        "    common_model.eval()\n",
        "    train_embed_means = compute_mean_embeddings_from_arrays(train_X, train_y, train_subjects, common_model, n_features=input_dim)\n",
        "    test_embed_means  = compute_mean_embeddings_from_arrays(test_X, test_y, test_subjects_per_sample, common_model, n_features=input_dim)\n",
        "\n",
        "    # --- Safety check\n",
        "    if test_subject_id not in test_input_means or test_subject_id not in test_embed_means:\n",
        "        raise ValueError(f\"❌ Test subject {test_subject_id} not found in test set!\")\n",
        "\n",
        "    # ========================================================\n",
        "    # Input-level similarities\n",
        "    # ========================================================\n",
        "    test_vec_input = test_input_means[test_subject_id].flatten().reshape(1, -1)\n",
        "\n",
        "    cosine_sims_input, euclidean_dists_input = {}, {}\n",
        "    for train_id, train_matrix in train_input_means.items():\n",
        "        train_vec = train_matrix.flatten().reshape(1, -1)\n",
        "        cosine_sims_input[train_id] = cosine_similarity(test_vec_input, train_vec)[0, 0]\n",
        "        euclidean_dists_input[train_id] = pairwise_distances(test_vec_input, train_vec, metric=\"euclidean\")[0, 0]\n",
        "\n",
        "    similar_input_cosine   = [sid for sid, _ in sorted(cosine_sims_input.items(), key=lambda x: x[1], reverse=True)]\n",
        "    similar_input_euclidean = [sid for sid, _ in sorted(euclidean_dists_input.items(), key=lambda x: x[1])]\n",
        "\n",
        "    # ========================================================\n",
        "    # Embedding-level similarities\n",
        "    # ========================================================\n",
        "    test_vec_embed = test_embed_means[test_subject_id].flatten().reshape(1, -1)\n",
        "\n",
        "    cosine_sims_embed, euclidean_dists_embed = {}, {}\n",
        "    for train_id, train_matrix in train_embed_means.items():\n",
        "        train_vec = train_matrix.flatten().reshape(1, -1)\n",
        "        cosine_sims_embed[train_id] = cosine_similarity(test_vec_embed, train_vec)[0, 0]\n",
        "        euclidean_dists_embed[train_id] = euclidean_distances(test_vec_embed, train_vec)[0, 0]\n",
        "\n",
        "    similar_embedding_cosine   = [sid for sid, _ in sorted(cosine_sims_embed.items(), key=lambda x: x[1], reverse=True)]\n",
        "    similar_embedding_euclidean = [sid for sid, _ in sorted(euclidean_dists_embed.items(), key=lambda x: x[1])]\n",
        "\n",
        "    return similar_input_cosine, similar_input_euclidean, similar_embedding_cosine, similar_embedding_euclidean\n",
        "\n",
        "# ============================================================\n",
        "# Example usage\n",
        "# ============================================================\n",
        "similar_input_cosine, similar_input_euclidean, similar_embedding_cosine, similar_embedding_euclidean = compute_similar_subjects(\n",
        "    train_X, train_y, train_subjects,\n",
        "    test_X, test_y, test_subjects,\n",
        "    test_subject_id=test_subjects[0]\n",
        ")\n",
        "\n",
        "print(\"✅ Similar train subjects (Input Cosine):\", similar_input_cosine[:5])\n",
        "print(\"✅ Similar train subjects (Input Euclidean):\", similar_input_euclidean[:5])\n",
        "print(\"✅ Similar train subjects (Embedding Cosine):\", similar_embedding_cosine[:5])\n",
        "print(\"✅ Similar train subjects (Embedding Euclidean):\", similar_embedding_euclidean[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jthgCY9rSr1h"
      },
      "source": [
        "Assemble a model the best train subject collection for the subject test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q629baYWUFnR",
        "outputId": "36ff71eb-9a8f-46fe-c116-69ca723634cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Best subset for test subject 16: [10], acc=90.71%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def build_average_personalized_model(\n",
        "    similar_train_subjects: list,\n",
        "    personalized_models: dict,\n",
        "    input_dim: int,\n",
        "    hidden_dims: list,\n",
        "    output_dim: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build averaged personalized model from a set of training subjects.\n",
        "    \"\"\"\n",
        "    if \"general\" not in personalized_models:\n",
        "        raise KeyError(\"personalized_models must contain the 'general' snapshot.\")\n",
        "    for sid in similar_train_subjects:\n",
        "        if sid not in personalized_models:\n",
        "            raise KeyError(f\"personalized_models has no classifier head for subject {sid}.\")\n",
        "\n",
        "    # Create backbone from general model\n",
        "    model = FlexibleMLP(input_dim, hidden_dims, output_dim)\n",
        "    general_sd = personalized_models[\"general\"]\n",
        "    backbone_sd = {k: v for k, v in general_sd.items() if k.startswith(\"feature_extractor.\")}\n",
        "    model.load_state_dict(backbone_sd, strict=False)\n",
        "\n",
        "    # Average classifier heads\n",
        "    weight_list, bias_list = [], []\n",
        "    for sid in similar_train_subjects:\n",
        "        head = personalized_models[sid]\n",
        "        weight_list.append(head[\"weight\"])\n",
        "        bias_list.append(head[\"bias\"])\n",
        "    avg_weight = torch.stack(weight_list).mean(dim=0)\n",
        "    avg_bias   = torch.stack(bias_list).mean(dim=0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.classifier.weight.copy_(avg_weight)\n",
        "        model.classifier.bias.copy_(avg_bias)\n",
        "\n",
        "    return model.float().cpu().eval()\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    X_test_tensor = torch.from_numpy(X_test.astype(\"float32\"))\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_test_tensor)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "    return accuracy_score(y_test, preds)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Greedy search for best subset of train subjects\n",
        "# ============================================================\n",
        "def find_best_similar_subjects(train_subject_ids, personalized_models,\n",
        "                               test_X, test_y_encoded,\n",
        "                               input_dim, hidden_dims, output_dim,\n",
        "                               max_subset_size=5):\n",
        "    best_subset = []\n",
        "    best_acc = 0.0\n",
        "    remaining = set(train_subject_ids)\n",
        "\n",
        "    for step in range(max_subset_size):\n",
        "        best_candidate, best_candidate_acc = None, 0.0\n",
        "\n",
        "        for sid in remaining:\n",
        "            candidate_subset = best_subset + [sid]\n",
        "            model = build_average_personalized_model(\n",
        "                candidate_subset, personalized_models,\n",
        "                input_dim, hidden_dims, output_dim\n",
        "            )\n",
        "            acc = evaluate_model(model, test_X, test_y_encoded)\n",
        "            if acc > best_candidate_acc:\n",
        "                best_candidate_acc = acc\n",
        "                best_candidate = sid\n",
        "\n",
        "        if best_candidate is not None and best_candidate_acc > best_acc:\n",
        "            best_subset.append(best_candidate)\n",
        "            remaining.remove(best_candidate)\n",
        "            best_acc = best_candidate_acc\n",
        "            #print(f\"Step {step+1}: added subject {best_candidate}, acc={best_acc:.2f}\")\n",
        "        else:\n",
        "            break  # no improvement\n",
        "\n",
        "    return best_subset, best_acc\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Example usage\n",
        "# =========================\n",
        "available_train_subjects = [sid for sid in range(1, 30) if sid in personalized_models]\n",
        "test_subject_id = test_subjects[0]  # choose a test subject\n",
        "\n",
        "best_subset, best_acc = find_best_similar_subjects(\n",
        "    train_subject_ids=available_train_subjects,\n",
        "    personalized_models=personalized_models,\n",
        "    test_X=test_X, test_y_encoded=test_y_encoded,\n",
        "    input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim,\n",
        "    max_subset_size=5\n",
        ")\n",
        "\n",
        "print(f\"✅ Best subset for test subject {test_subject_id}: {best_subset}, acc={best_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTWSoebngdBo"
      },
      "source": [
        "Assemble a model from the most similar train subjects in terms of feature or embedding similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5M9NwUNqC55",
        "outputId": "9f67f35a-da35-4b46-f9ca-73030ddfa9b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Built averaged personalized model for test subject 16\n",
            "✅ Accuracy of averaged model on test subject 16: 87.98% (n=366)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def build_average_personalized_model(\n",
        "    similar_train_subjects: list,\n",
        "    personalized_models: dict,\n",
        "    input_dim: int,\n",
        "    hidden_dims: list,\n",
        "    output_dim: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a personalized model for a test subject by:\n",
        "      1) Loading the backbone (feature_extractor) from the general model\n",
        "      2) Averaging the personalized classifier layers of similar train subjects\n",
        "    Returns a ready-to-infer CPU/float32 model in eval() mode.\n",
        "    \"\"\"\n",
        "    if \"general\" not in personalized_models:\n",
        "        raise KeyError(\"personalized_models must contain the 'general' snapshot.\")\n",
        "    for sid in similar_train_subjects:\n",
        "        if sid not in personalized_models:\n",
        "            raise KeyError(f\"personalized_models has no classifier head for subject {sid}.\")\n",
        "\n",
        "    # ----- 1) Create fresh model and load backbone -----\n",
        "    model = FlexibleMLP(input_dim, hidden_dims, output_dim)\n",
        "    general_sd = personalized_models[\"general\"]\n",
        "    backbone_sd = {k: v for k, v in general_sd.items() if k.startswith(\"feature_extractor.\")}\n",
        "    model.load_state_dict(backbone_sd, strict=False)\n",
        "\n",
        "    # ----- 2) Average classifier layers of similar subjects -----\n",
        "    weight_list = []\n",
        "    bias_list = []\n",
        "    for sid in similar_train_subjects:\n",
        "        head = personalized_models[sid]  # dict with 'weight' and 'bias'\n",
        "        weight_list.append(head[\"weight\"])\n",
        "        bias_list.append(head[\"bias\"])\n",
        "\n",
        "    # Compute mean\n",
        "    avg_weight = torch.stack(weight_list).mean(dim=0)\n",
        "    avg_bias   = torch.stack(bias_list).mean(dim=0)\n",
        "\n",
        "    # ----- 3) Attach averaged classifier to the model -----\n",
        "    with torch.no_grad():\n",
        "        model.classifier.weight.copy_(avg_weight)\n",
        "        model.classifier.bias.copy_(avg_bias)\n",
        "\n",
        "    # Set dtype/device and eval mode\n",
        "    model = model.float().cpu().eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Example usage\n",
        "# =========================\n",
        "similar_train_subjects = similar_embedding_euclidean[:3]  # the train subjects to average\n",
        "test_subject_id = test_subjects[0]\n",
        "\n",
        "avg_model = build_average_personalized_model(\n",
        "    similar_train_subjects=similar_train_subjects,\n",
        "    personalized_models=personalized_models,\n",
        "    input_dim=input_dim,\n",
        "    hidden_dims=hidden_dims,\n",
        "    output_dim=output_dim\n",
        ")\n",
        "\n",
        "print(\"✅ Built averaged personalized model for test subject\", test_subject_id)\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluate on test subject\n",
        "# ---------------------------\n",
        "X_test_tensor = torch.from_numpy(test_X.astype(\"float32\"))\n",
        "\n",
        "avg_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = avg_model(X_test_tensor)\n",
        "    preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "acc = accuracy_score(test_y_encoded, preds) * 100\n",
        "print(f\"✅ Accuracy of averaged model on test subject {test_subject_id}: {acc:.2f}% (n={len(test_y_encoded)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z_3yAdcpIt-0",
        "outputId": "684bbd53-4668-4f44-f096-9a17a722eefd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================== Test Subject 1 ==================\n",
            "Train Loss=0.0359, Train Acc=0.9868 | Test Loss=0.0149, Test Acc=0.9942\n",
            "General model accuracy: 99.42%\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 1: 99.42% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 1: 99.42% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 1: 99.71% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 1: 99.71% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 1: 99.14% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 1: 98.27% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 1: 99.42% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 1: 99.71% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 1: 99.42% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 1: 99.71% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 1: 99.42% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 1: 100.00% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 1: 99.71% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 1: 98.56% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 1: 100.00% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 1: 99.14% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 1: 99.14% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 1: 99.71% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 1: 99.14% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 1: 99.42% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 1: 100.00% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 1: 100.00% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 1: 98.85% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 1: 100.00% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 1: 99.71% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 1: 100.00% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 1: 99.14% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 1: 99.71% (n=347)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 1: 99.71% (n=347)\n",
            "\n",
            "Top-1 similarity subjects (input): [13], accuracy: 100.00%\n",
            "Top-1 similarity subjects (embedding): [13], accuracy: 100.00%\n",
            "Best subset: [13], accuracy: 100.00%\n",
            "\n",
            "================== Test Subject 2 ==================\n",
            "Train Loss=0.0322, Train Acc=0.9871 | Test Loss=0.3192, Test Acc=0.9007\n",
            "General model accuracy: 90.07%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 2: 97.68% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 2: 96.69% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 2: 94.04% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 2: 98.34% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 2: 92.38% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 2: 98.34% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 2: 99.67% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 2: 100.00% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 2: 98.68% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 2: 99.34% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 2: 97.02% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 2: 97.68% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 2: 95.70% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 2: 97.35% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 2: 97.35% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 2: 96.36% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 2: 97.35% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 2: 99.01% (n=302)\n",
            "\n",
            "Top-1 similarity subjects (input): [12], accuracy: 99.01%\n",
            "Top-1 similarity subjects (embedding): [12], accuracy: 99.01%\n",
            "Best subset: [18], accuracy: 100.00%\n",
            "\n",
            "================== Test Subject 3 ==================\n",
            "Train Loss=0.0356, Train Acc=0.9868 | Test Loss=0.0228, Test Acc=0.9912\n",
            "General model accuracy: 99.12%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 3: 98.83% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 3: 99.41% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 3: 97.65% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 3: 96.48% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 3: 98.53% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 3: 96.19% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 3: 97.95% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 3: 98.53% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 3: 97.36% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 3: 98.83% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 3: 98.83% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 3: 98.83% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 3: 98.24% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 3: 97.95% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 3: 97.95% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 3: 96.77% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 3: 97.65% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 3: 98.83% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 3: 99.71% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 3: 97.65% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 3: 98.83% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 3: 98.53% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 3: 97.07% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 3: 99.12% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 3: 97.65% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 3: 98.83% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 3: 98.53% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 3: 98.53% (n=341)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 3: 99.12% (n=341)\n",
            "\n",
            "Top-1 similarity subjects (input): [12], accuracy: 98.83%\n",
            "Top-1 similarity subjects (embedding): [12], accuracy: 98.83%\n",
            "Best subset: [20], accuracy: 99.71%\n",
            "\n",
            "================== Test Subject 4 ==================\n",
            "Train Loss=0.0331, Train Acc=0.9875 | Test Loss=0.1029, Test Acc=0.9621\n",
            "General model accuracy: 96.21%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 4: 88.01% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 4: 93.06% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 4: 94.95% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 4: 96.53% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 4: 96.85% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 4: 97.16% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 4: 91.80% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 4: 92.74% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 4: 96.53% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 4: 94.01% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 4: 95.27% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 4: 87.70% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 4: 85.49% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 4: 95.90% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 4: 96.21% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 4: 92.43% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 4: 90.54% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 4: 95.27% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 4: 87.70% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 4: 94.64% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 4: 94.32% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 4: 94.01% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 4: 96.21% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 4: 93.69% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 4: 96.53% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 4: 93.69% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 4: 94.95% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 4: 97.16% (n=317)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 4: 92.74% (n=317)\n",
            "\n",
            "Top-1 similarity subjects (input): [3], accuracy: 94.95%\n",
            "Top-1 similarity subjects (embedding): [3], accuracy: 94.95%\n",
            "Best subset: [7], accuracy: 97.16%\n",
            "\n",
            "================== Test Subject 5 ==================\n",
            "Train Loss=0.0283, Train Acc=0.9898 | Test Loss=0.3043, Test Acc=0.9007\n",
            "General model accuracy: 90.07%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 5: 91.06% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 5: 88.74% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 5: 89.74% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 5: 91.72% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 5: 89.74% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 5: 90.07% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 5: 93.71% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 5: 91.06% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 5: 90.40% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 5: 91.06% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 5: 89.74% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 5: 89.74% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 5: 88.08% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 5: 90.40% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 5: 90.73% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 5: 88.74% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 5: 88.08% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 5: 90.40% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 5: 87.42% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 5: 90.07% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 5: 91.72% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 5: 93.05% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 5: 93.71% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 5: 90.73% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 5: 91.39% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 5: 91.72% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 5: 90.73% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 5: 92.72% (n=302)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 5: 90.73% (n=302)\n",
            "\n",
            "Top-1 similarity subjects (input): [26], accuracy: 91.39%\n",
            "Top-1 similarity subjects (embedding): [26], accuracy: 91.39%\n",
            "Best subset: [8, 24], accuracy: 94.70%\n",
            "\n",
            "================== Test Subject 6 ==================\n",
            "Train Loss=0.0309, Train Acc=0.9882 | Test Loss=0.5231, Test Acc=0.8985\n",
            "General model accuracy: 89.85%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 6: 84.00% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 6: 87.08% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 6: 90.46% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 6: 90.77% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 6: 87.38% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 6: 88.62% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 6: 82.46% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 6: 88.62% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 6: 89.85% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 6: 86.77% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 6: 89.85% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 6: 84.92% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 6: 83.69% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 6: 89.54% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 6: 87.08% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 6: 86.46% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 6: 86.15% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 6: 90.15% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 6: 89.23% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 6: 88.62% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 6: 86.15% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 6: 88.62% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 6: 88.62% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 6: 86.15% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 6: 89.85% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 6: 86.46% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 6: 88.00% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 6: 89.54% (n=325)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 6: 86.15% (n=325)\n",
            "\n",
            "Top-1 similarity subjects (input): [5], accuracy: 87.38%\n",
            "Top-1 similarity subjects (embedding): [5], accuracy: 87.38%\n",
            "Best subset: [4, 15], accuracy: 91.38%\n",
            "\n",
            "================== Test Subject 7 ==================\n",
            "Train Loss=0.0315, Train Acc=0.9886 | Test Loss=0.4381, Test Acc=0.8831\n",
            "General model accuracy: 88.31%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 7: 90.58% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 7: 92.86% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 7: 94.16% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 7: 93.18% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 7: 93.18% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 7: 92.86% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 7: 88.31% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 7: 92.86% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 7: 92.53% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 7: 93.51% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 7: 92.53% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 7: 89.94% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 7: 88.31% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 7: 94.81% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 7: 95.45% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 7: 92.53% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 7: 86.04% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 7: 93.83% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 7: 89.94% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 7: 93.18% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 7: 92.21% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 7: 93.83% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 7: 96.10% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 7: 91.23% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 7: 95.78% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 7: 94.16% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 7: 91.88% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 7: 92.53% (n=308)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 7: 92.21% (n=308)\n",
            "\n",
            "Top-1 similarity subjects (input): [13], accuracy: 89.94%\n",
            "Top-1 similarity subjects (embedding): [13], accuracy: 89.94%\n",
            "Best subset: [24], accuracy: 96.10%\n",
            "\n",
            "================== Test Subject 8 ==================\n",
            "Train Loss=0.0385, Train Acc=0.9858 | Test Loss=0.1637, Test Acc=0.9324\n",
            "General model accuracy: 93.24%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 8: 97.15% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 8: 93.24% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 8: 91.81% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 8: 95.02% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 8: 93.24% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 8: 93.24% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 8: 86.48% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 8: 87.19% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 8: 91.10% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 8: 93.24% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 8: 93.95% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 8: 95.73% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 8: 93.95% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 8: 88.26% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 8: 94.31% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 8: 85.77% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 8: 94.66% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 8: 88.97% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 8: 91.81% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 8: 91.10% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 8: 92.88% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 8: 97.15% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 8: 94.66% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 8: 98.22% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 8: 93.24% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 8: 95.02% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 8: 91.46% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 8: 96.44% (n=281)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 8: 93.95% (n=281)\n",
            "\n",
            "Top-1 similarity subjects (input): [2], accuracy: 93.24%\n",
            "Top-1 similarity subjects (embedding): [19], accuracy: 88.97%\n",
            "Best subset: [25], accuracy: 98.22%\n",
            "\n",
            "================== Test Subject 9 ==================\n",
            "Train Loss=0.0354, Train Acc=0.9868 | Test Loss=0.2927, Test Acc=0.9028\n",
            "General model accuracy: 90.28%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 9: 92.36% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 9: 90.28% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 9: 93.06% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 9: 90.28% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 9: 89.24% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 9: 90.97% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 9: 94.10% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 9: 88.89% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 9: 93.06% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 9: 86.11% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 9: 89.93% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 9: 90.62% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 9: 86.46% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 9: 89.58% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 9: 89.93% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 9: 88.54% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 9: 84.03% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 9: 94.10% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 9: 89.93% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 9: 89.24% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 9: 92.36% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 9: 93.40% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 9: 91.67% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 9: 92.01% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 9: 89.93% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 9: 92.01% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 9: 87.85% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 9: 90.62% (n=288)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 9: 91.67% (n=288)\n",
            "\n",
            "Top-1 similarity subjects (input): [16], accuracy: 89.93%\n",
            "Top-1 similarity subjects (embedding): [16], accuracy: 89.93%\n",
            "Best subset: [7, 19], accuracy: 94.79%\n",
            "\n",
            "================== Test Subject 10 ==================\n",
            "Train Loss=0.0337, Train Acc=0.9870 | Test Loss=0.4640, Test Acc=0.8469\n",
            "General model accuracy: 84.69%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 10: 85.03% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 10: 87.07% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 10: 86.05% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 10: 85.03% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 10: 84.01% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 10: 85.37% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 10: 89.12% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 10: 81.63% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 10: 91.16% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 10: 82.31% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 10: 85.71% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 10: 86.05% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 10: 82.99% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 10: 87.07% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 10: 90.82% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 10: 81.63% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 10: 83.33% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 10: 86.05% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 10: 84.69% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 10: 89.80% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 10: 85.71% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 10: 87.76% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 10: 83.67% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 10: 82.99% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 10: 82.31% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 10: 86.05% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 10: 88.44% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 10: 89.46% (n=294)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 10: 89.12% (n=294)\n",
            "\n",
            "Top-1 similarity subjects (input): [15], accuracy: 87.07%\n",
            "Top-1 similarity subjects (embedding): [29], accuracy: 89.46%\n",
            "Best subset: [9, 29], accuracy: 92.86%\n",
            "\n",
            "================== Test Subject 11 ==================\n",
            "Train Loss=0.0330, Train Acc=0.9867 | Test Loss=0.0062, Test Acc=0.9968\n",
            "General model accuracy: 99.68%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 11: 100.00% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 11: 99.05% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 11: 99.37% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 11: 99.37% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 11: 99.37% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 11: 99.05% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 11: 100.00% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 11: 99.37% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 11: 100.00% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 11: 100.00% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 11: 100.00% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 11: 99.37% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 11: 99.05% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 11: 99.68% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 11: 99.37% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 11: 99.37% (n=316)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 11: 100.00% (n=316)\n",
            "\n",
            "Top-1 similarity subjects (input): [15], accuracy: 99.68%\n",
            "Top-1 similarity subjects (embedding): [25], accuracy: 99.68%\n",
            "Best subset: [2], accuracy: 100.00%\n",
            "\n",
            "================== Test Subject 12 ==================\n",
            "Train Loss=0.0337, Train Acc=0.9880 | Test Loss=0.1727, Test Acc=0.9312\n",
            "General model accuracy: 93.12%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 12: 95.00% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 12: 97.19% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 12: 96.88% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 12: 96.25% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 12: 93.75% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 12: 97.19% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 12: 95.94% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 12: 90.94% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 12: 97.19% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 12: 95.94% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 12: 97.50% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 12: 97.19% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 12: 96.88% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 12: 97.50% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 12: 96.25% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 12: 97.19% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 12: 97.81% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 12: 95.94% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 12: 96.88% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 12: 96.88% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 12: 96.56% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 12: 96.25% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 12: 98.12% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 12: 92.19% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 12: 97.19% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 12: 95.31% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 12: 98.44% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 12: 94.69% (n=320)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 12: 98.12% (n=320)\n",
            "\n",
            "Top-1 similarity subjects (input): [3], accuracy: 96.88%\n",
            "Top-1 similarity subjects (embedding): [3], accuracy: 96.88%\n",
            "Best subset: [28, 11], accuracy: 98.75%\n",
            "\n",
            "================== Test Subject 13 ==================\n",
            "Train Loss=0.0344, Train Acc=0.9859 | Test Loss=0.0560, Test Acc=0.9878\n",
            "General model accuracy: 98.78%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 13: 99.39% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 13: 98.78% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 13: 98.78% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 13: 98.78% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 13: 97.25% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 13: 99.08% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 13: 98.17% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 13: 98.47% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 13: 93.58% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 13: 97.55% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 13: 97.25% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 13: 98.78% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 13: 99.39% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 13: 95.72% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 13: 98.47% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 13: 95.41% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 13: 99.39% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 13: 97.55% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 13: 98.78% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 13: 97.55% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 13: 95.11% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 13: 99.08% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 13: 98.17% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 13: 98.47% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 13: 98.17% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 13: 98.78% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 13: 98.17% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 13: 99.08% (n=327)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 13: 98.47% (n=327)\n",
            "\n",
            "Top-1 similarity subjects (input): [17], accuracy: 95.41%\n",
            "Top-1 similarity subjects (embedding): [21], accuracy: 97.55%\n",
            "Best subset: [1], accuracy: 99.39%\n",
            "\n",
            "================== Test Subject 14 ==================\n",
            "Train Loss=0.0359, Train Acc=0.9850 | Test Loss=1.3039, Test Acc=0.7864\n",
            "General model accuracy: 78.64%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 14: 79.88% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 14: 80.50% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 14: 80.19% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 14: 75.85% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 14: 74.30% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 14: 79.88% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 14: 78.02% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 14: 75.85% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 14: 79.26% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 14: 79.57% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 14: 81.42% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 14: 78.95% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 14: 80.80% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 14: 79.88% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 14: 81.11% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 14: 81.73% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 14: 82.04% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 14: 80.19% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 14: 82.66% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 14: 81.42% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 14: 80.19% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 14: 79.57% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 14: 76.16% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 14: 79.57% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 14: 77.40% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 14: 79.57% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 14: 78.02% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 14: 78.95% (n=323)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 14: 80.50% (n=323)\n",
            "\n",
            "Top-1 similarity subjects (input): [8], accuracy: 75.85%\n",
            "Top-1 similarity subjects (embedding): [8], accuracy: 75.85%\n",
            "Best subset: [20, 17], accuracy: 82.97%\n",
            "\n",
            "================== Test Subject 15 ==================\n",
            "Train Loss=0.0369, Train Acc=0.9856 | Test Loss=0.0158, Test Acc=1.0000\n",
            "General model accuracy: 100.00%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 15: 97.56% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 15: 99.39% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 15: 99.70% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 15: 100.00% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 15: 99.39% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 15: 100.00% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 15: 99.70% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 15: 98.17% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 15: 100.00% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 15: 100.00% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 15: 100.00% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 15: 100.00% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 15: 97.26% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 15: 97.87% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 15: 99.70% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 15: 99.39% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 15: 98.17% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 15: 100.00% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 15: 99.09% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 15: 100.00% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 15: 100.00% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 15: 99.39% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 15: 99.70% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 15: 99.39% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 15: 99.70% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 15: 99.39% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 15: 99.09% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 15: 99.70% (n=328)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 15: 100.00% (n=328)\n",
            "\n",
            "Top-1 similarity subjects (input): [11], accuracy: 100.00%\n",
            "Top-1 similarity subjects (embedding): [12], accuracy: 100.00%\n",
            "Best subset: [4], accuracy: 100.00%\n",
            "\n",
            "================== Test Subject 16 ==================\n",
            "Train Loss=0.0300, Train Acc=0.9894 | Test Loss=0.3189, Test Acc=0.8962\n",
            "General model accuracy: 89.62%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 16: 86.61% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 16: 85.79% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 16: 86.61% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 16: 86.61% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 16: 88.80% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 16: 84.97% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 16: 87.43% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 16: 88.52% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 16: 89.34% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 16: 89.89% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 16: 86.34% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 16: 86.89% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 16: 88.25% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 16: 88.52% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 16: 86.34% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 16: 84.43% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 16: 87.70% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 16: 89.07% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 16: 87.43% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 16: 87.70% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 16: 84.97% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 16: 90.16% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 16: 88.80% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 16: 86.07% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 16: 87.70% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 16: 87.16% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 16: 84.70% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 16: 86.89% (n=366)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 16: 88.52% (n=366)\n",
            "\n",
            "Top-1 similarity subjects (input): [17], accuracy: 84.43%\n",
            "Top-1 similarity subjects (embedding): [24], accuracy: 88.80%\n",
            "Best subset: [23, 9], accuracy: 90.44%\n",
            "\n",
            "================== Test Subject 17 ==================\n",
            "Train Loss=0.0298, Train Acc=0.9887 | Test Loss=0.0392, Test Acc=0.9837\n",
            "General model accuracy: 98.37%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 17: 96.47% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 17: 98.10% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 17: 98.91% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 17: 96.74% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 17: 92.93% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 17: 98.37% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 17: 98.64% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 17: 94.02% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 17: 99.46% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 17: 98.10% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 17: 98.10% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 17: 98.37% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 17: 97.55% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 17: 97.55% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 17: 99.18% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 17: 96.74% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 17: 98.64% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 17: 98.64% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 17: 98.37% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 17: 99.46% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 17: 96.47% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 17: 97.83% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 17: 96.47% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 17: 97.55% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 17: 97.01% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 17: 96.20% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 17: 97.55% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 17: 96.20% (n=368)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 17: 98.91% (n=368)\n",
            "\n",
            "Top-1 similarity subjects (input): [13], accuracy: 97.55%\n",
            "Top-1 similarity subjects (embedding): [24], accuracy: 96.47%\n",
            "Best subset: [9], accuracy: 99.46%\n",
            "\n",
            "================== Test Subject 18 ==================\n",
            "Train Loss=0.0256, Train Acc=0.9910 | Test Loss=0.0793, Test Acc=0.9808\n",
            "General model accuracy: 98.08%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 18: 98.63% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 18: 98.35% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 18: 98.63% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 18: 96.43% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 18: 96.43% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 18: 95.88% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 18: 95.05% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 18: 97.80% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 18: 97.80% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 18: 96.70% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 18: 97.80% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 18: 98.08% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 18: 98.35% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 18: 99.73% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 18: 96.15% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 18: 97.80% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 18: 97.25% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 18: 97.25% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 18: 99.18% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 18: 98.63% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 18: 97.80% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 18: 96.98% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 18: 94.51% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 18: 97.80% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 18: 94.23% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 18: 97.80% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 18: 97.80% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 18: 97.80% (n=364)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 18: 98.90% (n=364)\n",
            "\n",
            "Top-1 similarity subjects (input): [11], accuracy: 97.80%\n",
            "Top-1 similarity subjects (embedding): [30], accuracy: 98.90%\n",
            "Best subset: [14], accuracy: 99.73%\n",
            "\n",
            "================== Test Subject 19 ==================\n",
            "Train Loss=0.0336, Train Acc=0.9876 | Test Loss=0.2467, Test Acc=0.9139\n",
            "General model accuracy: 91.39%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 19: 89.44% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 19: 93.33% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 19: 94.17% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 19: 89.17% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 19: 93.89% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 19: 89.72% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 19: 93.33% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 19: 91.39% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 19: 94.17% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 19: 94.17% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 19: 92.22% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 19: 90.83% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 19: 85.83% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 19: 89.17% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 19: 91.67% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 19: 88.06% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 19: 88.06% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 19: 88.06% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 19: 91.94% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 19: 90.56% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 19: 94.17% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 19: 91.11% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 19: 90.83% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 19: 91.11% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 19: 96.39% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 19: 95.56% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 19: 90.00% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 19: 87.78% (n=360)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 19: 91.94% (n=360)\n",
            "\n",
            "Top-1 similarity subjects (input): [8], accuracy: 91.39%\n",
            "Top-1 similarity subjects (embedding): [8], accuracy: 91.39%\n",
            "Best subset: [26], accuracy: 96.39%\n",
            "\n",
            "================== Test Subject 20 ==================\n",
            "Train Loss=0.0314, Train Acc=0.9885 | Test Loss=0.1571, Test Acc=0.9605\n",
            "General model accuracy: 96.05%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 20: 96.33% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 20: 96.89% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 20: 94.92% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 20: 90.40% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 20: 90.11% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 20: 95.20% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 20: 90.11% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 20: 91.24% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 20: 93.50% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 20: 92.94% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 20: 95.48% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 20: 94.35% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 20: 98.59% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 20: 98.31% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 20: 95.76% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 20: 94.07% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 20: 96.05% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 20: 98.02% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 20: 92.66% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 20: 95.76% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 20: 94.63% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 20: 94.35% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 20: 91.81% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 20: 95.48% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 20: 92.37% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 20: 93.50% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 20: 96.05% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 20: 91.81% (n=354)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 20: 96.05% (n=354)\n",
            "\n",
            "Top-1 similarity subjects (input): [6], accuracy: 95.20%\n",
            "Top-1 similarity subjects (embedding): [22], accuracy: 94.63%\n",
            "Best subset: [13], accuracy: 98.59%\n",
            "\n",
            "================== Test Subject 21 ==================\n",
            "Train Loss=0.0266, Train Acc=0.9907 | Test Loss=0.1185, Test Acc=0.9583\n",
            "General model accuracy: 95.83%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 21: 95.10% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 21: 97.79% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 21: 97.79% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 21: 96.81% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 21: 91.67% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 21: 97.79% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 21: 98.04% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 21: 89.95% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 21: 96.81% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 21: 95.34% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 21: 96.32% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 21: 97.79% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 21: 96.57% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 21: 97.55% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 21: 97.79% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 21: 94.12% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 21: 97.55% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 21: 98.04% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 21: 96.08% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 21: 98.28% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 21: 92.89% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 21: 96.32% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 21: 95.83% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 21: 92.89% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 21: 94.85% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 21: 91.67% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 21: 96.57% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 21: 94.12% (n=408)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 21: 97.30% (n=408)\n",
            "\n",
            "Top-1 similarity subjects (input): [13], accuracy: 96.57%\n",
            "Top-1 similarity subjects (embedding): [13], accuracy: 96.57%\n",
            "Best subset: [20], accuracy: 98.28%\n",
            "\n",
            "================== Test Subject 22 ==================\n",
            "Train Loss=0.0343, Train Acc=0.9873 | Test Loss=0.0284, Test Acc=0.9875\n",
            "General model accuracy: 98.75%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 22: 97.51% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 22: 99.69% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 22: 99.07% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 22: 96.57% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 22: 98.44% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 22: 97.20% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 22: 98.44% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 22: 96.88% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 22: 99.38% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 22: 99.69% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 22: 99.38% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 22: 99.07% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 22: 97.20% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 22: 96.26% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 22: 99.38% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 22: 98.75% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 22: 99.38% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 22: 98.13% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 22: 99.38% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 22: 98.75% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 22: 99.69% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 22: 97.51% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 22: 99.07% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 22: 100.00% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 22: 98.75% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 22: 99.07% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 22: 99.38% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 22: 96.88% (n=321)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 22: 99.07% (n=321)\n",
            "\n",
            "Top-1 similarity subjects (input): [2], accuracy: 99.69%\n",
            "Top-1 similarity subjects (embedding): [2], accuracy: 99.69%\n",
            "Best subset: [25], accuracy: 100.00%\n",
            "\n",
            "================== Test Subject 23 ==================\n",
            "Train Loss=0.0363, Train Acc=0.9854 | Test Loss=0.0415, Test Acc=0.9785\n",
            "General model accuracy: 97.85%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 23: 98.66% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 23: 97.85% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 23: 97.58% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 23: 98.12% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 23: 98.12% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 23: 97.58% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 23: 98.39% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 23: 97.31% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 23: 96.24% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 23: 98.12% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 23: 97.31% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 23: 97.85% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 23: 97.85% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 23: 97.31% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 23: 97.85% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 23: 98.12% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 23: 96.77% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 23: 96.77% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 23: 97.58% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 23: 97.58% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 23: 97.58% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 23: 98.39% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 23: 98.39% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 23: 98.92% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 23: 98.12% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 23: 98.92% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 23: 95.70% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 23: 98.66% (n=372)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 23: 98.12% (n=372)\n",
            "\n",
            "Top-1 similarity subjects (input): [7], accuracy: 98.39%\n",
            "Top-1 similarity subjects (embedding): [13], accuracy: 97.85%\n",
            "Best subset: [25], accuracy: 98.92%\n",
            "\n",
            "================== Test Subject 24 ==================\n",
            "Train Loss=0.0318, Train Acc=0.9884 | Test Loss=0.0295, Test Acc=0.9869\n",
            "General model accuracy: 98.69%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 24: 96.59% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 24: 97.90% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 24: 98.69% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 24: 99.48% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 24: 98.95% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 24: 99.48% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 24: 99.48% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 24: 96.59% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 24: 98.16% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 24: 98.43% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 24: 97.90% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 24: 98.43% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 24: 98.16% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 24: 96.06% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 24: 99.21% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 24: 98.95% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 24: 98.16% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 24: 96.06% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 24: 98.95% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 24: 96.06% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 24: 98.43% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 24: 99.21% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 24: 98.16% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 24: 98.43% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 24: 100.00% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 24: 98.43% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 24: 96.06% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 24: 98.43% (n=381)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 24: 98.16% (n=381)\n",
            "\n",
            "Top-1 similarity subjects (input): [26], accuracy: 100.00%\n",
            "Top-1 similarity subjects (embedding): [26], accuracy: 100.00%\n",
            "Best subset: [26], accuracy: 100.00%\n",
            "\n",
            "================== Test Subject 25 ==================\n",
            "Train Loss=0.0318, Train Acc=0.9878 | Test Loss=0.3985, Test Acc=0.8924\n",
            "General model accuracy: 89.24%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 25: 94.13% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 25: 90.46% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 25: 89.24% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 25: 89.73% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 25: 88.51% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 25: 88.75% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 25: 85.82% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 25: 92.91% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 25: 90.46% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 25: 89.00% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 25: 91.93% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 25: 87.53% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 25: 91.93% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 25: 93.89% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 25: 86.06% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 25: 89.98% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 25: 89.73% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 25: 92.18% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 25: 89.24% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 25: 91.20% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 25: 87.78% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 25: 90.22% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 25: 89.49% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 25: 86.80% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 25: 88.51% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 25: 91.69% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 25: 87.04% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 25: 91.44% (n=409)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 25: 91.20% (n=409)\n",
            "\n",
            "Top-1 similarity subjects (input): [18], accuracy: 92.18%\n",
            "Top-1 similarity subjects (embedding): [18], accuracy: 92.18%\n",
            "Best subset: [1, 14], accuracy: 94.87%\n",
            "\n",
            "================== Test Subject 26 ==================\n",
            "Train Loss=0.0355, Train Acc=0.9870 | Test Loss=0.0255, Test Acc=0.9847\n",
            "General model accuracy: 98.47%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 26: 97.96% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 26: 98.21% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 26: 97.96% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 26: 99.49% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 26: 99.49% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 26: 98.47% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 26: 98.21% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 26: 97.96% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 26: 97.45% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 26: 98.72% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 26: 99.23% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 26: 96.94% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 26: 96.94% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 26: 94.13% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 26: 97.96% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 26: 96.17% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 26: 97.70% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 26: 96.43% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 26: 98.47% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 26: 96.68% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 26: 94.64% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 26: 99.49% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 26: 98.47% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 26: 98.72% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 26: 99.74% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 26: 99.74% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 26: 94.13% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 26: 98.47% (n=392)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 26: 98.21% (n=392)\n",
            "\n",
            "Top-1 similarity subjects (input): [24], accuracy: 98.72%\n",
            "Top-1 similarity subjects (embedding): [3], accuracy: 97.96%\n",
            "Best subset: [25, 6], accuracy: 100.00%\n",
            "\n",
            "================== Test Subject 27 ==================\n",
            "Train Loss=0.0355, Train Acc=0.9861 | Test Loss=0.0323, Test Acc=0.9867\n",
            "General model accuracy: 98.67%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 27: 98.94% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 27: 97.61% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 27: 97.87% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 27: 99.20% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 27: 98.40% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 27: 98.14% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 27: 97.61% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 27: 98.40% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 27: 96.81% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 27: 98.14% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 27: 98.40% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 27: 97.87% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 27: 98.40% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 27: 98.14% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 27: 97.87% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 27: 99.20% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 27: 98.14% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 27: 97.34% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 27: 98.14% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 27: 97.34% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 27: 98.14% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 27: 99.20% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 27: 98.94% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 27: 98.94% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 27: 99.20% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 27: 98.67% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 27: 97.34% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 27: 99.47% (n=376)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 27: 97.87% (n=376)\n",
            "\n",
            "Top-1 similarity subjects (input): [26], accuracy: 98.67%\n",
            "Top-1 similarity subjects (embedding): [26], accuracy: 98.67%\n",
            "Best subset: [29], accuracy: 99.47%\n",
            "\n",
            "================== Test Subject 28 ==================\n",
            "Train Loss=0.0369, Train Acc=0.9864 | Test Loss=0.1424, Test Acc=0.9450\n",
            "General model accuracy: 94.50%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 28: 91.88% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 28: 96.86% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 28: 96.86% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 28: 94.76% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 28: 91.62% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 28: 96.60% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 28: 94.50% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 28: 88.48% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 28: 95.55% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 28: 94.50% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 28: 95.29% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 28: 97.64% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 28: 97.12% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 28: 94.50% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 28: 96.60% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 28: 94.24% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 28: 96.60% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 28: 97.12% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 28: 93.72% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 28: 97.64% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 28: 96.86% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 28: 91.62% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 28: 93.98% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 28: 92.67% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 28: 91.10% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 28: 92.41% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 28: 91.10% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 28: 91.10% (n=382)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 28: 95.29% (n=382)\n",
            "\n",
            "Top-1 similarity subjects (input): [12], accuracy: 97.64%\n",
            "Top-1 similarity subjects (embedding): [12], accuracy: 97.64%\n",
            "Best subset: [12], accuracy: 97.64%\n",
            "\n",
            "================== Test Subject 29 ==================\n",
            "Train Loss=0.0339, Train Acc=0.9865 | Test Loss=0.0178, Test Acc=1.0000\n",
            "General model accuracy: 100.00%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 29: 99.13% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 29: 94.48% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 29: 95.35% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 29: 98.55% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 29: 99.13% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 29: 94.77% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 29: 98.26% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 29: 99.13% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 29: 98.26% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 29: 97.38% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 29: 97.09% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 29: 95.06% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 29: 97.67% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 29: 95.64% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 29: 95.35% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 29: 98.55% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 29: 95.35% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 29: 94.77% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 29: 96.51% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 29: 94.19% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 29: 94.19% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 29: 99.13% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 29: 97.67% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 29: 98.55% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 29: 100.00% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 29: 98.84% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 29: 99.42% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 29: 92.44% (n=344)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "✅ Train Subj 30 → Test Subj 29: 94.77% (n=344)\n",
            "\n",
            "Top-1 similarity subjects (input): [15], accuracy: 95.35%\n",
            "Top-1 similarity subjects (embedding): [27], accuracy: 99.42%\n",
            "Best subset: [25], accuracy: 100.00%\n",
            "\n",
            "================== Test Subject 30 ==================\n",
            "Train Loss=0.0340, Train Acc=0.9868 | Test Loss=0.0110, Test Acc=1.0000\n",
            "General model accuracy: 100.00%\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "✅ Train Subj 1 → Test Subj 30: 99.48% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "✅ Train Subj 2 → Test Subj 30: 99.48% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "✅ Train Subj 3 → Test Subj 30: 99.22% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "✅ Train Subj 4 → Test Subj 30: 98.43% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "✅ Train Subj 5 → Test Subj 30: 99.22% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "✅ Train Subj 6 → Test Subj 30: 99.74% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "✅ Train Subj 7 → Test Subj 30: 98.69% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "✅ Train Subj 8 → Test Subj 30: 97.65% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "✅ Train Subj 9 → Test Subj 30: 100.00% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "✅ Train Subj 10 → Test Subj 30: 100.00% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "✅ Train Subj 11 → Test Subj 30: 100.00% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "✅ Train Subj 12 → Test Subj 30: 98.96% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "✅ Train Subj 13 → Test Subj 30: 100.00% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 14\n",
            "✅ Train Subj 14 → Test Subj 30: 100.00% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "✅ Train Subj 15 → Test Subj 30: 99.74% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "✅ Train Subj 16 → Test Subj 30: 98.69% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "✅ Train Subj 17 → Test Subj 30: 99.22% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "✅ Train Subj 18 → Test Subj 30: 99.22% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "✅ Train Subj 19 → Test Subj 30: 99.74% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "✅ Train Subj 20 → Test Subj 30: 98.96% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "✅ Train Subj 21 → Test Subj 30: 100.00% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "✅ Train Subj 22 → Test Subj 30: 99.22% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "✅ Train Subj 23 → Test Subj 30: 99.48% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "✅ Train Subj 24 → Test Subj 30: 99.48% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "✅ Train Subj 25 → Test Subj 30: 98.17% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "✅ Train Subj 26 → Test Subj 30: 99.48% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "✅ Train Subj 27 → Test Subj 30: 99.22% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "✅ Train Subj 28 → Test Subj 30: 98.69% (n=383)\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "✅ Train Subj 29 → Test Subj 30: 99.22% (n=383)\n",
            "\n",
            "Top-1 similarity subjects (input): [18], accuracy: 99.22%\n",
            "Top-1 similarity subjects (embedding): [1], accuracy: 99.48%\n",
            "Best subset: [9], accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "results = {}\n",
        "K = 1  # number of most similar subjects\n",
        "all_test_subjects = np.unique(all_subjects)\n",
        "\n",
        "for test_subject_id in all_test_subjects:\n",
        "    print(f\"\\n================== Test Subject {test_subject_id} ==================\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # Split data for this LOSO round\n",
        "    # ---------------------------\n",
        "    train_mask = all_subjects != test_subject_id\n",
        "    test_mask  = all_subjects == test_subject_id\n",
        "\n",
        "    train_X, train_y = all_X[train_mask], all_y[train_mask]\n",
        "    test_X, test_y   = all_X[test_mask], all_y[test_mask]\n",
        "    train_subjects   = all_subjects[train_mask]\n",
        "\n",
        "    # ---------------------------\n",
        "    # Encode labels (fit only on train subjects)\n",
        "    # ---------------------------\n",
        "    label_encoder = LabelEncoder()\n",
        "    train_y_encoded = label_encoder.fit_transform(train_y)\n",
        "    test_y_encoded  = label_encoder.transform(test_y)\n",
        "\n",
        "    train_X = train_X.astype(\"float32\")\n",
        "    test_X  = test_X.astype(\"float32\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # Train new general model for current LOSO split\n",
        "    # ---------------------------\n",
        "    input_dim = train_X.shape[1]\n",
        "    hidden_dims = [100, 100, 100]\n",
        "    output_dim = len(label_encoder.classes_)\n",
        "\n",
        "    general_model = FlexibleMLP(input_dim, hidden_dims, output_dim)\n",
        "    general_model = train_model(\n",
        "        train_X, train_y_encoded,\n",
        "        general_model,\n",
        "        test_X, test_y_encoded,\n",
        "        epochs=200, lr=0.01, batch_size=64\n",
        "    )\n",
        "\n",
        "    acc_general = evaluate_model(general_model, test_X, test_y_encoded)\n",
        "    print(f\"General model accuracy: {acc_general*100:.2f}%\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # Fine-tune per train subject\n",
        "    # ---------------------------\n",
        "    personalized_models = {\"general\": general_model.state_dict()}\n",
        "    unique_train_subjects = np.unique(train_subjects)\n",
        "\n",
        "    for subj in unique_train_subjects:\n",
        "        print(f\"🔹 Fine-tuning on Train Subject {subj}\")\n",
        "\n",
        "        subj_mask = train_subjects == subj\n",
        "        subj_X, subj_y = train_X[subj_mask], train_y_encoded[subj_mask]\n",
        "\n",
        "        # Convert to tensors for PyTorch\n",
        "        subj_X_tensor = torch.from_numpy(subj_X)\n",
        "        subj_y_tensor = torch.from_numpy(subj_y)\n",
        "\n",
        "        # Clone general model\n",
        "        personal_model = FlexibleMLP(input_dim, hidden_dims, output_dim)\n",
        "        personal_model.load_state_dict(general_model.state_dict())\n",
        "\n",
        "        # Freeze all layers except classifier (optional: last feature layer)\n",
        "        for param in personal_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in personal_model.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Optimizer for trainable parameters\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, personal_model.parameters()), lr=0.005)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # Simple training loop\n",
        "        personal_model.train()\n",
        "        batch_size = 16\n",
        "        epochs_ft = 30\n",
        "        dataset = torch.utils.data.TensorDataset(subj_X_tensor, subj_y_tensor)\n",
        "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(epochs_ft):\n",
        "            for batch_X, batch_y in loader:\n",
        "                optimizer.zero_grad()\n",
        "                logits = personal_model(batch_X.float())\n",
        "                loss = criterion(logits, batch_y.long())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Save personalized classifier\n",
        "        personalized_models[subj] = {\n",
        "            \"weight\": personal_model.classifier.weight.clone(),\n",
        "            \"bias\": personal_model.classifier.bias.clone()\n",
        "        }\n",
        "\n",
        "        # ---------------------------\n",
        "        # Evaluate personalized model on test subject\n",
        "        # ---------------------------\n",
        "        model_eval = FlexibleMLP(input_dim, hidden_dims, output_dim)\n",
        "        model_eval.load_state_dict(general_model.state_dict())\n",
        "        with torch.no_grad():\n",
        "            model_eval.classifier.weight.copy_(personalized_models[subj][\"weight\"])\n",
        "            model_eval.classifier.bias.copy_(personalized_models[subj][\"bias\"])\n",
        "\n",
        "        X_test_tensor = torch.from_numpy(test_X)\n",
        "        model_eval.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = model_eval(X_test_tensor.float()).argmax(dim=1).numpy()\n",
        "\n",
        "        acc = accuracy_score(test_y_encoded, preds) * 100\n",
        "        print(f\"✅ Train Subj {subj} → Test Subj {test_subject_id}: {acc:.2f}% (n={len(test_y_encoded)})\\n\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # Compute similarities and assemble top-K or best subset\n",
        "    # ---------------------------\n",
        "    similar_input_cosine, similar_input_euclidean, similar_embedding_cosine, similar_embedding_euclidean = compute_similar_subjects(\n",
        "        train_X, train_y, train_subjects,\n",
        "        test_X, test_y, np.repeat(test_subject_id, len(test_X)),\n",
        "        test_subject_id=test_subject_id,\n",
        "        input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim\n",
        "    )\n",
        "\n",
        "    topK_subjects_input = similar_input_cosine[:K]\n",
        "    avg_model_topK_input = build_average_personalized_model(\n",
        "        similar_train_subjects=topK_subjects_input,\n",
        "        personalized_models=personalized_models,\n",
        "        input_dim=input_dim,\n",
        "        hidden_dims=hidden_dims,\n",
        "        output_dim=output_dim\n",
        "    )\n",
        "    acc_topK_input = evaluate_model(avg_model_topK_input, test_X, test_y_encoded)\n",
        "    print(f\"Top-{K} similarity subjects (input): {topK_subjects_input}, accuracy: {acc_topK_input*100:.2f}%\")\n",
        "\n",
        "    topK_subjects_embedding = similar_embedding_cosine[:K]\n",
        "    avg_model_topK_embedding = build_average_personalized_model(\n",
        "        similar_train_subjects=topK_subjects_embedding,\n",
        "        personalized_models=personalized_models,\n",
        "        input_dim=input_dim,\n",
        "        hidden_dims=hidden_dims,\n",
        "        output_dim=output_dim\n",
        "    )\n",
        "    acc_topK_embedding = evaluate_model(avg_model_topK_embedding, test_X, test_y_encoded)\n",
        "    print(f\"Top-{K} similarity subjects (embedding): {topK_subjects_embedding}, accuracy: {acc_topK_embedding*100:.2f}%\")\n",
        "\n",
        "    available_train_subjects = [sid for sid in similar_embedding_cosine if sid in personalized_models]\n",
        "    best_subset, best_acc = find_best_similar_subjects(\n",
        "        train_subject_ids=available_train_subjects,\n",
        "        personalized_models=personalized_models,\n",
        "        test_X=test_X, test_y_encoded=test_y_encoded,\n",
        "        input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim,\n",
        "        max_subset_size=3\n",
        "    )\n",
        "    print(f\"Best subset: {best_subset}, accuracy: {best_acc*100:.2f}%\")\n",
        "\n",
        "    results[test_subject_id] = {\n",
        "        \"general_acc\": acc_general,\n",
        "        \"topK_similar_acc_input\": acc_topK_input,\n",
        "        \"topK_similar_acc_embedding\": acc_topK_embedding,\n",
        "        \"best_subset_acc\": best_acc,\n",
        "        \"best_subset\": best_subset,\n",
        "        \"topK_subjects_input\": topK_subjects_input,\n",
        "        \"topK_subjects_embedding\": topK_subjects_embedding\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrhPP5ir1vti",
        "outputId": "ab9fa1c2-0815-4935-b8b7-ec50571f4515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======= LOSO Summary =======\n",
            "mean_general_acc: 94.57%\n",
            "mean_topK_similar_acc_input: 94.75%\n",
            "mean_topK_similar_acc_embedding: 94.98%\n",
            "mean_best_subset_acc: 97.33%\n"
          ]
        }
      ],
      "source": [
        "general_accs = [v[\"general_acc\"] for v in results.values()]\n",
        "topK_input_accs = [v[\"topK_similar_acc_input\"] for v in results.values()]\n",
        "topK_embed_accs = [v[\"topK_similar_acc_embedding\"] for v in results.values()]\n",
        "best_subset_accs = [v[\"best_subset_acc\"] for v in results.values()]\n",
        "\n",
        "mean_results = {\n",
        "    \"mean_general_acc\": np.mean(general_accs),\n",
        "    \"mean_topK_similar_acc_input\": np.mean(topK_input_accs),\n",
        "    \"mean_topK_similar_acc_embedding\": np.mean(topK_embed_accs),\n",
        "    \"mean_best_subset_acc\": np.mean(best_subset_accs)\n",
        "}\n",
        "\n",
        "print(\"\\n======= LOSO Summary =======\")\n",
        "for k, v in mean_results.items():\n",
        "    print(f\"{k}: {v*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncLbbmu0eIk4"
      },
      "source": [
        "# **Deep Learning On Raw Signals**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07yIljB-dmFB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Importing libraries\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, TimeDistributed, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxsDiRAhg0g1",
        "outputId": "bd75d48b-497b-459a-d654-221373a5af0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btbkg76xyOEF",
        "outputId": "ca2e3838-f22c-48aa-8554-3a96eb7c2b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, clone_model\n",
        "from tensorflow.keras.layers import TimeDistributed, Conv1D, MaxPooling1D, Dropout, Flatten, LSTM, Dense, InputLayer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "dataset_path = \"/content/drive/MyDrive/UCI_HAR_Dataset\"\n",
        "test_subjects = [14]  # hold-out subjects for testing\n",
        "batch_size = 32\n",
        "epochs_general = 30   # epochs for general model\n",
        "epochs_finetune = 5   # epochs for per-subject fine-tuning\n",
        "n_steps, n_length = 4, 32\n",
        "n_hidden = 16\n",
        "\n",
        "SIGNALS = [\n",
        "    \"body_acc_x\", \"body_acc_y\", \"body_acc_z\",\n",
        "    \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\",\n",
        "    \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"\n",
        "]\n",
        "\n",
        "def load_data(dataset_path, test_subjects):\n",
        "    def _read_csv(filename):\n",
        "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "    def load_signals(subset):\n",
        "        signals_data = []\n",
        "        for signal in SIGNALS:\n",
        "            filename = f\"{dataset_path}/{subset}/Inertial Signals/{signal}_{subset}.txt\"\n",
        "            signals_data.append(_read_csv(filename).to_numpy())\n",
        "        return np.transpose(signals_data, (1, 2, 0))  # (samples, timesteps=128, 9 signals)\n",
        "\n",
        "    def load_y(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/y_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    def load_subjects(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/subject_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    X_train, y_train, subj_train = load_signals(\"train\"), load_y(\"train\"), load_subjects(\"train\")\n",
        "    X_test,  y_test,  subj_test  = load_signals(\"test\"),  load_y(\"test\"),  load_subjects(\"test\")\n",
        "\n",
        "    X_all = np.vstack([X_train, X_test])\n",
        "    y_all = np.concatenate([y_train, y_test])\n",
        "    subjects = np.concatenate([subj_train, subj_test])\n",
        "\n",
        "    # Split by subject IDs\n",
        "    train_mask = ~np.isin(subjects, test_subjects)\n",
        "    test_mask  = np.isin(subjects, test_subjects)\n",
        "\n",
        "    train_X, train_y = X_all[train_mask], y_all[train_mask]\n",
        "    test_X,  test_y  = X_all[test_mask],  y_all[test_mask]\n",
        "    train_subjects   = subjects[train_mask]\n",
        "\n",
        "    print(f\"Selected test subjects: {test_subjects}\")\n",
        "    print(\"Train:\", train_X.shape, train_y.shape)\n",
        "    print(\"Test :\", test_X.shape, test_y.shape)\n",
        "\n",
        "    return train_X, train_y, test_X, test_y, train_subjects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QrZoNqdzaVZ"
      },
      "source": [
        "**Load Train and Test data and Train the General Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MJVT00gKzUg6",
        "outputId": "3b7aa7cb-7111-4ec3-a7e5-b6ebf202cee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected test subjects: [14]\n",
            "Train: (9976, 128, 9) (9976,)\n",
            "Test : (323, 128, 9) (323,)\n",
            "Classes found: [1 2 3 4 5 6]\n",
            "Epoch 1/30 Train Loss: 1.4960, Train Acc: 0.4672 | Val Loss: 1.2271, Val Acc: 0.5666\n",
            "Epoch 2/30 Train Loss: 1.2119, Train Acc: 0.5593 | Val Loss: 1.0611, Val Acc: 0.5666\n",
            "Epoch 3/30 Train Loss: 1.1137, Train Acc: 0.5998 | Val Loss: 1.0124, Val Acc: 0.6316\n",
            "Epoch 4/30 Train Loss: 1.0854, Train Acc: 0.6237 | Val Loss: 0.9586, Val Acc: 0.6935\n",
            "Epoch 5/30 Train Loss: 1.0483, Train Acc: 0.6584 | Val Loss: 0.9539, Val Acc: 0.6873\n",
            "Epoch 6/30 Train Loss: 1.0347, Train Acc: 0.6743 | Val Loss: 0.9190, Val Acc: 0.7337\n",
            "Epoch 7/30 Train Loss: 1.0088, Train Acc: 0.6888 | Val Loss: 0.8874, Val Acc: 0.7895\n",
            "Epoch 8/30 Train Loss: 0.9924, Train Acc: 0.7052 | Val Loss: 0.8833, Val Acc: 0.7616\n",
            "Epoch 9/30 Train Loss: 0.9718, Train Acc: 0.7160 | Val Loss: 0.8220, Val Acc: 0.7616\n",
            "Epoch 10/30 Train Loss: 0.9525, Train Acc: 0.7312 | Val Loss: 0.7716, Val Acc: 0.8545\n",
            "Epoch 11/30 Train Loss: 0.9476, Train Acc: 0.7379 | Val Loss: 0.7986, Val Acc: 0.7152\n",
            "Epoch 12/30 Train Loss: 0.9382, Train Acc: 0.7452 | Val Loss: 0.8045, Val Acc: 0.7430\n",
            "Epoch 13/30 Train Loss: 0.9148, Train Acc: 0.7517 | Val Loss: 0.8334, Val Acc: 0.6842\n",
            "Epoch 14/30 Train Loss: 0.9079, Train Acc: 0.7622 | Val Loss: 0.8126, Val Acc: 0.7461\n",
            "Epoch 15/30 Train Loss: 0.9015, Train Acc: 0.7593 | Val Loss: 0.7946, Val Acc: 0.7430\n",
            "⏹ Early stopping triggered!\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ---------------------------\n",
        "# Load dataset\n",
        "# ---------------------------\n",
        "# assumes you have a function load_data() already\n",
        "X_train, y_train, X_test, y_test, train_subjects = load_data(dataset_path, test_subjects=test_subjects)\n",
        "\n",
        "# ---------------------------\n",
        "# Encode labels\n",
        "# ---------------------------\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded  = le.transform(y_test)\n",
        "\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Classes found:\", le.classes_)\n",
        "\n",
        "# reshape into subsequences for CNN-GRU\n",
        "X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, X_train.shape[2]))\n",
        "X_test  = X_test.reshape((X_test.shape[0],  n_steps, n_length, X_test.shape[2]))\n",
        "\n",
        "# convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Define CNN-GRU with residuals + norm\n",
        "# ---------------------------\n",
        "class CNNGRU(nn.Module):\n",
        "    def __init__(self, n_steps, n_length, input_dim, n_hidden, n_classes):\n",
        "        super(CNNGRU, self).__init__()\n",
        "\n",
        "        # CNN layers (smaller channels + batchnorm)\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=3)\n",
        "        self.bn1   = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3)\n",
        "        self.bn2   = nn.BatchNorm1d(16)\n",
        "        self.dropout1 = nn.Dropout(0.6)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # compute flattened size dynamically\n",
        "        dummy = torch.zeros(1, input_dim, n_length)\n",
        "        dummy_out = self.pool(self.conv2(self.conv1(dummy)))\n",
        "        flat_size = dummy_out.numel()\n",
        "\n",
        "        # GRU (smaller hidden size)\n",
        "        self.gru = nn.GRU(flat_size, n_hidden//2, batch_first=True)\n",
        "        self.layer_norm = nn.LayerNorm(n_hidden//2)\n",
        "\n",
        "        # residual projection\n",
        "        self.residual_proj = nn.Linear(flat_size, n_hidden//2)\n",
        "\n",
        "        # FC layers\n",
        "        self.dropout2 = nn.Dropout(0.6)\n",
        "        self.fc1 = nn.Linear(n_hidden//2, n_hidden//2)\n",
        "        self.fc2 = nn.Linear(n_hidden//2, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, n_steps, n_length, input_dim)\n",
        "        x = x.permute(0, 1, 3, 2)   # (B, steps, in_ch, len)\n",
        "        batch, n_steps, in_ch, seq_len = x.size()\n",
        "        x = x.reshape(batch * n_steps, in_ch, seq_len)\n",
        "\n",
        "        # CNN + BN + pool\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.reshape(batch, n_steps, -1)  # (B, steps, flat_size)\n",
        "\n",
        "        # GRU + residual\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]                   # last hidden\n",
        "        res = self.residual_proj(x[:, -1, :]) # project residual\n",
        "        out = self.layer_norm(out + res)\n",
        "\n",
        "        # FC\n",
        "        out = self.dropout2(out)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        logits = self.fc2(out)\n",
        "        return logits\n",
        "\n",
        "    def get_fc1_output(self, x):\n",
        "        \"\"\"Return the fc1 representation before the final classification layer.\"\"\"\n",
        "        x = x.permute(0, 1, 3, 2)   # (B, steps, in_ch, len)\n",
        "        batch, n_steps, in_ch, seq_len = x.size()\n",
        "        x = x.reshape(batch * n_steps, in_ch, seq_len)\n",
        "\n",
        "        # CNN + BN + pool\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.reshape(batch, n_steps, -1)  # (B, steps, flat_size)\n",
        "\n",
        "        # GRU + residual\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]                   # last hidden\n",
        "        res = self.residual_proj(x[:, -1, :]) # project residual\n",
        "        out = self.layer_norm(out + res)\n",
        "\n",
        "        # FC1 only\n",
        "        out = self.dropout2(out)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Train general model\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "general_model = CNNGRU(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "\n",
        "# Label smoothing loss\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "optimizer = torch.optim.Adam(general_model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "early_stop_patience = 10\n",
        "\n",
        "for epoch in range(epochs_general):\n",
        "    # --- Train ---\n",
        "    general_model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = general_model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(general_model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation ---\n",
        "    general_model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = general_model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    val_loss = val_loss / len(test_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs_general} \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_weights = general_model.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stop_patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "\n",
        "# restore best weights\n",
        "general_model.load_state_dict(best_weights)\n",
        "\n",
        "# Save weights of general model\n",
        "general_weights = general_model.state_dict()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbt_M90azlkK"
      },
      "source": [
        "**Fine-tune the general model on train subjects' datasets to achieve personalized models and evaluate on the test subject's data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5WKA0JVYgDH_",
        "outputId": "2bfc1b76-71cc-4bfa-fc8a-f3c2a62ee10e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔹 Fine-tuning on Train Subject 1\n",
            "Epoch 1: Train Subj 1 Acc = 99.71%, Test Subj [14] Acc = 75.23%\n",
            "Epoch 2: Train Subj 1 Acc = 99.71%, Test Subj [14] Acc = 75.85%\n",
            "Epoch 3: Train Subj 1 Acc = 99.71%, Test Subj [14] Acc = 77.09%\n",
            "Epoch 4: Train Subj 1 Acc = 99.71%, Test Subj [14] Acc = 77.09%\n",
            "Epoch 5: Train Subj 1 Acc = 99.71%, Test Subj [14] Acc = 77.40%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 2\n",
            "Epoch 1: Train Subj 2 Acc = 100.00%, Test Subj [14] Acc = 84.83%\n",
            "Epoch 2: Train Subj 2 Acc = 100.00%, Test Subj [14] Acc = 83.90%\n",
            "Epoch 3: Train Subj 2 Acc = 100.00%, Test Subj [14] Acc = 84.21%\n",
            "Epoch 4: Train Subj 2 Acc = 100.00%, Test Subj [14] Acc = 80.80%\n",
            "Epoch 5: Train Subj 2 Acc = 100.00%, Test Subj [14] Acc = 78.95%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 3\n",
            "Epoch 1: Train Subj 3 Acc = 100.00%, Test Subj [14] Acc = 77.09%\n",
            "Epoch 2: Train Subj 3 Acc = 100.00%, Test Subj [14] Acc = 77.09%\n",
            "Epoch 3: Train Subj 3 Acc = 100.00%, Test Subj [14] Acc = 77.09%\n",
            "Epoch 4: Train Subj 3 Acc = 100.00%, Test Subj [14] Acc = 77.09%\n",
            "Epoch 5: Train Subj 3 Acc = 100.00%, Test Subj [14] Acc = 76.78%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 4\n",
            "Epoch 1: Train Subj 4 Acc = 93.69%, Test Subj [14] Acc = 67.49%\n",
            "Epoch 2: Train Subj 4 Acc = 92.43%, Test Subj [14] Acc = 67.49%\n",
            "Epoch 3: Train Subj 4 Acc = 92.43%, Test Subj [14] Acc = 67.80%\n",
            "Epoch 4: Train Subj 4 Acc = 92.43%, Test Subj [14] Acc = 67.80%\n",
            "Epoch 5: Train Subj 4 Acc = 93.06%, Test Subj [14] Acc = 67.80%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 5\n",
            "Epoch 1: Train Subj 5 Acc = 96.36%, Test Subj [14] Acc = 64.40%\n",
            "Epoch 2: Train Subj 5 Acc = 96.03%, Test Subj [14] Acc = 62.54%\n",
            "Epoch 3: Train Subj 5 Acc = 95.36%, Test Subj [14] Acc = 63.16%\n",
            "Epoch 4: Train Subj 5 Acc = 95.36%, Test Subj [14] Acc = 62.85%\n",
            "Epoch 5: Train Subj 5 Acc = 95.36%, Test Subj [14] Acc = 62.85%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 6\n",
            "Epoch 1: Train Subj 6 Acc = 96.31%, Test Subj [14] Acc = 72.14%\n",
            "Epoch 2: Train Subj 6 Acc = 95.69%, Test Subj [14] Acc = 69.97%\n",
            "Epoch 3: Train Subj 6 Acc = 96.00%, Test Subj [14] Acc = 69.04%\n",
            "Epoch 4: Train Subj 6 Acc = 96.31%, Test Subj [14] Acc = 72.14%\n",
            "Epoch 5: Train Subj 6 Acc = 96.31%, Test Subj [14] Acc = 73.68%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 7\n",
            "Epoch 1: Train Subj 7 Acc = 99.03%, Test Subj [14] Acc = 75.23%\n",
            "Epoch 2: Train Subj 7 Acc = 99.35%, Test Subj [14] Acc = 75.54%\n",
            "Epoch 3: Train Subj 7 Acc = 99.35%, Test Subj [14] Acc = 74.92%\n",
            "Epoch 4: Train Subj 7 Acc = 99.35%, Test Subj [14] Acc = 75.23%\n",
            "Epoch 5: Train Subj 7 Acc = 99.35%, Test Subj [14] Acc = 75.54%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 8\n",
            "Epoch 1: Train Subj 8 Acc = 94.66%, Test Subj [14] Acc = 67.80%\n",
            "Epoch 2: Train Subj 8 Acc = 97.86%, Test Subj [14] Acc = 68.42%\n",
            "Epoch 3: Train Subj 8 Acc = 100.00%, Test Subj [14] Acc = 68.42%\n",
            "Epoch 4: Train Subj 8 Acc = 100.00%, Test Subj [14] Acc = 68.42%\n",
            "Epoch 5: Train Subj 8 Acc = 100.00%, Test Subj [14] Acc = 68.42%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 9\n",
            "Epoch 1: Train Subj 9 Acc = 88.54%, Test Subj [14] Acc = 67.80%\n",
            "Epoch 2: Train Subj 9 Acc = 88.54%, Test Subj [14] Acc = 66.87%\n",
            "Epoch 3: Train Subj 9 Acc = 88.54%, Test Subj [14] Acc = 66.56%\n",
            "Epoch 4: Train Subj 9 Acc = 87.85%, Test Subj [14] Acc = 67.18%\n",
            "Epoch 5: Train Subj 9 Acc = 87.85%, Test Subj [14] Acc = 66.56%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 10\n",
            "Epoch 1: Train Subj 10 Acc = 84.69%, Test Subj [14] Acc = 66.87%\n",
            "Epoch 2: Train Subj 10 Acc = 84.35%, Test Subj [14] Acc = 63.78%\n",
            "Epoch 3: Train Subj 10 Acc = 84.69%, Test Subj [14] Acc = 66.56%\n",
            "Epoch 4: Train Subj 10 Acc = 84.69%, Test Subj [14] Acc = 65.63%\n",
            "Epoch 5: Train Subj 10 Acc = 84.69%, Test Subj [14] Acc = 65.94%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 11\n",
            "Epoch 1: Train Subj 11 Acc = 98.73%, Test Subj [14] Acc = 68.11%\n",
            "Epoch 2: Train Subj 11 Acc = 98.73%, Test Subj [14] Acc = 69.97%\n",
            "Epoch 3: Train Subj 11 Acc = 98.73%, Test Subj [14] Acc = 69.97%\n",
            "Epoch 4: Train Subj 11 Acc = 100.00%, Test Subj [14] Acc = 71.21%\n",
            "Epoch 5: Train Subj 11 Acc = 98.73%, Test Subj [14] Acc = 70.28%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 12\n",
            "Epoch 1: Train Subj 12 Acc = 99.69%, Test Subj [14] Acc = 74.92%\n",
            "Epoch 2: Train Subj 12 Acc = 99.69%, Test Subj [14] Acc = 73.37%\n",
            "Epoch 3: Train Subj 12 Acc = 100.00%, Test Subj [14] Acc = 73.68%\n",
            "Epoch 4: Train Subj 12 Acc = 99.69%, Test Subj [14] Acc = 74.30%\n",
            "Epoch 5: Train Subj 12 Acc = 99.69%, Test Subj [14] Acc = 74.61%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 13\n",
            "Epoch 1: Train Subj 13 Acc = 99.39%, Test Subj [14] Acc = 77.09%\n",
            "Epoch 2: Train Subj 13 Acc = 99.69%, Test Subj [14] Acc = 78.33%\n",
            "Epoch 3: Train Subj 13 Acc = 99.39%, Test Subj [14] Acc = 77.40%\n",
            "Epoch 4: Train Subj 13 Acc = 99.69%, Test Subj [14] Acc = 78.02%\n",
            "Epoch 5: Train Subj 13 Acc = 99.39%, Test Subj [14] Acc = 78.33%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 15\n",
            "Epoch 1: Train Subj 15 Acc = 99.09%, Test Subj [14] Acc = 65.63%\n",
            "Epoch 2: Train Subj 15 Acc = 99.09%, Test Subj [14] Acc = 65.33%\n",
            "Epoch 3: Train Subj 15 Acc = 99.09%, Test Subj [14] Acc = 66.56%\n",
            "Epoch 4: Train Subj 15 Acc = 99.09%, Test Subj [14] Acc = 64.71%\n",
            "Epoch 5: Train Subj 15 Acc = 99.39%, Test Subj [14] Acc = 65.02%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 16\n",
            "Epoch 1: Train Subj 16 Acc = 72.40%, Test Subj [14] Acc = 72.45%\n",
            "Epoch 2: Train Subj 16 Acc = 72.95%, Test Subj [14] Acc = 72.76%\n",
            "Epoch 3: Train Subj 16 Acc = 73.77%, Test Subj [14] Acc = 72.76%\n",
            "Epoch 4: Train Subj 16 Acc = 73.77%, Test Subj [14] Acc = 71.52%\n",
            "Epoch 5: Train Subj 16 Acc = 74.32%, Test Subj [14] Acc = 72.14%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 17\n",
            "Epoch 1: Train Subj 17 Acc = 95.65%, Test Subj [14] Acc = 76.47%\n",
            "Epoch 2: Train Subj 17 Acc = 95.65%, Test Subj [14] Acc = 76.78%\n",
            "Epoch 3: Train Subj 17 Acc = 95.65%, Test Subj [14] Acc = 76.16%\n",
            "Epoch 4: Train Subj 17 Acc = 95.65%, Test Subj [14] Acc = 76.16%\n",
            "Epoch 5: Train Subj 17 Acc = 95.65%, Test Subj [14] Acc = 75.23%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 18\n",
            "Epoch 1: Train Subj 18 Acc = 99.45%, Test Subj [14] Acc = 79.57%\n",
            "Epoch 2: Train Subj 18 Acc = 100.00%, Test Subj [14] Acc = 82.35%\n",
            "Epoch 3: Train Subj 18 Acc = 100.00%, Test Subj [14] Acc = 82.66%\n",
            "Epoch 4: Train Subj 18 Acc = 100.00%, Test Subj [14] Acc = 85.45%\n",
            "Epoch 5: Train Subj 18 Acc = 100.00%, Test Subj [14] Acc = 85.76%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 19\n",
            "Epoch 1: Train Subj 19 Acc = 100.00%, Test Subj [14] Acc = 76.78%\n",
            "Epoch 2: Train Subj 19 Acc = 100.00%, Test Subj [14] Acc = 75.54%\n",
            "Epoch 3: Train Subj 19 Acc = 99.72%, Test Subj [14] Acc = 77.40%\n",
            "Epoch 4: Train Subj 19 Acc = 100.00%, Test Subj [14] Acc = 77.09%\n",
            "Epoch 5: Train Subj 19 Acc = 100.00%, Test Subj [14] Acc = 75.85%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 20\n",
            "Epoch 1: Train Subj 20 Acc = 100.00%, Test Subj [14] Acc = 77.71%\n",
            "Epoch 2: Train Subj 20 Acc = 100.00%, Test Subj [14] Acc = 78.64%\n",
            "Epoch 3: Train Subj 20 Acc = 100.00%, Test Subj [14] Acc = 77.71%\n",
            "Epoch 4: Train Subj 20 Acc = 100.00%, Test Subj [14] Acc = 83.28%\n",
            "Epoch 5: Train Subj 20 Acc = 100.00%, Test Subj [14] Acc = 86.69%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 21\n",
            "Epoch 1: Train Subj 21 Acc = 99.75%, Test Subj [14] Acc = 82.66%\n",
            "Epoch 2: Train Subj 21 Acc = 100.00%, Test Subj [14] Acc = 87.00%\n",
            "Epoch 3: Train Subj 21 Acc = 100.00%, Test Subj [14] Acc = 89.16%\n",
            "Epoch 4: Train Subj 21 Acc = 100.00%, Test Subj [14] Acc = 87.62%\n",
            "Epoch 5: Train Subj 21 Acc = 100.00%, Test Subj [14] Acc = 87.31%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 22\n",
            "Epoch 1: Train Subj 22 Acc = 99.69%, Test Subj [14] Acc = 76.47%\n",
            "Epoch 2: Train Subj 22 Acc = 100.00%, Test Subj [14] Acc = 73.37%\n",
            "Epoch 3: Train Subj 22 Acc = 100.00%, Test Subj [14] Acc = 75.23%\n",
            "Epoch 4: Train Subj 22 Acc = 99.69%, Test Subj [14] Acc = 84.21%\n",
            "Epoch 5: Train Subj 22 Acc = 100.00%, Test Subj [14] Acc = 76.16%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 23\n",
            "Epoch 1: Train Subj 23 Acc = 97.31%, Test Subj [14] Acc = 67.49%\n",
            "Epoch 2: Train Subj 23 Acc = 98.12%, Test Subj [14] Acc = 67.80%\n",
            "Epoch 3: Train Subj 23 Acc = 98.66%, Test Subj [14] Acc = 67.80%\n",
            "Epoch 4: Train Subj 23 Acc = 98.39%, Test Subj [14] Acc = 68.11%\n",
            "Epoch 5: Train Subj 23 Acc = 98.92%, Test Subj [14] Acc = 68.11%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 24\n",
            "Epoch 1: Train Subj 24 Acc = 99.74%, Test Subj [14] Acc = 67.18%\n",
            "Epoch 2: Train Subj 24 Acc = 99.74%, Test Subj [14] Acc = 65.02%\n",
            "Epoch 3: Train Subj 24 Acc = 99.74%, Test Subj [14] Acc = 65.02%\n",
            "Epoch 4: Train Subj 24 Acc = 99.74%, Test Subj [14] Acc = 65.02%\n",
            "Epoch 5: Train Subj 24 Acc = 99.74%, Test Subj [14] Acc = 65.33%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 25\n",
            "Epoch 1: Train Subj 25 Acc = 99.76%, Test Subj [14] Acc = 75.54%\n",
            "Epoch 2: Train Subj 25 Acc = 99.27%, Test Subj [14] Acc = 74.30%\n",
            "Epoch 3: Train Subj 25 Acc = 99.27%, Test Subj [14] Acc = 69.35%\n",
            "Epoch 4: Train Subj 25 Acc = 99.27%, Test Subj [14] Acc = 70.59%\n",
            "Epoch 5: Train Subj 25 Acc = 100.00%, Test Subj [14] Acc = 73.99%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 26\n",
            "Epoch 1: Train Subj 26 Acc = 94.39%, Test Subj [14] Acc = 68.73%\n",
            "Epoch 2: Train Subj 26 Acc = 94.39%, Test Subj [14] Acc = 66.25%\n",
            "Epoch 3: Train Subj 26 Acc = 94.39%, Test Subj [14] Acc = 71.21%\n",
            "Epoch 4: Train Subj 26 Acc = 94.39%, Test Subj [14] Acc = 69.04%\n",
            "Epoch 5: Train Subj 26 Acc = 94.90%, Test Subj [14] Acc = 70.90%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 27\n",
            "Epoch 1: Train Subj 27 Acc = 100.00%, Test Subj [14] Acc = 75.54%\n",
            "Epoch 2: Train Subj 27 Acc = 100.00%, Test Subj [14] Acc = 75.85%\n",
            "Epoch 3: Train Subj 27 Acc = 100.00%, Test Subj [14] Acc = 76.16%\n",
            "Epoch 4: Train Subj 27 Acc = 100.00%, Test Subj [14] Acc = 76.16%\n",
            "Epoch 5: Train Subj 27 Acc = 100.00%, Test Subj [14] Acc = 75.54%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 28\n",
            "Epoch 1: Train Subj 28 Acc = 90.05%, Test Subj [14] Acc = 82.66%\n",
            "Epoch 2: Train Subj 28 Acc = 90.05%, Test Subj [14] Acc = 85.14%\n",
            "Epoch 3: Train Subj 28 Acc = 90.05%, Test Subj [14] Acc = 83.59%\n",
            "Epoch 4: Train Subj 28 Acc = 90.31%, Test Subj [14] Acc = 82.97%\n",
            "Epoch 5: Train Subj 28 Acc = 90.58%, Test Subj [14] Acc = 82.66%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 29\n",
            "Epoch 1: Train Subj 29 Acc = 95.06%, Test Subj [14] Acc = 65.94%\n",
            "Epoch 2: Train Subj 29 Acc = 97.97%, Test Subj [14] Acc = 64.71%\n",
            "Epoch 3: Train Subj 29 Acc = 100.00%, Test Subj [14] Acc = 64.71%\n",
            "Epoch 4: Train Subj 29 Acc = 100.00%, Test Subj [14] Acc = 64.71%\n",
            "Epoch 5: Train Subj 29 Acc = 100.00%, Test Subj [14] Acc = 65.63%\n",
            "\n",
            "🔹 Fine-tuning on Train Subject 30\n",
            "Epoch 1: Train Subj 30 Acc = 91.12%, Test Subj [14] Acc = 77.71%\n",
            "Epoch 2: Train Subj 30 Acc = 91.12%, Test Subj [14] Acc = 77.71%\n",
            "Epoch 3: Train Subj 30 Acc = 90.86%, Test Subj [14] Acc = 77.71%\n",
            "Epoch 4: Train Subj 30 Acc = 90.60%, Test Subj [14] Acc = 79.26%\n",
            "Epoch 5: Train Subj 30 Acc = 90.86%, Test Subj [14] Acc = 79.26%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import copy\n",
        "\n",
        "results = {}\n",
        "personalized_models = {}\n",
        "personalized_models[\"general\"] = copy.deepcopy(general_model.state_dict())\n",
        "unique_subjects = np.unique(train_subjects)\n",
        "\n",
        "for subj in unique_subjects:\n",
        "    print(f\"\\n🔹 Fine-tuning on Train Subject {subj}\")\n",
        "\n",
        "    # ---------------- Subject-specific data ----------------\n",
        "    subj_mask = train_subjects == subj\n",
        "    subj_X = X_train[subj_mask]\n",
        "    subj_y = y_train[subj_mask]\n",
        "\n",
        "    # keep global encoding (so test set matches)\n",
        "    subj_y_encoded = le.transform(subj_y)\n",
        "\n",
        "    subj_X_tensor = torch.tensor(subj_X, dtype=torch.float32)\n",
        "    subj_y_tensor = torch.tensor(subj_y_encoded, dtype=torch.long)\n",
        "    subj_dataset = TensorDataset(subj_X_tensor, subj_y_tensor)\n",
        "    subj_loader = DataLoader(subj_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # ---------------- Clone general model ----------------\n",
        "    personal_model = CNNGRU(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "    personal_model.load_state_dict(copy.deepcopy(general_weights))\n",
        "\n",
        "    # Freeze all parameters\n",
        "    for param in personal_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Unfreeze last FC layers for personalization\n",
        "    for param in personal_model.fc1.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in personal_model.fc2.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Loss + optimizer\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)  # smoothing helps small datasets\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, personal_model.parameters()), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    train_accs, test_accs = [], []\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_weights = None\n",
        "\n",
        "    for epoch in range(epochs_finetune):\n",
        "        # ---------------- Train ----------------\n",
        "        personal_model.train()\n",
        "        total_loss = 0\n",
        "        for X_batch, y_batch in subj_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = personal_model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(personal_model.parameters(), max_norm=5.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(subj_loader)\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        # ---------------- Evaluate ----------------\n",
        "        personal_model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Train subject accuracy\n",
        "            train_outputs = personal_model(subj_X_tensor.to(device))\n",
        "            train_preds = torch.argmax(train_outputs, dim=1).cpu().numpy()\n",
        "            train_acc = accuracy_score(subj_y_encoded, train_preds) * 100\n",
        "\n",
        "            # Test subject accuracy (uses global encoding)\n",
        "            test_outputs = personal_model(X_test_tensor.to(device))\n",
        "            test_preds = torch.argmax(test_outputs, dim=1).cpu().numpy()\n",
        "            test_acc = accuracy_score(y_test_encoded, test_preds) * 100\n",
        "\n",
        "        train_accs.append(train_acc)\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Subj {subj} Acc = {train_acc:.2f}%, \"\n",
        "              f\"Test Subj {test_subjects} Acc = {test_acc:.2f}%\")\n",
        "\n",
        "        # save best subject model by training loss\n",
        "        if avg_loss < best_val_loss:\n",
        "            best_val_loss = avg_loss\n",
        "            best_weights = copy.deepcopy(personal_model.state_dict())\n",
        "\n",
        "    # ---------------- Save results ----------------\n",
        "    results[subj] = {\"train_accs\": train_accs, \"test_accs\": test_accs}\n",
        "    personalized_models[subj] = best_weights if best_weights else copy.deepcopy(personal_model.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeZiC_aCWL2K"
      },
      "source": [
        "**Inference of the general model and personalized ones on test samples and calculate the entropy of predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z1gjxfyS4os",
        "outputId": "fb94a828-6c21-4700-f231-106ca22af007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Test Sample 8\n",
            "   Ground Truth: 5 (class 4)\n",
            "   General Model → Pred: 5 (class 4), Entropy=0.8091\n",
            "   Personal Model 1 → Pred: 5 (class 4), Entropy=0.7777\n",
            "   Personal Model 2 → Pred: 4 (class 3), Entropy=0.9662\n",
            "   Personal Model 3 → Pred: 5 (class 4), Entropy=0.7783\n",
            "   Personal Model 4 → Pred: 5 (class 4), Entropy=0.7812\n",
            "   Personal Model 5 → Pred: 5 (class 4), Entropy=0.7546\n",
            "   Personal Model 6 → Pred: 5 (class 4), Entropy=0.7561\n",
            "   Personal Model 7 → Pred: 5 (class 4), Entropy=0.7280\n",
            "   Personal Model 8 → Pred: 5 (class 4), Entropy=0.6768\n",
            "   Personal Model 9 → Pred: 5 (class 4), Entropy=0.8001\n",
            "   Personal Model 10 → Pred: 5 (class 4), Entropy=0.8277\n",
            "   Personal Model 11 → Pred: 5 (class 4), Entropy=0.7480\n",
            "   Personal Model 12 → Pred: 5 (class 4), Entropy=0.7672\n",
            "   Personal Model 13 → Pred: 5 (class 4), Entropy=0.7885\n",
            "   Personal Model 15 → Pred: 5 (class 4), Entropy=0.7329\n",
            "   Personal Model 16 → Pred: 5 (class 4), Entropy=0.8212\n",
            "   Personal Model 17 → Pred: 5 (class 4), Entropy=0.7825\n",
            "   Personal Model 18 → Pred: 5 (class 4), Entropy=0.7619\n",
            "   Personal Model 19 → Pred: 5 (class 4), Entropy=0.8063\n",
            "   Personal Model 20 → Pred: 5 (class 4), Entropy=0.8311\n",
            "   Personal Model 21 → Pred: 5 (class 4), Entropy=0.8940\n",
            "   Personal Model 22 → Pred: 5 (class 4), Entropy=0.7739\n",
            "   Personal Model 23 → Pred: 5 (class 4), Entropy=0.6884\n",
            "   Personal Model 24 → Pred: 5 (class 4), Entropy=0.6303\n",
            "   Personal Model 25 → Pred: 5 (class 4), Entropy=0.6891\n",
            "   Personal Model 26 → Pred: 5 (class 4), Entropy=0.7976\n",
            "   Personal Model 27 → Pred: 5 (class 4), Entropy=0.7335\n",
            "   Personal Model 28 → Pred: 4 (class 3), Entropy=0.9448\n",
            "   Personal Model 29 → Pred: 5 (class 4), Entropy=0.6564\n",
            "   Personal Model 30 → Pred: 5 (class 4), Entropy=0.8437\n",
            "\n",
            "✨ Entropy-Weighted Ensemble → Pred: 5 (class 4), Entropy=0.8039\n",
            "   Fused Probs (Entropy-Weighted): [0.0135 0.019  0.0054 0.2104 0.7296 0.0221]\n",
            "✨ Simple Average Ensemble → Pred: 5 (class 4), Entropy=0.8159\n",
            "   Fused Probs (Simple Average): [0.0139 0.0188 0.0056 0.2193 0.7205 0.0219]\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import entropy\n",
        "import numpy as np\n",
        "\n",
        "def entropy_weighted_ensemble(probs_list):\n",
        "    \"\"\"Fuse predictions using entropy-based weights.\"\"\"\n",
        "    entropies = np.array([entropy(p) for p in probs_list])\n",
        "    weights = 1.0 / (entropies + 1e-8)  # inverse entropy\n",
        "    weights /= weights.sum()            # normalize\n",
        "    fused_probs = np.sum([w * p for w, p in zip(weights, probs_list)], axis=0)\n",
        "    final_pred = fused_probs.argmax()\n",
        "    return fused_probs, final_pred\n",
        "\n",
        "def simple_average_ensemble(probs_list):\n",
        "    \"\"\"Fuse predictions using simple average.\"\"\"\n",
        "    fused_probs = np.mean(probs_list, axis=0)\n",
        "    final_pred = fused_probs.argmax()\n",
        "    return fused_probs, final_pred\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Inspect one specific test sample\n",
        "# ---------------------------\n",
        "sample_idx = 8   # <-- change index if you want\n",
        "x_single = X_test_tensor[sample_idx].unsqueeze(0).to(device)\n",
        "y_true   = y_test_encoded[sample_idx]\n",
        "\n",
        "print(f\"\\n🔍 Test Sample {sample_idx}\")\n",
        "print(f\"   Ground Truth: {le.inverse_transform([y_true])[0]} (class {y_true})\")\n",
        "\n",
        "# ---- General Model ----\n",
        "general_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = general_model(x_single)\n",
        "    probs_general = torch.softmax(logits, dim=1).cpu().numpy().flatten()\n",
        "    pred_general = probs_general.argmax()\n",
        "    ent_general = entropy(probs_general)\n",
        "\n",
        "print(f\"   General Model → Pred: {le.inverse_transform([pred_general])[0]} \"\n",
        "      f\"(class {pred_general}), Entropy={ent_general:.4f}\")\n",
        "\n",
        "# ---- Personalized Models ----\n",
        "all_probs = []  # store probability vectors for ensemble\n",
        "\n",
        "for subj in [s for s in personalized_models.keys() if s != \"general\"]:\n",
        "    model_eval = CNNGRU(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "    model_eval.load_state_dict(personalized_models[subj])  # load saved weights\n",
        "\n",
        "    model_eval.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model_eval(x_single)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy().flatten()\n",
        "        pred = probs.argmax()\n",
        "        ent = entropy(probs)\n",
        "\n",
        "    all_probs.append(probs)\n",
        "\n",
        "    print(f\"   Personal Model {subj} → Pred: {le.inverse_transform([pred])[0]} \"\n",
        "          f\"(class {pred}), Entropy={ent:.4f}\")\n",
        "\n",
        "# ---- Entropy-Weighted Ensemble ----\n",
        "fused_probs_ent, fused_pred_ent = entropy_weighted_ensemble(all_probs)\n",
        "print(f\"\\n✨ Entropy-Weighted Ensemble → Pred: {le.inverse_transform([fused_pred_ent])[0]} \"\n",
        "      f\"(class {fused_pred_ent}), Entropy={entropy(fused_probs_ent):.4f}\")\n",
        "print(\"   Fused Probs (Entropy-Weighted):\", np.round(fused_probs_ent, 4))\n",
        "\n",
        "# ---- Simple Average Ensemble ----\n",
        "fused_probs_avg, fused_pred_avg = simple_average_ensemble(all_probs)\n",
        "print(f\"✨ Simple Average Ensemble → Pred: {le.inverse_transform([fused_pred_avg])[0]} \"\n",
        "      f\"(class {fused_pred_avg}), Entropy={entropy(fused_probs_avg):.4f}\")\n",
        "print(\"   Fused Probs (Simple Average):\", np.round(fused_probs_avg, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yephSD2AKov"
      },
      "source": [
        "## **Test Time Adaptation (unlabelled test data + source models)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lu1arXQives",
        "outputId": "ae9fdb2c-49c0-40b8-df00-f53f13c025a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ General Model Accuracy on Test Set: 0.7430\n",
            "✅ Simple Average Ensemble Accuracy on Test Set: 0.7523\n",
            "✅ Hybrid Strategy Accuracy (general unless uncertain → best personal): 0.7616\n",
            "✅ Hybrid-AvgK5 Accuracy (general unless uncertain → avg of K=5 lowest-entropy): 0.7183\n",
            "✅ Hybrid-VoteK5 Accuracy (general unless uncertain → vote of K=5 lowest-entropy): 0.7059\n",
            "✅ Hybrid-KNN Accuracy (first 100 hybrid, then K=5 similar past samples in fc1 space): 0.7430\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import entropy\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def simple_average_ensemble(probs_list):\n",
        "    \"\"\"Fuse predictions using simple average.\"\"\"\n",
        "    fused_probs = np.mean(probs_list, axis=0)\n",
        "    final_pred = fused_probs.argmax()\n",
        "    return fused_probs, final_pred\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluate on all test samples\n",
        "# ---------------------------\n",
        "general_correct = 0\n",
        "ensemble_correct = 0\n",
        "hybrid_correct = 0\n",
        "hybrid_avgk5_correct = 0\n",
        "hybrid_votek5_correct = 0\n",
        "hybrid_knn_correct = 0   # new metric\n",
        "\n",
        "n_samples = len(X_test_tensor)\n",
        "entropy_threshold = 0.2\n",
        "K = 5   # neighbors\n",
        "N = 100 # first N samples use hybrid strategy\n",
        "\n",
        "# store past representations & probs for knn-based hybrid\n",
        "past_reps = []\n",
        "past_probs = []\n",
        "past_labels = []\n",
        "\n",
        "for sample_idx in range(n_samples):\n",
        "    x_single = X_test_tensor[sample_idx].unsqueeze(0).to(device)\n",
        "    y_true   = y_test_encoded[sample_idx]\n",
        "\n",
        "    # ---- General Model ----\n",
        "    general_model.eval()\n",
        "    with torch.no_grad():\n",
        "        # get fc1 representation\n",
        "        rep_tensor = general_model.get_fc1_output(x_single)\n",
        "        rep = rep_tensor.cpu().numpy().flatten()\n",
        "\n",
        "        logits = general_model(x_single)\n",
        "        probs_general = torch.softmax(logits, dim=1).cpu().numpy().flatten()\n",
        "        pred_general = probs_general.argmax()\n",
        "        ent_general = entropy(probs_general)\n",
        "\n",
        "    if pred_general == y_true:\n",
        "        general_correct += 1\n",
        "\n",
        "    # ---- Personalized Models ----\n",
        "    all_probs = []\n",
        "    all_ents = []\n",
        "    all_preds = []\n",
        "\n",
        "    for subj in [s for s in personalized_models.keys() if s != \"general\"]:\n",
        "        model_eval = CNNGRU(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "        model_eval.load_state_dict(personalized_models[subj])  # load saved weights\n",
        "\n",
        "        model_eval.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model_eval(x_single)\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy().flatten()\n",
        "            pred = probs.argmax()\n",
        "            ent = entropy(probs)\n",
        "\n",
        "        all_probs.append(probs)\n",
        "        all_ents.append(ent)\n",
        "        all_preds.append(pred)\n",
        "\n",
        "    # ---- Simple Average Ensemble ----\n",
        "    fused_probs_avg, fused_pred_avg = simple_average_ensemble(all_probs)\n",
        "    if fused_pred_avg == y_true:\n",
        "        ensemble_correct += 1\n",
        "\n",
        "    # ---- Hybrid (General vs Best Personal Model) ----\n",
        "    if ent_general <= entropy_threshold:\n",
        "        hybrid_pred = pred_general\n",
        "    else:\n",
        "        best_idx = int(np.argmin(all_ents))\n",
        "        hybrid_pred = all_preds[best_idx]\n",
        "\n",
        "    if hybrid_pred == y_true:\n",
        "        hybrid_correct += 1\n",
        "\n",
        "    # ---- Hybrid-AvgK5 ----\n",
        "    if ent_general <= entropy_threshold:\n",
        "        hybrid_avgk5_pred = pred_general\n",
        "    else:\n",
        "        # pick K lowest-entropy personalized models\n",
        "        k_idx = np.argsort(all_ents)[:K]\n",
        "        k_probs = [all_probs[i] for i in k_idx]\n",
        "        fused_probs_k, hybrid_avgk5_pred = simple_average_ensemble(k_probs)\n",
        "\n",
        "    if hybrid_avgk5_pred == y_true:\n",
        "        hybrid_avgk5_correct += 1\n",
        "\n",
        "    # ---- Hybrid-VoteK5 ----\n",
        "    if ent_general <= entropy_threshold:\n",
        "        hybrid_votek5_pred = pred_general\n",
        "    else:\n",
        "        k_idx = np.argsort(all_ents)[:K]\n",
        "        k_preds = [all_preds[i] for i in k_idx]\n",
        "        majority_pred = Counter(k_preds).most_common(1)[0][0]\n",
        "        hybrid_votek5_pred = majority_pred\n",
        "\n",
        "    if hybrid_votek5_pred == y_true:\n",
        "        hybrid_votek5_correct += 1\n",
        "\n",
        "    # ---- Hybrid-KNN (new) ----\n",
        "    if ent_general <= entropy_threshold:\n",
        "        hybrid_knn_pred = pred_general\n",
        "    else:\n",
        "        if sample_idx < N or len(past_reps) < K:\n",
        "            # fallback to best personalized if not enough past samples\n",
        "            best_idx = int(np.argmin(all_ents))\n",
        "            hybrid_knn_pred = all_preds[best_idx]\n",
        "        else:\n",
        "            # compute cosine similarity to past representations\n",
        "            sims = cosine_similarity(rep.reshape(1, -1), np.vstack(past_reps))[0]\n",
        "            topk_idx = np.argsort(sims)[-K:]  # most similar K\n",
        "            knn_probs = [past_probs[j] for j in topk_idx]\n",
        "            fused_probs_knn, hybrid_knn_pred = simple_average_ensemble(knn_probs)\n",
        "\n",
        "    if hybrid_knn_pred == y_true:\n",
        "        hybrid_knn_correct += 1\n",
        "\n",
        "    # store current sample’s rep & probs for future neighbors\n",
        "    past_reps.append(rep)\n",
        "    past_probs.append(probs_general)  # store general’s probs\n",
        "    past_labels.append(y_true)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Report results\n",
        "# ---------------------------\n",
        "general_acc = general_correct / n_samples\n",
        "ensemble_acc = ensemble_correct / n_samples\n",
        "hybrid_acc = hybrid_correct / n_samples\n",
        "hybrid_avgk5_acc = hybrid_avgk5_correct / n_samples\n",
        "hybrid_votek5_acc = hybrid_votek5_correct / n_samples\n",
        "hybrid_knn_acc = hybrid_knn_correct / n_samples\n",
        "\n",
        "print(f\"\\n✅ General Model Accuracy on Test Set: {general_acc:.4f}\")\n",
        "print(f\"✅ Simple Average Ensemble Accuracy on Test Set: {ensemble_acc:.4f}\")\n",
        "print(f\"✅ Hybrid Strategy Accuracy (general unless uncertain → best personal): {hybrid_acc:.4f}\")\n",
        "print(f\"✅ Hybrid-AvgK5 Accuracy (general unless uncertain → avg of K=5 lowest-entropy): {hybrid_avgk5_acc:.4f}\")\n",
        "print(f\"✅ Hybrid-VoteK5 Accuracy (general unless uncertain → vote of K=5 lowest-entropy): {hybrid_votek5_acc:.4f}\")\n",
        "print(f\"✅ Hybrid-KNN Accuracy (first {N} hybrid, then K=5 similar past samples in fc1 space): {hybrid_knn_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "e637DBpdCOZ0",
        "outputId": "584f7257-cc8b-470d-e457-36039aae02a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================== Test Subject 1 ==================\n",
            "Selected test subjects: [np.int64(1)]\n",
            "Train: (9952, 128, 9) (9952,)\n",
            "Test : (347, 128, 9) (347,)\n",
            "General model → Acc: 99.42%, Mean Loss: 0.0528\n",
            "Simple average → Acc: 99.42%, Mean Loss: 1.0783\n",
            "Hybrid → Acc: 99.42%, Mean Loss: 0.1614\n",
            "Hybrid-KLE → Acc: 99.42%, Mean Loss: 0.1621\n",
            "Hybrid-KLE (rep-based) → Acc: 99.42%, Mean Loss: 0.1681\n",
            "\n",
            "================== Test Subject 2 ==================\n",
            "Selected test subjects: [np.int64(2)]\n",
            "Train: (9997, 128, 9) (9997,)\n",
            "Test : (302, 128, 9) (302,)\n",
            "General model → Acc: 88.41%, Mean Loss: 0.3839\n",
            "Simple average → Acc: 88.74%, Mean Loss: 1.2488\n",
            "Hybrid → Acc: 86.42%, Mean Loss: 0.8682\n",
            "Hybrid-KLE → Acc: 86.42%, Mean Loss: 0.8741\n",
            "Hybrid-KLE (rep-based) → Acc: 88.41%, Mean Loss: 0.8882\n",
            "\n",
            "================== Test Subject 3 ==================\n",
            "Selected test subjects: [np.int64(3)]\n",
            "Train: (9958, 128, 9) (9958,)\n",
            "Test : (341, 128, 9) (341,)\n",
            "General model → Acc: 98.24%, Mean Loss: 0.1775\n",
            "Simple average → Acc: 98.24%, Mean Loss: 1.1721\n",
            "Hybrid → Acc: 98.53%, Mean Loss: 0.5542\n",
            "Hybrid-KLE → Acc: 98.24%, Mean Loss: 0.5641\n",
            "Hybrid-KLE (rep-based) → Acc: 97.95%, Mean Loss: 0.6109\n",
            "\n",
            "================== Test Subject 4 ==================\n",
            "Selected test subjects: [np.int64(4)]\n",
            "Train: (9982, 128, 9) (9982,)\n",
            "Test : (317, 128, 9) (317,)\n",
            "General model → Acc: 95.90%, Mean Loss: 0.2597\n",
            "Simple average → Acc: 92.11%, Mean Loss: 1.2295\n",
            "Hybrid → Acc: 82.97%, Mean Loss: 0.8398\n",
            "Hybrid-KLE → Acc: 85.49%, Mean Loss: 0.8198\n",
            "Hybrid-KLE (rep-based) → Acc: 93.38%, Mean Loss: 0.8101\n",
            "\n",
            "================== Test Subject 5 ==================\n",
            "Selected test subjects: [np.int64(5)]\n",
            "Train: (9997, 128, 9) (9997,)\n",
            "Test : (302, 128, 9) (302,)\n",
            "General model → Acc: 80.79%, Mean Loss: 0.4091\n",
            "Simple average → Acc: 83.77%, Mean Loss: 1.2491\n",
            "Hybrid → Acc: 81.13%, Mean Loss: 0.8200\n",
            "Hybrid-KLE → Acc: 80.79%, Mean Loss: 0.8148\n",
            "Hybrid-KLE (rep-based) → Acc: 80.13%, Mean Loss: 0.8210\n",
            "\n",
            "================== Test Subject 6 ==================\n",
            "Selected test subjects: [np.int64(6)]\n",
            "Train: (9974, 128, 9) (9974,)\n",
            "Test : (325, 128, 9) (325,)\n",
            "General model → Acc: 93.54%, Mean Loss: 0.1209\n",
            "Simple average → Acc: 95.38%, Mean Loss: 1.1210\n",
            "Hybrid → Acc: 95.69%, Mean Loss: 0.3881\n",
            "Hybrid-KLE → Acc: 95.38%, Mean Loss: 0.3953\n",
            "Hybrid-KLE (rep-based) → Acc: 93.23%, Mean Loss: 0.4027\n",
            "\n",
            "================== Test Subject 7 ==================\n",
            "Selected test subjects: [np.int64(7)]\n",
            "Train: (9991, 128, 9) (9991,)\n",
            "Test : (308, 128, 9) (308,)\n",
            "General model → Acc: 93.83%, Mean Loss: 0.1717\n",
            "Simple average → Acc: 95.45%, Mean Loss: 1.1587\n",
            "Hybrid → Acc: 97.40%, Mean Loss: 0.3965\n",
            "Hybrid-KLE → Acc: 96.43%, Mean Loss: 0.4056\n",
            "Hybrid-KLE (rep-based) → Acc: 94.48%, Mean Loss: 0.4564\n",
            "\n",
            "================== Test Subject 8 ==================\n",
            "Selected test subjects: [np.int64(8)]\n",
            "Train: (10018, 128, 9) (10018,)\n",
            "Test : (281, 128, 9) (281,)\n",
            "General model → Acc: 93.95%, Mean Loss: 0.2006\n",
            "Simple average → Acc: 90.04%, Mean Loss: 1.1988\n",
            "Hybrid → Acc: 89.32%, Mean Loss: 0.7826\n",
            "Hybrid-KLE → Acc: 87.90%, Mean Loss: 0.7886\n",
            "Hybrid-KLE (rep-based) → Acc: 93.59%, Mean Loss: 0.7895\n",
            "\n",
            "================== Test Subject 9 ==================\n",
            "Selected test subjects: [np.int64(9)]\n",
            "Train: (10011, 128, 9) (10011,)\n",
            "Test : (288, 128, 9) (288,)\n",
            "General model → Acc: 85.76%, Mean Loss: 0.5015\n",
            "Simple average → Acc: 84.72%, Mean Loss: 1.2741\n",
            "Hybrid → Acc: 75.69%, Mean Loss: 0.8611\n",
            "Hybrid-KLE → Acc: 76.04%, Mean Loss: 0.8525\n",
            "Hybrid-KLE (rep-based) → Acc: 84.38%, Mean Loss: 0.8389\n",
            "\n",
            "================== Test Subject 10 ==================\n",
            "Selected test subjects: [np.int64(10)]\n",
            "Train: (10005, 128, 9) (10005,)\n",
            "Test : (294, 128, 9) (294,)\n",
            "General model → Acc: 71.09%, Mean Loss: 0.8467\n",
            "Simple average → Acc: 72.11%, Mean Loss: 1.3845\n",
            "Hybrid → Acc: 71.77%, Mean Loss: 1.0341\n",
            "Hybrid-KLE → Acc: 71.77%, Mean Loss: 1.0412\n",
            "Hybrid-KLE (rep-based) → Acc: 71.43%, Mean Loss: 1.0479\n",
            "\n",
            "================== Test Subject 11 ==================\n",
            "Selected test subjects: [np.int64(11)]\n",
            "Train: (9983, 128, 9) (9983,)\n",
            "Test : (316, 128, 9) (316,)\n",
            "General model → Acc: 98.73%, Mean Loss: 0.1081\n",
            "Simple average → Acc: 99.37%, Mean Loss: 1.1331\n",
            "Hybrid → Acc: 97.78%, Mean Loss: 0.4595\n",
            "Hybrid-KLE → Acc: 98.73%, Mean Loss: 0.4594\n",
            "Hybrid-KLE (rep-based) → Acc: 100.00%, Mean Loss: 0.4811\n",
            "\n",
            "================== Test Subject 12 ==================\n",
            "Selected test subjects: [np.int64(12)]\n",
            "Train: (9979, 128, 9) (9979,)\n",
            "Test : (320, 128, 9) (320,)\n",
            "General model → Acc: 92.19%, Mean Loss: 0.2536\n",
            "Simple average → Acc: 92.81%, Mean Loss: 1.2173\n",
            "Hybrid → Acc: 92.81%, Mean Loss: 0.7226\n",
            "Hybrid-KLE → Acc: 92.50%, Mean Loss: 0.7302\n",
            "Hybrid-KLE (rep-based) → Acc: 92.81%, Mean Loss: 0.7755\n",
            "\n",
            "================== Test Subject 13 ==================\n",
            "Selected test subjects: [np.int64(13)]\n",
            "Train: (9972, 128, 9) (9972,)\n",
            "Test : (327, 128, 9) (327,)\n",
            "General model → Acc: 96.94%, Mean Loss: 0.0904\n",
            "Simple average → Acc: 96.33%, Mean Loss: 1.1144\n",
            "Hybrid → Acc: 96.64%, Mean Loss: 0.2369\n",
            "Hybrid-KLE → Acc: 96.64%, Mean Loss: 0.2350\n",
            "Hybrid-KLE (rep-based) → Acc: 96.94%, Mean Loss: 0.2438\n",
            "\n",
            "================== Test Subject 14 ==================\n",
            "Selected test subjects: [np.int64(14)]\n",
            "Train: (9976, 128, 9) (9976,)\n",
            "Test : (323, 128, 9) (323,)\n",
            "General model → Acc: 69.04%, Mean Loss: 1.4677\n",
            "Simple average → Acc: 67.18%, Mean Loss: 1.3940\n",
            "Hybrid → Acc: 65.63%, Mean Loss: 1.8498\n",
            "Hybrid-KLE → Acc: 68.11%, Mean Loss: 1.8399\n",
            "Hybrid-KLE (rep-based) → Acc: 67.80%, Mean Loss: 1.8634\n",
            "\n",
            "================== Test Subject 15 ==================\n",
            "Selected test subjects: [np.int64(15)]\n",
            "Train: (9971, 128, 9) (9971,)\n",
            "Test : (328, 128, 9) (328,)\n",
            "General model → Acc: 98.78%, Mean Loss: 0.1458\n",
            "Simple average → Acc: 99.70%, Mean Loss: 1.1425\n",
            "Hybrid → Acc: 99.39%, Mean Loss: 0.5652\n",
            "Hybrid-KLE → Acc: 98.78%, Mean Loss: 0.5724\n",
            "Hybrid-KLE (rep-based) → Acc: 99.39%, Mean Loss: 0.6215\n",
            "\n",
            "================== Test Subject 16 ==================\n",
            "Selected test subjects: [np.int64(16)]\n",
            "Train: (9933, 128, 9) (9933,)\n",
            "Test : (366, 128, 9) (366,)\n",
            "General model → Acc: 71.04%, Mean Loss: 0.6552\n",
            "Simple average → Acc: 70.77%, Mean Loss: 1.3300\n",
            "Hybrid → Acc: 73.77%, Mean Loss: 1.0441\n",
            "Hybrid-KLE → Acc: 74.59%, Mean Loss: 1.0527\n",
            "Hybrid-KLE (rep-based) → Acc: 70.49%, Mean Loss: 1.0515\n",
            "\n",
            "================== Test Subject 17 ==================\n",
            "Selected test subjects: [np.int64(17)]\n",
            "Train: (9931, 128, 9) (9931,)\n",
            "Test : (368, 128, 9) (368,)\n",
            "General model → Acc: 94.57%, Mean Loss: 0.2329\n",
            "Simple average → Acc: 94.84%, Mean Loss: 1.1951\n",
            "Hybrid → Acc: 95.38%, Mean Loss: 0.8382\n",
            "Hybrid-KLE → Acc: 94.84%, Mean Loss: 0.8467\n",
            "Hybrid-KLE (rep-based) → Acc: 94.84%, Mean Loss: 0.9017\n",
            "\n",
            "================== Test Subject 18 ==================\n",
            "Selected test subjects: [np.int64(18)]\n",
            "Train: (9935, 128, 9) (9935,)\n",
            "Test : (364, 128, 9) (364,)\n",
            "General model → Acc: 94.78%, Mean Loss: 0.2469\n",
            "Simple average → Acc: 94.23%, Mean Loss: 1.2044\n",
            "Hybrid → Acc: 95.33%, Mean Loss: 0.8434\n",
            "Hybrid-KLE → Acc: 94.78%, Mean Loss: 0.8597\n",
            "Hybrid-KLE (rep-based) → Acc: 94.51%, Mean Loss: 0.8942\n",
            "\n",
            "================== Test Subject 19 ==================\n",
            "Selected test subjects: [np.int64(19)]\n",
            "Train: (9939, 128, 9) (9939,)\n",
            "Test : (360, 128, 9) (360,)\n",
            "General model → Acc: 98.89%, Mean Loss: 0.0613\n",
            "Simple average → Acc: 95.56%, Mean Loss: 1.0984\n",
            "Hybrid → Acc: 97.50%, Mean Loss: 0.1834\n",
            "Hybrid-KLE → Acc: 99.17%, Mean Loss: 0.1733\n",
            "Hybrid-KLE (rep-based) → Acc: 98.89%, Mean Loss: 0.1731\n",
            "\n",
            "================== Test Subject 20 ==================\n",
            "Selected test subjects: [np.int64(20)]\n",
            "Train: (9945, 128, 9) (9945,)\n",
            "Test : (354, 128, 9) (354,)\n",
            "General model → Acc: 98.87%, Mean Loss: 0.1503\n",
            "Simple average → Acc: 99.44%, Mean Loss: 1.1575\n",
            "Hybrid → Acc: 98.02%, Mean Loss: 0.5080\n",
            "Hybrid-KLE → Acc: 98.87%, Mean Loss: 0.5104\n",
            "Hybrid-KLE (rep-based) → Acc: 99.44%, Mean Loss: 0.5451\n",
            "\n",
            "================== Test Subject 21 ==================\n",
            "Selected test subjects: [np.int64(21)]\n",
            "Train: (9891, 128, 9) (9891,)\n",
            "Test : (408, 128, 9) (408,)\n",
            "General model → Acc: 96.81%, Mean Loss: 0.1736\n",
            "Simple average → Acc: 97.55%, Mean Loss: 1.1591\n",
            "Hybrid → Acc: 99.02%, Mean Loss: 0.5450\n",
            "Hybrid-KLE → Acc: 98.04%, Mean Loss: 0.5636\n",
            "Hybrid-KLE (rep-based) → Acc: 97.30%, Mean Loss: 0.6105\n",
            "\n",
            "================== Test Subject 22 ==================\n",
            "Selected test subjects: [np.int64(22)]\n",
            "Train: (9978, 128, 9) (9978,)\n",
            "Test : (321, 128, 9) (321,)\n",
            "General model → Acc: 100.00%, Mean Loss: 0.0520\n",
            "Simple average → Acc: 100.00%, Mean Loss: 1.0876\n",
            "Hybrid → Acc: 100.00%, Mean Loss: 0.2392\n",
            "Hybrid-KLE → Acc: 100.00%, Mean Loss: 0.2403\n",
            "Hybrid-KLE (rep-based) → Acc: 100.00%, Mean Loss: 0.2491\n",
            "\n",
            "================== Test Subject 23 ==================\n",
            "Selected test subjects: [np.int64(23)]\n",
            "Train: (9927, 128, 9) (9927,)\n",
            "Test : (372, 128, 9) (372,)\n",
            "General model → Acc: 92.74%, Mean Loss: 0.3919\n",
            "Simple average → Acc: 90.05%, Mean Loss: 1.2389\n",
            "Hybrid → Acc: 90.59%, Mean Loss: 0.7754\n",
            "Hybrid-KLE → Acc: 90.32%, Mean Loss: 0.7845\n",
            "Hybrid-KLE (rep-based) → Acc: 92.20%, Mean Loss: 0.8342\n",
            "\n",
            "================== Test Subject 24 ==================\n",
            "Selected test subjects: [np.int64(24)]\n",
            "Train: (9918, 128, 9) (9918,)\n",
            "Test : (381, 128, 9) (381,)\n",
            "General model → Acc: 85.56%, Mean Loss: 0.2866\n",
            "Simple average → Acc: 85.56%, Mean Loss: 1.2473\n",
            "Hybrid → Acc: 77.69%, Mean Loss: 0.8817\n",
            "Hybrid-KLE → Acc: 80.58%, Mean Loss: 0.8788\n",
            "Hybrid-KLE (rep-based) → Acc: 84.25%, Mean Loss: 0.8602\n",
            "\n",
            "================== Test Subject 25 ==================\n",
            "Selected test subjects: [np.int64(25)]\n",
            "Train: (9890, 128, 9) (9890,)\n",
            "Test : (409, 128, 9) (409,)\n",
            "General model → Acc: 87.53%, Mean Loss: 0.3813\n",
            "Simple average → Acc: 87.78%, Mean Loss: 1.2415\n",
            "Hybrid → Acc: 85.09%, Mean Loss: 0.8688\n",
            "Hybrid-KLE → Acc: 86.06%, Mean Loss: 0.8666\n",
            "Hybrid-KLE (rep-based) → Acc: 88.02%, Mean Loss: 0.8803\n",
            "\n",
            "================== Test Subject 26 ==================\n",
            "Selected test subjects: [np.int64(26)]\n",
            "Train: (9907, 128, 9) (9907,)\n",
            "Test : (392, 128, 9) (392,)\n",
            "General model → Acc: 100.00%, Mean Loss: 0.0947\n",
            "Simple average → Acc: 100.00%, Mean Loss: 1.1275\n",
            "Hybrid → Acc: 97.19%, Mean Loss: 0.3297\n",
            "Hybrid-KLE → Acc: 99.74%, Mean Loss: 0.3161\n",
            "Hybrid-KLE (rep-based) → Acc: 99.74%, Mean Loss: 0.3476\n",
            "\n",
            "================== Test Subject 27 ==================\n",
            "Selected test subjects: [np.int64(27)]\n",
            "Train: (9923, 128, 9) (9923,)\n",
            "Test : (376, 128, 9) (376,)\n",
            "General model → Acc: 99.20%, Mean Loss: 0.1447\n",
            "Simple average → Acc: 99.47%, Mean Loss: 1.1522\n",
            "Hybrid → Acc: 98.40%, Mean Loss: 0.6126\n",
            "Hybrid-KLE → Acc: 99.73%, Mean Loss: 0.6108\n",
            "Hybrid-KLE (rep-based) → Acc: 98.67%, Mean Loss: 0.6592\n",
            "\n",
            "================== Test Subject 28 ==================\n",
            "Selected test subjects: [np.int64(28)]\n",
            "Train: (9917, 128, 9) (9917,)\n",
            "Test : (382, 128, 9) (382,)\n",
            "General model → Acc: 89.79%, Mean Loss: 0.3245\n",
            "Simple average → Acc: 89.79%, Mean Loss: 1.1746\n",
            "Hybrid → Acc: 89.53%, Mean Loss: 0.3256\n",
            "Hybrid-KLE → Acc: 89.53%, Mean Loss: 0.3244\n",
            "Hybrid-KLE (rep-based) → Acc: 89.27%, Mean Loss: 0.3394\n",
            "\n",
            "================== Test Subject 29 ==================\n",
            "Selected test subjects: [np.int64(29)]\n",
            "Train: (9955, 128, 9) (9955,)\n",
            "Test : (344, 128, 9) (344,)\n",
            "General model → Acc: 89.53%, Mean Loss: 0.2305\n",
            "Simple average → Acc: 87.79%, Mean Loss: 1.2210\n",
            "Hybrid → Acc: 88.95%, Mean Loss: 0.8386\n",
            "Hybrid-KLE → Acc: 88.37%, Mean Loss: 0.8371\n",
            "Hybrid-KLE (rep-based) → Acc: 89.83%, Mean Loss: 0.8563\n",
            "\n",
            "================== Test Subject 30 ==================\n",
            "Selected test subjects: [np.int64(30)]\n",
            "Train: (9916, 128, 9) (9916,)\n",
            "Test : (383, 128, 9) (383,)\n",
            "General model → Acc: 91.64%, Mean Loss: 0.2423\n",
            "Simple average → Acc: 92.17%, Mean Loss: 1.2030\n",
            "Hybrid → Acc: 95.30%, Mean Loss: 0.7139\n",
            "Hybrid-KLE → Acc: 95.30%, Mean Loss: 0.7284\n",
            "Hybrid-KLE (rep-based) → Acc: 91.91%, Mean Loss: 0.7765\n",
            "\n",
            "================== Average Accuracies & Losses Across Subjects ==================\n",
            "📊 General Avg Accuracy: 91.5864, Mean Loss: 0.2953\n",
            "📊 Ensemble Avg Accuracy: 91.3457, Mean Loss: 1.1985\n",
            "📊 Hybrid Avg Accuracy: 90.4128, Mean Loss: 0.6696\n",
            "📊 Hybrid-KLE Avg Accuracy: 90.7528, Mean Loss: 0.6716\n",
            "📊 Hybrid-KLE (rep-based) Avg Accuracy: 91.4232, Mean Loss: 0.6933\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "from scipy.stats import entropy\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import copy\n",
        "\n",
        "# ---------------------------\n",
        "# CNN-GRU Model\n",
        "# ---------------------------\n",
        "class CNNGRU(nn.Module):\n",
        "    def __init__(self, n_steps, n_length, input_dim, n_hidden, n_classes):\n",
        "        super(CNNGRU, self).__init__()\n",
        "\n",
        "        # CNN layers (smaller channels + batchnorm)\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=3)\n",
        "        self.bn1   = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3)\n",
        "        self.bn2   = nn.BatchNorm1d(16)\n",
        "        self.dropout1 = nn.Dropout(0.6)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # compute flattened size dynamically\n",
        "        dummy = torch.zeros(1, input_dim, n_length)\n",
        "        dummy_out = self.pool(self.conv2(self.conv1(dummy)))\n",
        "        flat_size = dummy_out.numel()\n",
        "\n",
        "        # GRU\n",
        "        self.gru = nn.GRU(flat_size, n_hidden//2, batch_first=True)\n",
        "        self.layer_norm = nn.LayerNorm(n_hidden//2)\n",
        "\n",
        "        # residual projection\n",
        "        self.residual_proj = nn.Linear(flat_size, n_hidden//2)\n",
        "\n",
        "        # FC layers\n",
        "        self.dropout2 = nn.Dropout(0.6)\n",
        "        self.fc1 = nn.Linear(n_hidden//2, n_hidden//2)\n",
        "        self.fc2 = nn.Linear(n_hidden//2, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass: returns logits\"\"\"\n",
        "        batch, n_steps, in_ch, seq_len = x.size(0), x.size(1), x.size(3), x.size(2)\n",
        "        x = x.permute(0, 1, 3, 2).reshape(batch * n_steps, in_ch, seq_len)\n",
        "\n",
        "        # CNN + BN + pool\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.reshape(batch, n_steps, -1)\n",
        "\n",
        "        # GRU + residual\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]\n",
        "        res = self.residual_proj(x[:, -1, :])\n",
        "        out = self.layer_norm(out + res)\n",
        "\n",
        "        # FC\n",
        "        out = self.dropout2(out)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        logits = self.fc2(out)\n",
        "        return logits\n",
        "\n",
        "    def get_fc1_output(self, x):\n",
        "        \"\"\"Return the fc1 representation before final classification\"\"\"\n",
        "        batch, n_steps, in_ch, seq_len = x.size(0), x.size(1), x.size(3), x.size(2)\n",
        "        x = x.permute(0, 1, 3, 2).reshape(batch * n_steps, in_ch, seq_len)\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.reshape(batch, n_steps, -1)\n",
        "\n",
        "        # GRU + residual\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]\n",
        "        res = self.residual_proj(x[:, -1, :])\n",
        "        out = self.layer_norm(out + res)\n",
        "\n",
        "        # FC1 only\n",
        "        out = self.dropout2(out)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        return out\n",
        "\n",
        "    def forward_with_fc1(self, x):\n",
        "        \"\"\"Return both logits and fc1 features for loss & rep-based ensembles\"\"\"\n",
        "        fc1_out = self.get_fc1_output(x)\n",
        "        logits = self.fc2(fc1_out)\n",
        "        return logits, fc1_out\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Simple Average Ensemble\n",
        "# ---------------------------\n",
        "def simple_average_ensemble(probs_list):\n",
        "    fused_probs = np.mean(probs_list, axis=0)\n",
        "    final_pred = fused_probs.argmax()\n",
        "    return fused_probs, final_pred\n",
        "\n",
        "# ---------------------------\n",
        "# Results dict\n",
        "# ---------------------------\n",
        "results = {}\n",
        "all_subjects = np.arange(1, 31)\n",
        "entropy_threshold = 0.3\n",
        "K = 3\n",
        "N = 50\n",
        "\n",
        "for test_subject_id in all_subjects:\n",
        "    print(f\"\\n================== Test Subject {test_subject_id} ==================\")\n",
        "\n",
        "    # Load data\n",
        "    X_train, y_train, X_test, y_test, train_subjects = load_data(dataset_path, test_subjects=[test_subject_id])\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_train_encoded = le.fit_transform(y_train)\n",
        "    y_test_encoded  = le.transform(y_test)\n",
        "    n_classes = len(le.classes_)\n",
        "\n",
        "    # Reshape\n",
        "    X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, X_train.shape[2]))\n",
        "    X_test  = X_test.reshape((X_test.shape[0], n_steps, n_length, X_test.shape[2]))\n",
        "\n",
        "    # Torch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "    y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "    # DataLoader\n",
        "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # General model\n",
        "    general_model = CNNGRU(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(general_model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "    # Train general model\n",
        "    for epoch in range(epochs_general):\n",
        "        general_model.train()\n",
        "        for Xb, yb in train_loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = general_model(Xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(general_model.parameters(), max_norm=5.0)\n",
        "            optimizer.step()\n",
        "        scheduler.step(loss)\n",
        "\n",
        "    # Store general weights\n",
        "    general_weights = general_model.state_dict()\n",
        "    personalized_models = {\"general\": copy.deepcopy(general_weights)}\n",
        "\n",
        "    # Fine-tune per training subject\n",
        "    unique_train_subj = np.unique(train_subjects)\n",
        "    for subj in unique_train_subj:\n",
        "        subj_mask = train_subjects == subj\n",
        "        subj_X, subj_y = X_train[subj_mask], y_train_encoded[subj_mask]\n",
        "\n",
        "        subj_X_tensor = torch.tensor(subj_X, dtype=torch.float32).to(device)\n",
        "        subj_y_tensor = torch.tensor(subj_y, dtype=torch.long).to(device)\n",
        "\n",
        "        personal_model = CNNGRU(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "        personal_model.load_state_dict(copy.deepcopy(general_weights))\n",
        "\n",
        "        # Freeze all except fc2\n",
        "        for param in personal_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in personal_model.fc2.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        optimizer_subj = torch.optim.Adam(filter(lambda p: p.requires_grad, personal_model.parameters()), lr=0.005)\n",
        "        criterion_subj = nn.CrossEntropyLoss()\n",
        "        loader_subj = DataLoader(TensorDataset(subj_X_tensor, subj_y_tensor), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Fine-tune\n",
        "        personal_model.train()\n",
        "        for epoch in range(epochs_finetune):\n",
        "            for Xb, yb in loader_subj:\n",
        "                optimizer_subj.zero_grad()\n",
        "                outputs = personal_model(Xb)\n",
        "                loss = criterion_subj(outputs, yb)\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(personal_model.parameters(), max_norm=5.0)\n",
        "                optimizer_subj.step()\n",
        "\n",
        "        personalized_models[subj] = copy.deepcopy(personal_model.state_dict())\n",
        "\n",
        "    # ---------------------------\n",
        "    # Instantiate all personalized models ONCE\n",
        "    # ---------------------------\n",
        "    eval_models = {}\n",
        "    for subj, state in personalized_models.items():\n",
        "        model_eval = CNNGRU(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "        model_eval.load_state_dict(state)\n",
        "        model_eval.eval()   # evaluation mode\n",
        "        eval_models[subj] = model_eval\n",
        "\n",
        "    # ---------------------------\n",
        "    # Evaluate test set (shuffled) with loss\n",
        "    # ---------------------------\n",
        "    general_correct = 0\n",
        "    ensemble_correct = 0\n",
        "    hybrid_correct = 0\n",
        "    hybrid_kle_correct = 0\n",
        "    hybrid_kle_rep_correct = 0\n",
        "\n",
        "    # Initialize loss accumulators\n",
        "    total_loss_general = 0\n",
        "    total_loss_ensemble = 0\n",
        "    total_loss_hybrid = 0\n",
        "    total_loss_kle = 0\n",
        "    total_loss_kle_rep = 0\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    n_samples = len(X_test_tensor)\n",
        "    shuffled_indices = np.random.permutation(n_samples)\n",
        "\n",
        "    past_reps, past_probs, past_labels = [], [], []\n",
        "\n",
        "    for i in shuffled_indices:\n",
        "        x_single = X_test_tensor[i].unsqueeze(0).to(device)\n",
        "        y_true   = torch.tensor([y_test_encoded[i]], dtype=torch.long).to(device)\n",
        "\n",
        "        # --- General model ---\n",
        "        general_model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = general_model(x_single)\n",
        "            probs_general = torch.softmax(logits, dim=1)\n",
        "            pred_general = probs_general.argmax(dim=1).item()\n",
        "            ent_general = entropy(probs_general.cpu().numpy().flatten())\n",
        "            loss_general = criterion(logits, y_true).item()\n",
        "            total_loss_general += loss_general\n",
        "\n",
        "            # fc1 representation\n",
        "            rep = general_model.get_fc1_output(x_single).cpu().numpy().flatten()\n",
        "\n",
        "        if pred_general == y_true.item():\n",
        "            general_correct += 1\n",
        "\n",
        "        # --- Personalized models ---\n",
        "        all_probs, all_preds, all_ents = [], [], []\n",
        "        for subj, state in personalized_models.items():\n",
        "            if subj == \"general\":\n",
        "                continue\n",
        "            model_eval = CNNGRU(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "            model_eval.load_state_dict(state)\n",
        "            model_eval.eval()\n",
        "            with torch.no_grad():\n",
        "                logits = model_eval(x_single)\n",
        "                probs = torch.softmax(logits, dim=1).cpu().numpy().flatten()\n",
        "                all_probs.append(probs)\n",
        "                all_preds.append(probs.argmax())\n",
        "                all_ents.append(entropy(probs))\n",
        "\n",
        "        # --- Simple Average Ensemble ---\n",
        "        fused_probs_avg, fused_pred_avg = simple_average_ensemble(all_probs)\n",
        "        total_loss_ensemble += criterion(torch.tensor(fused_probs_avg).unsqueeze(0), y_true).item()\n",
        "        if fused_pred_avg == y_true.item():\n",
        "            ensemble_correct += 1\n",
        "\n",
        "        # --- Hybrid (General vs Best Personal) ---\n",
        "        if ent_general <= entropy_threshold:\n",
        "            hybrid_pred = pred_general\n",
        "            hybrid_logits = logits\n",
        "        else:\n",
        "            best_idx = int(np.argmin(all_ents))\n",
        "            hybrid_pred = all_preds[best_idx]\n",
        "            hybrid_logits = torch.tensor(all_probs[best_idx]).unsqueeze(0)\n",
        "        total_loss_hybrid += criterion(hybrid_logits, y_true).item()\n",
        "        if hybrid_pred == y_true.item():\n",
        "            hybrid_correct += 1\n",
        "\n",
        "        # --- Hybrid-KLE (avg of K lowest-entropy personalized models) ---\n",
        "        if ent_general <= entropy_threshold:\n",
        "            hybrid_kle_pred = pred_general\n",
        "            hybrid_kle_logits = logits\n",
        "        else:\n",
        "            k_idx = np.argsort(all_ents)[:K]\n",
        "            k_probs = [all_probs[j] for j in k_idx]\n",
        "            fused_probs_kle, hybrid_kle_pred = simple_average_ensemble(k_probs)\n",
        "            hybrid_kle_logits = torch.tensor(fused_probs_kle).unsqueeze(0)\n",
        "        total_loss_kle += criterion(hybrid_kle_logits, y_true).item()\n",
        "        if hybrid_kle_pred == y_true.item():\n",
        "            hybrid_kle_correct += 1\n",
        "\n",
        "        # --- Hybrid-KLE (representation-based using past fc1 reps) ---\n",
        "        if ent_general <= entropy_threshold:\n",
        "            hybrid_kle_rep_pred = pred_general\n",
        "            hybrid_kle_rep_logits = logits\n",
        "        else:\n",
        "            if len(past_reps) < K or len(past_reps) < N:\n",
        "                best_idx = int(np.argmin(all_ents))\n",
        "                hybrid_kle_rep_pred = all_preds[best_idx]\n",
        "                hybrid_kle_rep_logits = torch.tensor(all_probs[best_idx]).unsqueeze(0)\n",
        "            else:\n",
        "                sims = cosine_similarity(rep.reshape(1, -1), np.vstack(past_reps))[0]\n",
        "                topk_idx = np.argsort(sims)[-K:]\n",
        "                kle_probs = [past_probs[j] for j in topk_idx]\n",
        "                fused_probs_kle, hybrid_kle_rep_pred = simple_average_ensemble(kle_probs)\n",
        "                hybrid_kle_rep_logits = torch.tensor(fused_probs_kle).unsqueeze(0)\n",
        "        total_loss_kle_rep += criterion(hybrid_kle_rep_logits, y_true).item()\n",
        "        if hybrid_kle_rep_pred == y_true.item():\n",
        "            hybrid_kle_rep_correct += 1\n",
        "\n",
        "        # Store past reps & general probs\n",
        "        past_reps.append(rep)\n",
        "        past_probs.append(probs_general.cpu().numpy().flatten())\n",
        "        past_labels.append(y_true.item())\n",
        "\n",
        "    # ---------------------------\n",
        "    # Report results with mean loss\n",
        "    # ---------------------------\n",
        "    general_acc = (general_correct / n_samples) * 100\n",
        "    ensemble_acc = (ensemble_correct / n_samples) * 100\n",
        "    hybrid_acc = (hybrid_correct / n_samples) * 100\n",
        "    hybrid_kle_acc = (hybrid_kle_correct / n_samples) * 100\n",
        "    hybrid_kle_rep_acc = (hybrid_kle_rep_correct / n_samples) * 100\n",
        "\n",
        "    mean_loss_general = total_loss_general / n_samples\n",
        "    mean_loss_ensemble = total_loss_ensemble / n_samples\n",
        "    mean_loss_hybrid = total_loss_hybrid / n_samples\n",
        "    mean_loss_kle = total_loss_kle / n_samples\n",
        "    mean_loss_kle_rep = total_loss_kle_rep / n_samples\n",
        "\n",
        "    print(f\"General model → Acc: {general_acc:.2f}%, Mean Loss: {mean_loss_general:.4f}\")\n",
        "    print(f\"Simple average → Acc: {ensemble_acc:.2f}%, Mean Loss: {mean_loss_ensemble:.4f}\")\n",
        "    print(f\"Hybrid → Acc: {hybrid_acc:.2f}%, Mean Loss: {mean_loss_hybrid:.4f}\")\n",
        "    print(f\"Hybrid-KLE → Acc: {hybrid_kle_acc:.2f}%, Mean Loss: {mean_loss_kle:.4f}\")\n",
        "    print(f\"Hybrid-KLE (rep-based) → Acc: {hybrid_kle_rep_acc:.2f}%, Mean Loss: {mean_loss_kle_rep:.4f}\")\n",
        "\n",
        "\n",
        "    results[test_subject_id] = {\n",
        "    \"general_acc\": general_acc,\n",
        "    \"ensemble_acc\": ensemble_acc,\n",
        "    \"hybrid_acc\": hybrid_acc,\n",
        "    \"hybrid_kle_acc\": hybrid_kle_acc,\n",
        "    \"hybrid_kle_rep_acc\": hybrid_kle_rep_acc,\n",
        "    \"general_loss\": mean_loss_general,\n",
        "    \"ensemble_loss\": mean_loss_ensemble,\n",
        "    \"hybrid_loss\": mean_loss_hybrid,\n",
        "    \"hybrid_kle_loss\": mean_loss_kle,\n",
        "    \"hybrid_kle_rep_loss\": mean_loss_kle_rep,\n",
        "}\n",
        "\n",
        "# Average across subjects\n",
        "avg_general_acc  = np.mean([res[\"general_acc\"] for res in results.values()])\n",
        "avg_ensemble_acc = np.mean([res[\"ensemble_acc\"] for res in results.values()])\n",
        "avg_hybrid_acc   = np.mean([res[\"hybrid_acc\"]   for res in results.values()])\n",
        "avg_kle_acc      = np.mean([res[\"hybrid_kle_acc\"] for res in results.values()])\n",
        "avg_kle_rep_acc  = np.mean([res[\"hybrid_kle_rep_acc\"] for res in results.values()])\n",
        "\n",
        "avg_general_loss  = np.mean([res[\"general_loss\"] for res in results.values()])\n",
        "avg_ensemble_loss = np.mean([res[\"ensemble_loss\"] for res in results.values()])\n",
        "avg_hybrid_loss   = np.mean([res[\"hybrid_loss\"]   for res in results.values()])\n",
        "avg_kle_loss      = np.mean([res[\"hybrid_kle_loss\"] for res in results.values()])\n",
        "avg_kle_rep_loss  = np.mean([res[\"hybrid_kle_rep_loss\"] for res in results.values()])\n",
        "\n",
        "print(\"\\n================== Average Accuracies & Losses Across Subjects ==================\")\n",
        "print(f\"📊 General Avg Accuracy: {avg_general_acc:.4f}, Mean Loss: {avg_general_loss:.4f}\")\n",
        "print(f\"📊 Ensemble Avg Accuracy: {avg_ensemble_acc:.4f}, Mean Loss: {avg_ensemble_loss:.4f}\")\n",
        "print(f\"📊 Hybrid Avg Accuracy: {avg_hybrid_acc:.4f}, Mean Loss: {avg_hybrid_loss:.4f}\")\n",
        "print(f\"📊 Hybrid-KLE Avg Accuracy: {avg_kle_acc:.4f}, Mean Loss: {avg_kle_loss:.4f}\")\n",
        "print(f\"📊 Hybrid-KLE (rep-based) Avg Accuracy: {avg_kle_rep_acc:.4f}, Mean Loss: {avg_kle_rep_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRmlwtlmASyF"
      },
      "source": [
        "## **Domain Adaptation (source and target data + source models)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLhrH1nCEwHk"
      },
      "source": [
        "**Assembly the best personalized model for the test subjects**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLraxXBmBR4V",
        "outputId": "1621ac1d-85ae-4081-eae3-de576146dc17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Best subset for test subject 9: [np.int64(19), np.int64(25)], acc=84.38%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import copy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ============================================================\n",
        "# Build averaged personalized CNN-LSTM model\n",
        "# ============================================================\n",
        "def build_average_personalized_cnnlstm(\n",
        "    similar_train_subjects: list,\n",
        "    personalized_models: dict,\n",
        "    n_steps: int,\n",
        "    n_length: int,\n",
        "    n_channels: int,\n",
        "    n_hidden: int,\n",
        "    n_classes: int,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Build an averaged personalized CNN-LSTM model from subject-specific heads.\n",
        "    \"\"\"\n",
        "    if \"general\" not in personalized_models:\n",
        "        raise KeyError(\"personalized_models must contain the 'general' snapshot.\")\n",
        "\n",
        "    # Backbone from general model\n",
        "    model = CNNLSTM(n_steps, n_length, n_channels, n_hidden, n_classes).to(device)\n",
        "    general_sd = personalized_models[\"general\"]\n",
        "\n",
        "    # Load full general weights\n",
        "    model.load_state_dict(copy.deepcopy(general_sd))\n",
        "\n",
        "    # Collect subject-specific classifier heads\n",
        "    weight_list, bias_list = [], []\n",
        "    for sid in similar_train_subjects:\n",
        "        if sid not in personalized_models:\n",
        "            raise KeyError(f\"personalized_models has no head for subject {sid}.\")\n",
        "        subj_state = personalized_models[sid]\n",
        "        weight_list.append(subj_state[\"fc2.weight\"])\n",
        "        bias_list.append(subj_state[\"fc2.bias\"])\n",
        "\n",
        "    # Average heads\n",
        "    avg_weight = torch.stack(weight_list).mean(dim=0)\n",
        "    avg_bias   = torch.stack(bias_list).mean(dim=0)\n",
        "\n",
        "    # Inject averaged head into the model\n",
        "    with torch.no_grad():\n",
        "        model.fc2.weight.copy_(avg_weight)\n",
        "        model.fc2.bias.copy_(avg_bias)\n",
        "\n",
        "    return model.eval()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Evaluation helper\n",
        "# ============================================================\n",
        "def evaluate_model(model, X_test, y_test, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_test_tensor)\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    return accuracy_score(y_test, preds)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Greedy search for best subset of training subjects\n",
        "# ============================================================\n",
        "def find_best_similar_subjects(\n",
        "    train_subject_ids,\n",
        "    personalized_models,\n",
        "    test_X,\n",
        "    test_y_encoded,\n",
        "    n_steps,\n",
        "    n_length,\n",
        "    n_channels,\n",
        "    n_hidden,\n",
        "    n_classes,\n",
        "    device=\"cpu\",\n",
        "    max_subset_size=5\n",
        "):\n",
        "    best_subset = []\n",
        "    best_acc = 0.0\n",
        "    remaining = set(train_subject_ids)\n",
        "\n",
        "    for step in range(max_subset_size):\n",
        "        best_candidate, best_candidate_acc = None, 0.0\n",
        "\n",
        "        for sid in remaining:\n",
        "            candidate_subset = best_subset + [sid]\n",
        "            model = build_average_personalized_cnnlstm(\n",
        "                candidate_subset,\n",
        "                personalized_models,\n",
        "                n_steps, n_length, n_channels, n_hidden, n_classes,\n",
        "                device=device\n",
        "            )\n",
        "            acc = evaluate_model(model, test_X, test_y_encoded, device=device)\n",
        "            if acc > best_candidate_acc:\n",
        "                best_candidate_acc = acc\n",
        "                best_candidate = sid\n",
        "\n",
        "        if best_candidate is not None and best_candidate_acc > best_acc:\n",
        "            best_subset.append(best_candidate)\n",
        "            remaining.remove(best_candidate)\n",
        "            best_acc = best_candidate_acc\n",
        "        else:\n",
        "            break  # no improvement\n",
        "\n",
        "    return best_subset, best_acc\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Example usage\n",
        "# ============================================================\n",
        "available_train_subjects = [sid for sid in unique_subjects if sid in personalized_models]\n",
        "test_subject_id = test_subjects[0]\n",
        "\n",
        "best_subset, best_acc = find_best_similar_subjects(\n",
        "    train_subject_ids=available_train_subjects,\n",
        "    personalized_models=personalized_models,\n",
        "    test_X=X_test,\n",
        "    test_y_encoded=y_test_encoded,\n",
        "    n_steps=n_steps, n_length=n_length, n_channels=X_train.shape[3],\n",
        "    n_hidden=n_hidden, n_classes=n_classes,\n",
        "    device=device,\n",
        "    max_subset_size=3\n",
        ")\n",
        "\n",
        "print(f\"✅ Best subset for test subject {test_subject_id}: {best_subset}, acc={best_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvNY1DnwEnfs"
      },
      "source": [
        "**The LOSO loop over all subjects**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pgvQt3ETde9",
        "outputId": "4aa70592-4217-430d-a575-b9e40c2e7f85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-3484008046.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  return pd.read_csv(filename, sep='\\s+', header=None)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================== Test Subject 1 ==================\n",
            "General model accuracy: 100.00%\n",
            "Assembled model accuracy (greedy best subset): 100.00%\n",
            "Best subset for test subject 1: [np.int64(2)]\n",
            "\n",
            "================== Test Subject 2 ==================\n",
            "General model accuracy: 89.07%\n",
            "Assembled model accuracy (greedy best subset): 89.74%\n",
            "Best subset for test subject 2: [np.int64(28)]\n",
            "\n",
            "================== Test Subject 3 ==================\n",
            "General model accuracy: 96.19%\n",
            "Assembled model accuracy (greedy best subset): 97.65%\n",
            "Best subset for test subject 3: [np.int64(16)]\n",
            "\n",
            "================== Test Subject 4 ==================\n",
            "General model accuracy: 83.28%\n",
            "Assembled model accuracy (greedy best subset): 93.06%\n",
            "Best subset for test subject 4: [np.int64(25), np.int64(30)]\n",
            "\n",
            "================== Test Subject 5 ==================\n",
            "General model accuracy: 83.77%\n",
            "Assembled model accuracy (greedy best subset): 92.72%\n",
            "Best subset for test subject 5: [np.int64(8)]\n",
            "\n",
            "================== Test Subject 6 ==================\n",
            "General model accuracy: 100.00%\n",
            "Assembled model accuracy (greedy best subset): 100.00%\n",
            "Best subset for test subject 6: [np.int64(2)]\n",
            "\n",
            "================== Test Subject 7 ==================\n",
            "General model accuracy: 75.65%\n",
            "Assembled model accuracy (greedy best subset): 84.74%\n",
            "Best subset for test subject 7: [np.int64(18), np.int64(4)]\n",
            "\n",
            "================== Test Subject 8 ==================\n",
            "General model accuracy: 87.54%\n",
            "Assembled model accuracy (greedy best subset): 99.29%\n",
            "Best subset for test subject 8: [np.int64(29)]\n",
            "\n",
            "================== Test Subject 9 ==================\n",
            "General model accuracy: 77.78%\n",
            "Assembled model accuracy (greedy best subset): 81.94%\n",
            "Best subset for test subject 9: [np.int64(13)]\n",
            "\n",
            "================== Test Subject 10 ==================\n",
            "General model accuracy: 60.54%\n",
            "Assembled model accuracy (greedy best subset): 62.24%\n",
            "Best subset for test subject 10: [np.int64(5)]\n",
            "\n",
            "================== Test Subject 11 ==================\n",
            "General model accuracy: 99.05%\n",
            "Assembled model accuracy (greedy best subset): 100.00%\n",
            "Best subset for test subject 11: [np.int64(2)]\n",
            "\n",
            "================== Test Subject 12 ==================\n",
            "General model accuracy: 98.44%\n",
            "Assembled model accuracy (greedy best subset): 99.06%\n",
            "Best subset for test subject 12: [np.int64(9)]\n",
            "\n",
            "================== Test Subject 13 ==================\n",
            "General model accuracy: 94.19%\n",
            "Assembled model accuracy (greedy best subset): 97.55%\n",
            "Best subset for test subject 13: [np.int64(2), np.int64(12)]\n",
            "\n",
            "================== Test Subject 14 ==================\n",
            "General model accuracy: 78.33%\n",
            "Assembled model accuracy (greedy best subset): 79.88%\n",
            "Best subset for test subject 14: [np.int64(28)]\n",
            "\n",
            "================== Test Subject 15 ==================\n",
            "General model accuracy: 100.00%\n",
            "Assembled model accuracy (greedy best subset): 100.00%\n",
            "Best subset for test subject 15: [np.int64(1)]\n",
            "\n",
            "================== Test Subject 16 ==================\n",
            "General model accuracy: 71.04%\n",
            "Assembled model accuracy (greedy best subset): 75.96%\n",
            "Best subset for test subject 16: [np.int64(2)]\n",
            "\n",
            "================== Test Subject 17 ==================\n",
            "General model accuracy: 92.66%\n",
            "Assembled model accuracy (greedy best subset): 95.38%\n",
            "Best subset for test subject 17: [np.int64(28)]\n",
            "\n",
            "================== Test Subject 18 ==================\n",
            "General model accuracy: 97.53%\n",
            "Assembled model accuracy (greedy best subset): 99.45%\n",
            "Best subset for test subject 18: [np.int64(3)]\n",
            "\n",
            "================== Test Subject 19 ==================\n",
            "General model accuracy: 97.22%\n",
            "Assembled model accuracy (greedy best subset): 98.06%\n",
            "Best subset for test subject 19: [np.int64(10)]\n",
            "\n",
            "================== Test Subject 20 ==================\n",
            "General model accuracy: 94.07%\n",
            "Assembled model accuracy (greedy best subset): 100.00%\n",
            "Best subset for test subject 20: [np.int64(2)]\n",
            "\n",
            "================== Test Subject 21 ==================\n",
            "General model accuracy: 98.28%\n",
            "Assembled model accuracy (greedy best subset): 98.77%\n",
            "Best subset for test subject 21: [np.int64(2)]\n",
            "\n",
            "================== Test Subject 22 ==================\n",
            "General model accuracy: 99.69%\n",
            "Assembled model accuracy (greedy best subset): 99.69%\n",
            "Best subset for test subject 22: [np.int64(1)]\n",
            "\n",
            "================== Test Subject 23 ==================\n",
            "General model accuracy: 88.98%\n",
            "Assembled model accuracy (greedy best subset): 98.39%\n",
            "Best subset for test subject 23: [np.int64(9)]\n",
            "\n",
            "================== Test Subject 24 ==================\n",
            "General model accuracy: 100.00%\n",
            "Assembled model accuracy (greedy best subset): 100.00%\n",
            "Best subset for test subject 24: [np.int64(1)]\n",
            "\n",
            "================== Test Subject 25 ==================\n",
            "General model accuracy: 79.22%\n",
            "Assembled model accuracy (greedy best subset): 80.44%\n",
            "Best subset for test subject 25: [np.int64(4), np.int64(12)]\n",
            "\n",
            "================== Test Subject 26 ==================\n",
            "General model accuracy: 97.19%\n",
            "Assembled model accuracy (greedy best subset): 97.96%\n",
            "Best subset for test subject 26: [np.int64(19)]\n",
            "\n",
            "================== Test Subject 27 ==================\n",
            "General model accuracy: 90.96%\n",
            "Assembled model accuracy (greedy best subset): 97.07%\n",
            "Best subset for test subject 27: [np.int64(12), np.int64(13), np.int64(18)]\n",
            "\n",
            "================== Test Subject 28 ==================\n",
            "General model accuracy: 85.34%\n",
            "Assembled model accuracy (greedy best subset): 85.34%\n",
            "Best subset for test subject 28: [np.int64(1)]\n",
            "\n",
            "================== Test Subject 29 ==================\n",
            "General model accuracy: 90.99%\n",
            "Assembled model accuracy (greedy best subset): 95.06%\n",
            "Best subset for test subject 29: [np.int64(5)]\n",
            "\n",
            "================== Test Subject 30 ==================\n",
            "General model accuracy: 92.43%\n",
            "Assembled model accuracy (greedy best subset): 94.78%\n",
            "Best subset for test subject 30: [np.int64(11)]\n"
          ]
        }
      ],
      "source": [
        "def load_data(dataset_path, test_subjects):\n",
        "    def _read_csv(filename):\n",
        "        return pd.read_csv(filename, sep='\\s+', header=None)\n",
        "\n",
        "\n",
        "    def load_signals(subset):\n",
        "        signals_data = []\n",
        "        for signal in SIGNALS:\n",
        "            filename = f\"{dataset_path}/{subset}/Inertial Signals/{signal}_{subset}.txt\"\n",
        "            signals_data.append(_read_csv(filename).to_numpy())\n",
        "        return np.transpose(signals_data, (1, 2, 0))  # (samples, timesteps=128, 9 signals)\n",
        "\n",
        "    def load_y(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/y_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    def load_subjects(subset):\n",
        "        filename = f\"{dataset_path}/{subset}/subject_{subset}.txt\"\n",
        "        return _read_csv(filename)[0].to_numpy()\n",
        "\n",
        "    X_train, y_train, subj_train = load_signals(\"train\"), load_y(\"train\"), load_subjects(\"train\")\n",
        "    X_test,  y_test,  subj_test  = load_signals(\"test\"),  load_y(\"test\"),  load_subjects(\"test\")\n",
        "\n",
        "    X_all = np.vstack([X_train, X_test])\n",
        "    y_all = np.concatenate([y_train, y_test])\n",
        "    subjects = np.concatenate([subj_train, subj_test])\n",
        "\n",
        "    # Split by subject IDs\n",
        "    train_mask = ~np.isin(subjects, test_subjects)\n",
        "    test_mask  = np.isin(subjects, test_subjects)\n",
        "\n",
        "    train_X, train_y = X_all[train_mask], y_all[train_mask]\n",
        "    test_X,  test_y  = X_all[test_mask],  y_all[test_mask]\n",
        "    train_subjects   = subjects[train_mask]\n",
        "\n",
        "    # No prints here to prevent outputs when loading from Drive\n",
        "    return train_X, train_y, test_X, test_y, train_subjects\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "all_subjects = np.arange(1, 31)  # IDs 1..30\n",
        "\n",
        "for test_subject_id in all_subjects:\n",
        "    print(f\"\\n================== Test Subject {test_subject_id} ==================\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # Load data for current test subject\n",
        "    # ---------------------------\n",
        "    X_train, y_train, X_test, y_test, train_subjects = load_data(dataset_path, test_subjects=[test_subject_id])\n",
        "\n",
        "    # ---------------------------\n",
        "    # Encode labels\n",
        "    # ---------------------------\n",
        "    le = LabelEncoder()\n",
        "    y_train_encoded = le.fit_transform(y_train)\n",
        "    y_test_encoded  = le.transform(y_test)\n",
        "    n_classes = len(le.classes_)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Reshape into subsequences for CNN-LSTM\n",
        "    # ---------------------------\n",
        "    X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, X_train.shape[2]))\n",
        "    X_test  = X_test.reshape((X_test.shape[0],  n_steps, n_length, X_test.shape[2]))\n",
        "\n",
        "    # ---------------------------\n",
        "    # Convert to torch tensors\n",
        "    # ---------------------------\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "    y_test_tensor  = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Create datasets and loaders\n",
        "    # ---------------------------\n",
        "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Train general model\n",
        "    # ---------------------------\n",
        "    general_model = CNNLSTM(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(general_model.parameters(), lr=0.001)\n",
        "\n",
        "    general_model.train()\n",
        "    for epoch in range(epochs_general):\n",
        "        for Xb, yb in train_loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = general_model(Xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # ---------------------------\n",
        "    # Evaluate general model\n",
        "    # ---------------------------\n",
        "    general_model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = general_model(X_test_tensor.to(device))\n",
        "        preds_general = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    acc_general = accuracy_score(y_test_encoded, preds_general) * 100\n",
        "    print(f\"General model accuracy: {acc_general:.2f}%\")\n",
        "\n",
        "    general_weights = general_model.state_dict()\n",
        "    personalized_models = {\"general\": copy.deepcopy(general_weights)}\n",
        "\n",
        "    # ---------------------------\n",
        "    # Fine-tune per training subject\n",
        "    # ---------------------------\n",
        "    unique_train_subj = np.unique(train_subjects)\n",
        "    for subj in unique_train_subj:\n",
        "        subj_mask = train_subjects == subj\n",
        "        subj_X, subj_y = X_train[subj_mask], y_train_encoded[subj_mask]\n",
        "\n",
        "        subj_X_tensor = torch.tensor(subj_X, dtype=torch.float32).to(device)\n",
        "        subj_y_tensor = torch.tensor(subj_y, dtype=torch.long).to(device)\n",
        "\n",
        "        personal_model = CNNLSTM(n_steps, n_length, X_train.shape[3], n_hidden, n_classes).to(device)\n",
        "        personal_model.load_state_dict(copy.deepcopy(general_weights))\n",
        "\n",
        "        # Freeze all except fc1 + fc2\n",
        "        for param in personal_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in personal_model.fc1.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in personal_model.fc2.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        optimizer_subj = torch.optim.Adam(filter(lambda p: p.requires_grad, personal_model.parameters()), lr=0.005)\n",
        "        criterion_subj = nn.CrossEntropyLoss()\n",
        "\n",
        "        dataset_subj = TensorDataset(subj_X_tensor, subj_y_tensor)\n",
        "        loader_subj = DataLoader(dataset_subj, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        personal_model.train()\n",
        "        for epoch in range(epochs_finetune):\n",
        "            for Xb, yb in loader_subj:\n",
        "                optimizer_subj.zero_grad()\n",
        "                outputs = personal_model(Xb)\n",
        "                loss = criterion_subj(outputs, yb)\n",
        "                loss.backward()\n",
        "                optimizer_subj.step()\n",
        "\n",
        "        # Save classifier head only\n",
        "        personalized_models[subj] = {\n",
        "            \"fc2.weight\": personal_model.fc2.weight.detach().cpu().clone(),\n",
        "            \"fc2.bias\":   personal_model.fc2.bias.detach().cpu().clone()\n",
        "        }\n",
        "\n",
        "    # ---------------------------\n",
        "    # Greedy search for best subset\n",
        "    # ---------------------------\n",
        "    available_train_subjects = [sid for sid in unique_train_subj if sid in personalized_models and sid != test_subject_id]\n",
        "    if len(available_train_subjects) == 0:\n",
        "        results[test_subject_id] = {\n",
        "            \"general_acc\": acc_general,\n",
        "            \"assembled_acc\": 0.0,\n",
        "            \"best_subset\": []\n",
        "        }\n",
        "        continue\n",
        "\n",
        "    best_subset, best_acc = find_best_similar_subjects(\n",
        "        train_subject_ids=available_train_subjects,\n",
        "        personalized_models=personalized_models,\n",
        "        test_X=X_test,\n",
        "        test_y_encoded=y_test_encoded,\n",
        "        n_steps=n_steps,\n",
        "        n_length=n_length,\n",
        "        n_channels=X_train.shape[3],\n",
        "        n_hidden=n_hidden,\n",
        "        n_classes=n_classes,\n",
        "        device=device,\n",
        "        max_subset_size=3\n",
        "    )\n",
        "\n",
        "    print(f\"Assembled model accuracy (greedy best subset): {best_acc*100:.2f}%\")\n",
        "    print(f\"Best subset for test subject {test_subject_id}: {best_subset}\")\n",
        "\n",
        "    results[test_subject_id] = {\n",
        "        \"general_acc\": acc_general,\n",
        "        \"assembled_acc\": best_acc,\n",
        "        \"best_subset\": best_subset\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAwOkESdgVgX",
        "outputId": "f3de01d8-a7f3-4d0d-f7c5-1918137d1642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================== Summary ==================\n",
            "Average general model accuracy: 89.98%\n",
            "Average assembled model (best subset) accuracy: 0.93%\n"
          ]
        }
      ],
      "source": [
        "# After running the LOSO loop and filling the results dict\n",
        "\n",
        "general_acc_list = []\n",
        "assembled_acc_list = []\n",
        "\n",
        "for subj_id, res in results.items():\n",
        "    general_acc_list.append(res[\"general_acc\"])\n",
        "    assembled_acc_list.append(res[\"assembled_acc\"])\n",
        "\n",
        "avg_general_acc = np.mean(general_acc_list)\n",
        "avg_assembled_acc = np.mean(assembled_acc_list)\n",
        "\n",
        "print(f\"\\n================== Summary ==================\")\n",
        "print(f\"Average general model accuracy: {avg_general_acc:.2f}%\")\n",
        "print(f\"Average assembled model (best subset) accuracy: {avg_assembled_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpxbJP-g6eG2"
      },
      "source": [
        "# Hand-Crafted Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZTWMScMNzpL",
        "outputId": "c35a15e8-d2ab-4a38-e05c-7d9939e76500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "dataset_path = \"/content/drive/MyDrive/UCI_HAR_Dataset/\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MMB-_CGyTGZ1",
        "outputId": "94bf5910-d444-4db3-8d5f-09d835c099eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spectrum\n",
            "  Downloading spectrum-0.9.0.tar.gz (231 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/231.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m174.1/231.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.5/231.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting easydev (from spectrum)\n",
            "  Downloading easydev-0.13.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from spectrum) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from spectrum) (1.16.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from spectrum) (3.10.0)\n",
            "Collecting colorama<0.5.0,>=0.4.6 (from easydev->spectrum)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting colorlog<7.0.0,>=6.8.2 (from easydev->spectrum)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting line-profiler<5.0.0,>=4.1.2 (from easydev->spectrum)\n",
            "  Downloading line_profiler-4.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: pexpect<5.0.0,>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from easydev->spectrum) (4.9.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from easydev->spectrum) (4.3.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spectrum) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spectrum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spectrum) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spectrum) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spectrum) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spectrum) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spectrum) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spectrum) (2.9.0.post0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect<5.0.0,>=4.9.0->easydev->spectrum) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->spectrum) (1.17.0)\n",
            "Downloading easydev-0.13.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading line_profiler-4.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (720 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.1/720.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: spectrum\n",
            "  Building wheel for spectrum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spectrum: filename=spectrum-0.9.0-cp312-cp312-linux_x86_64.whl size=236768 sha256=2b84c2abe089f2a2e5bbc020237e59a2dc4b57c60dd90c879025dd663f37072f\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/a0/e0/e04656d89dd723adbe6ea41ab5fe702f5d4ccf95653eb54b04\n",
            "Successfully built spectrum\n",
            "Installing collected packages: line-profiler, colorlog, colorama, easydev, spectrum\n",
            "Successfully installed colorama-0.4.6 colorlog-6.9.0 easydev-0.13.3 line-profiler-4.2.0 spectrum-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install spectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K34U3QC5zTG4",
        "outputId": "1c4ce044-f3b3-4c92-b4c8-d0e9c882f62f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting features for train set (7352 windows):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7352/7352 [07:31<00:00, 16.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved train features to /content/drive/MyDrive/UCI_HAR_Dataset/train_features.csv\n",
            "\n",
            "Extracting features for test set (2947 windows):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2947/2947 [02:51<00:00, 17.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved test features to /content/drive/MyDrive/UCI_HAR_Dataset/test_features.csv\n",
            "\n",
            "Final shapes:\n",
            "Train: (7352, 563)\n",
            "Test: (2947, 563)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis, iqr, entropy\n",
        "from spectrum import aryule  # for AR coefficients (Burg method)\n",
        "from scipy.fft import rfft\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# -------------------------\n",
        "# 1. Load segmented signals\n",
        "# -------------------------\n",
        "def load_signals(dataset_path, subset=\"train\"):\n",
        "    SIGNALS = [\n",
        "        \"body_acc_x\", \"body_acc_y\", \"body_acc_z\",\n",
        "        \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\",\n",
        "        \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"\n",
        "    ]\n",
        "    signal_data = {}\n",
        "    for signal in SIGNALS:\n",
        "        filename = f\"{dataset_path}/{subset}/Inertial Signals/{signal}_{subset}.txt\"\n",
        "        signal_data[signal] = np.loadtxt(filename)\n",
        "    return signal_data\n",
        "\n",
        "def load_labels(dataset_path, subset=\"train\"):\n",
        "    filename = f\"{dataset_path}/{subset}/y_{subset}.txt\"\n",
        "    return np.loadtxt(filename).astype(int)\n",
        "\n",
        "# -------------------------\n",
        "# 2. Derived signals\n",
        "# -------------------------\n",
        "def compute_jerk(x, fs=50):\n",
        "    return np.gradient(x, axis=1) * fs\n",
        "\n",
        "def compute_magnitude(x, y, z):\n",
        "    return np.sqrt(x**2 + y**2 + z**2)\n",
        "\n",
        "def compute_fft(sig):\n",
        "    N = sig.shape[1]\n",
        "    f = rfft(sig, axis=1)\n",
        "    return np.abs(f) / N\n",
        "\n",
        "# -------------------------\n",
        "# 3. Feature helpers\n",
        "# -------------------------\n",
        "def compute_basic_stats(sig):\n",
        "    N = sig.shape[0]\n",
        "    feats = {\n",
        "        \"mean\": np.mean(sig),\n",
        "        \"std\": np.std(sig),\n",
        "        \"mad\": np.median(np.abs(sig - np.median(sig))),\n",
        "        \"max\": np.max(sig),\n",
        "        \"min\": np.min(sig),\n",
        "        \"sma\": np.sum(np.abs(sig)) / N,\n",
        "        \"energy\": np.sum(sig**2) / N,\n",
        "        \"iqr\": iqr(sig),\n",
        "        \"entropy\": entropy(np.histogram(sig, bins=10, density=True)[0] + 1e-6)\n",
        "    }\n",
        "    try:\n",
        "        ar, _, _ = aryule(sig, order=4)\n",
        "        feats.update({f\"arCoeff_{i+1}\": ar[i] for i in range(4)})\n",
        "    except Exception:\n",
        "        feats.update({f\"arCoeff_{i+1}\": 0.0 for i in range(4)})\n",
        "    return feats\n",
        "\n",
        "def compute_correlation(x, y):\n",
        "    if np.std(x) == 0 or np.std(y) == 0:\n",
        "        return 0.0\n",
        "    return np.corrcoef(x, y)[0, 1]\n",
        "\n",
        "def compute_freq_stats(sig_fft, fs=50):\n",
        "    N = sig_fft.shape[0]\n",
        "    freqs = np.linspace(0, fs/2, N)\n",
        "    mags = sig_fft\n",
        "\n",
        "    feats = {\n",
        "        \"maxInds\": np.argmax(mags),\n",
        "        \"meanFreq\": np.sum(freqs * mags) / (np.sum(mags) + 1e-6),\n",
        "        \"skewness\": skew(mags),\n",
        "        \"kurtosis\": kurtosis(mags)\n",
        "    }\n",
        "\n",
        "    bands = np.array_split(mags, 8)\n",
        "    for i, b in enumerate(bands):\n",
        "        feats[f\"bandEnergy_{i+1}\"] = np.sum(b**2) / len(b)\n",
        "\n",
        "    grouped = [(0,2),(2,4),(4,6),(6,8),(0,4),(4,8),(0,6),(2,8)]\n",
        "    for idx,(start,end) in enumerate(grouped):\n",
        "        combined = np.concatenate(bands[start:end])\n",
        "        feats[f\"bandEnergy_group_{idx+1}\"] = np.sum(combined**2)/len(combined)\n",
        "    return feats\n",
        "\n",
        "def compute_angle(vec1, vec2):\n",
        "    dot = np.dot(vec1, vec2)\n",
        "    denom = np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-6\n",
        "    return np.arccos(np.clip(dot/denom, -1, 1))\n",
        "\n",
        "# -------------------------\n",
        "# 4. Feature extraction per window\n",
        "# -------------------------\n",
        "def extract_features_for_window(time_signals, freq_signals):\n",
        "    feats = {}\n",
        "    for name, sig in time_signals.items():\n",
        "        if sig.ndim == 1:\n",
        "            feats.update({f\"{name}_{k}\": v for k,v in compute_basic_stats(sig).items()})\n",
        "        else:\n",
        "            x, y, z = sig.T\n",
        "            for axis, s in zip(\"XYZ\",[x,y,z]):\n",
        "                feats.update({f\"{name}-{k}({axis})\": v for k,v in compute_basic_stats(s).items()})\n",
        "            feats[f\"{name}-correlation(X,Y)\"] = compute_correlation(x,y)\n",
        "            feats[f\"{name}-correlation(X,Z)\"] = compute_correlation(x,z)\n",
        "            feats[f\"{name}-correlation(Y,Z)\"] = compute_correlation(y,z)\n",
        "\n",
        "    for name, sig in freq_signals.items():\n",
        "        if sig.ndim == 1:\n",
        "            feats.update({f\"{name}_{k}\": v for k,v in compute_freq_stats(sig).items()})\n",
        "        else:\n",
        "            for axis, s in zip(\"XYZ\",[sig[:,i] for i in range(sig.shape[1])] ):\n",
        "                feats.update({f\"{name}-{axis}_{k}\": v for k,v in compute_freq_stats(s).items()})\n",
        "\n",
        "    mean_vectors = {\n",
        "        \"gravityMean\": np.mean(time_signals[\"tGravityAcc\"], axis=0),\n",
        "        \"tBodyAccMean\": np.mean(time_signals[\"tBodyAcc\"], axis=0),\n",
        "        \"tBodyAccJerkMean\": np.mean(time_signals[\"tBodyAccJerk\"], axis=0),\n",
        "        \"tBodyGyroMean\": np.mean(time_signals[\"tBodyGyro\"], axis=0),\n",
        "        \"tBodyGyroJerkMean\": np.mean(time_signals[\"tBodyGyroJerk\"], axis=0)\n",
        "    }\n",
        "\n",
        "    feats[\"angle_tBodyAccMean_gravityMean\"] = compute_angle(mean_vectors[\"tBodyAccMean\"], mean_vectors[\"gravityMean\"])\n",
        "    feats[\"angle_tBodyAccJerkMean_gravityMean\"] = compute_angle(mean_vectors[\"tBodyAccJerkMean\"], mean_vectors[\"gravityMean\"])\n",
        "    feats[\"angle_tBodyGyroMean_gravityMean\"] = compute_angle(mean_vectors[\"tBodyGyroMean\"], mean_vectors[\"gravityMean\"])\n",
        "    feats[\"angle_tBodyGyroJerkMean_gravityMean\"] = compute_angle(mean_vectors[\"tBodyGyroJerkMean\"], mean_vectors[\"gravityMean\"])\n",
        "    for i, axis in enumerate(\"XYZ\"):\n",
        "        feats[f\"angle_{axis}_gravityMean\"] = compute_angle(mean_vectors[\"gravityMean\"], np.eye(3)[i])\n",
        "    return feats\n",
        "\n",
        "# -------------------------\n",
        "# 5. Full feature matrix for all windows\n",
        "# -------------------------\n",
        "def extract_features_for_dataset(dataset_path, subset=\"train\"):\n",
        "    signals = load_signals(dataset_path, subset=subset)\n",
        "\n",
        "    body_acc = np.stack([signals[\"body_acc_x\"], signals[\"body_acc_y\"], signals[\"body_acc_z\"]], axis=2)\n",
        "    body_gyro = np.stack([signals[\"body_gyro_x\"], signals[\"body_gyro_y\"], signals[\"body_gyro_z\"]], axis=2)\n",
        "    total_acc = np.stack([signals[\"total_acc_x\"], signals[\"total_acc_y\"], signals[\"total_acc_z\"]], axis=2)\n",
        "    gravity_acc = total_acc - body_acc\n",
        "\n",
        "    derived_time = {\n",
        "        \"tBodyAcc\": body_acc,\n",
        "        \"tGravityAcc\": gravity_acc,\n",
        "        \"tBodyAccJerk\": compute_jerk(body_acc),\n",
        "        \"tBodyGyro\": body_gyro,\n",
        "        \"tBodyGyroJerk\": compute_jerk(body_gyro)\n",
        "    }\n",
        "\n",
        "    for key in [\"tBodyAcc\",\"tGravityAcc\",\"tBodyAccJerk\",\"tBodyGyro\",\"tBodyGyroJerk\"]:\n",
        "        sig = derived_time[key]\n",
        "        derived_time[key+\"Mag\"] = compute_magnitude(*np.rollaxis(sig,2))\n",
        "\n",
        "    derived_freq = {}\n",
        "    for key in [\"tBodyAcc\",\"tBodyAccJerk\",\"tBodyGyro\"]:\n",
        "        sig = derived_time[key]\n",
        "        derived_freq[\"f\"+key[1:]] = np.stack([compute_fft(sig[:,:,i]) for i in range(3)], axis=2)\n",
        "    for key in [\"tBodyAccMag\",\"tGravityAccMag\",\"tBodyAccJerkMag\",\"tBodyGyroMag\",\"tBodyGyroJerkMag\"]:\n",
        "        derived_freq[\"f\"+key[1:]] = compute_fft(derived_time[key])\n",
        "\n",
        "    n_windows = body_acc.shape[0]\n",
        "    feature_list = []\n",
        "    print(f\"\\nExtracting features for {subset} set ({n_windows} windows):\")\n",
        "    for i in tqdm(range(n_windows)):\n",
        "        time_win = {k:v[i] for k,v in derived_time.items()}\n",
        "        freq_win = {k:v[i] for k,v in derived_freq.items()}\n",
        "        feats = extract_features_for_window(time_win, freq_win)\n",
        "        feature_list.append(feats)\n",
        "\n",
        "    df = pd.DataFrame(feature_list)\n",
        "\n",
        "    # load labels and add them\n",
        "    labels = load_labels(dataset_path, subset=subset)\n",
        "    df[\"label\"] = labels\n",
        "\n",
        "    # save to CSV\n",
        "    out_path = os.path.join(dataset_path, f\"{subset}_features.csv\")\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"Saved {subset} features to {out_path}\")\n",
        "    return df\n",
        "\n",
        "# -------------------------\n",
        "# 6. Example usage\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = \"/content/drive/MyDrive/UCI_HAR_Dataset/\"\n",
        "\n",
        "    df_train = extract_features_for_dataset(dataset_path, subset=\"train\")\n",
        "    df_test  = extract_features_for_dataset(dataset_path, subset=\"test\")\n",
        "\n",
        "    print(\"\\nFinal shapes:\")\n",
        "    print(\"Train:\", df_train.shape)\n",
        "    print(\"Test:\", df_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5Hwg1B548zX",
        "outputId": "2fbba741-2bf2-41ef-96fd-c6a99593dc50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (7352, 563)  Test shape: (2947, 563)\n",
            "Epoch 1/30 | Loss=0.6498 | Test Acc=0.8728\n",
            "Epoch 2/30 | Loss=0.1811 | Test Acc=0.9226\n",
            "Epoch 3/30 | Loss=0.1132 | Test Acc=0.9335\n",
            "Epoch 4/30 | Loss=0.0905 | Test Acc=0.9311\n",
            "Epoch 5/30 | Loss=0.0800 | Test Acc=0.9399\n",
            "Epoch 6/30 | Loss=0.0698 | Test Acc=0.9386\n",
            "Epoch 7/30 | Loss=0.0508 | Test Acc=0.9420\n",
            "Epoch 8/30 | Loss=0.0479 | Test Acc=0.9389\n",
            "Epoch 9/30 | Loss=0.0431 | Test Acc=0.9281\n",
            "Epoch 10/30 | Loss=0.0339 | Test Acc=0.9460\n",
            "Epoch 11/30 | Loss=0.0339 | Test Acc=0.9491\n",
            "Epoch 12/30 | Loss=0.0379 | Test Acc=0.9450\n",
            "Epoch 13/30 | Loss=0.0403 | Test Acc=0.9450\n",
            "Epoch 14/30 | Loss=0.0510 | Test Acc=0.9467\n",
            "Epoch 15/30 | Loss=0.0249 | Test Acc=0.9515\n",
            "Epoch 16/30 | Loss=0.0258 | Test Acc=0.9508\n",
            "Epoch 17/30 | Loss=0.0262 | Test Acc=0.9450\n",
            "Epoch 18/30 | Loss=0.0183 | Test Acc=0.9528\n",
            "Epoch 19/30 | Loss=0.0150 | Test Acc=0.9491\n",
            "Epoch 20/30 | Loss=0.0112 | Test Acc=0.9562\n",
            "Epoch 21/30 | Loss=0.0160 | Test Acc=0.9532\n",
            "Epoch 22/30 | Loss=0.0257 | Test Acc=0.9467\n",
            "Epoch 23/30 | Loss=0.0162 | Test Acc=0.9460\n",
            "Epoch 24/30 | Loss=0.0329 | Test Acc=0.9484\n",
            "Epoch 25/30 | Loss=0.0103 | Test Acc=0.9586\n",
            "Epoch 26/30 | Loss=0.0155 | Test Acc=0.9488\n",
            "Epoch 27/30 | Loss=0.0126 | Test Acc=0.9555\n",
            "Epoch 28/30 | Loss=0.0125 | Test Acc=0.9430\n",
            "Epoch 29/30 | Loss=0.0085 | Test Acc=0.9569\n",
            "Epoch 30/30 | Loss=0.0139 | Test Acc=0.9539\n",
            "Final Test Accuracy: 0.9538513742789277\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --------------------------\n",
        "# 1. Load features & labels\n",
        "# --------------------------\n",
        "dataset_path = \"/content/drive/MyDrive/UCI_HAR_Dataset\"\n",
        "\n",
        "# Correct filenames\n",
        "X_train = pd.read_csv(f\"{dataset_path}/train_features.csv\").values\n",
        "X_test  = pd.read_csv(f\"{dataset_path}/test_features.csv\").values\n",
        "\n",
        "y_train = np.loadtxt(f\"{dataset_path}/train/y_train.txt\").astype(int) - 1  # labels are 1..6 → 0..5\n",
        "y_test  = np.loadtxt(f\"{dataset_path}/test/y_test.txt\").astype(int) - 1\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n",
        "\n",
        "# --------------------------\n",
        "# 2. Preprocess (scaling)\n",
        "# --------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# --------------------------\n",
        "# 3. Define MLP model (3 hidden layers, 100 neurons each)\n",
        "# --------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=6):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(100, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "model = MLP(input_dim=input_dim)\n",
        "\n",
        "# --------------------------\n",
        "# 4. Training setup\n",
        "# --------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 30  # can increase if needed\n",
        "\n",
        "# --------------------------\n",
        "# 5. Training loop\n",
        "# --------------------------\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Eval on test\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            preds = model(xb)\n",
        "            y_true.extend(yb.numpy())\n",
        "            y_pred.extend(preds.argmax(1).numpy())\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss={avg_loss:.4f} | Test Acc={acc:.4f}\")\n",
        "\n",
        "# --------------------------\n",
        "# 6. Final evaluation\n",
        "# --------------------------\n",
        "print(\"Final Test Accuracy:\", acc)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fECyEOphPfCc",
        "zqFPiB432eBU",
        "FRmlwtlmASyF",
        "IpxbJP-g6eG2"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}